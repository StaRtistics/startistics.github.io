[{"authors":null,"categories":null,"content":"Biostatistician with more than 5 years of experience working in the pharmaceutical industry. Primary therapeutic areas of interest are CVRM (cardiovascular, renal and metabolism). Experience in contributing to the development of the statistical strategy, overseeing the technical activities related to design, delivery and interpretation, as well as participating in high level internal governance committees and regulatory submissions of a project, and representing Biometrics in cross-functional collaborations. Teaching experience from Le Mans University, American University of Armenia and Yerevan State University. The main taught courses are Statistics and Data Science with R. Research interests are in the field of Statistics of Stochastic Processes.\n","date":1632355200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1632355200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://gasparyan.co/author/samvel-b.-gasparyan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/samvel-b.-gasparyan/","section":"authors","summary":"Biostatistician with more than 5 years of experience working in the pharmaceutical industry. Primary therapeutic areas of interest are CVRM (cardiovascular, renal and metabolism). Experience in contributing to the development of the statistical strategy, overseeing the technical activities related to design, delivery and interpretation, as well as participating in high level internal governance committees and regulatory submissions of a project, and representing Biometrics in cross-functional collaborations.","tags":null,"title":"Samvel B. Gasparyan","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e87efdfe209d90ea3c6332a7cbd9d08b","permalink":"https://gasparyan.co/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://gasparyan.co/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://gasparyan.co/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://gasparyan.co/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":["Samvel B. Gasparyan","Elaine K. Kowalewski","Folke Folkvaljon","Olof Bengtsson","Joan Buenconsejo","John Adler","Gary G. Koch"],"categories":null,"content":"","date":1632355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632355200,"objectID":"98ce87fde87e418b8dd8b36758ef5098","permalink":"https://gasparyan.co/publication/power-win-odds/","publishdate":"2021-09-23T00:00:00Z","relpermalink":"/publication/power-win-odds/","section":"publication","summary":"The win odds is a distribution-free method of comparing locations of distributions of two independent random variables. Introduced as a method for analyzing hierarchical composite endpoints, it is well suited to be used in the analysis of ordinal scale endpoints in COVID-19 clinical trials. For a single outcome, we provide power and sample size calculation formulas for the win odds test. We also provide an implementation of the win odds analysis method for a single ordinal outcome in a commonly used statistical software to make the win odds analysis fully reproducible.","tags":["Source Themes"],"title":"Power and sample size calculation for the win odds test: application to an ordinal endpoint in COVID-19 trials","type":"publication"},{"authors":["Samvel B. Gasparyan","Joan Buenconsejo"],"categories":null,"content":"","date":1632348000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632348000,"objectID":"8767345bef7160d2d70cfcd2739867d7","permalink":"https://gasparyan.co/talk/asa2021/","publishdate":"2021-09-23T00:00:00+02:00","relpermalink":"/talk/asa2021/","section":"talk","summary":"The global pandemic caused by coronavirus disease 2019 (COVID-19) has created challenges for researchers across the globe and incentives to accelerate development of new therapies. An important therapeutic goal of a successful COVID-19 treatment in hospitalized patients is recovery, usually defined in a fixed period of time, e.g. 30 days. Recovery in the simplest form is the outcome of discharge from hospital analyzed using time-to-event analysis methods or a responder analysis with a specific threshold for defining recovery based on the improvement in clinical status compared to baseline. A more comprehensive endpoint that includes patient recovery is the ordinal scale endpoint suggested by WHO that includes multiple clinical states (death and cure) and evaluation of the effect is done on the full range of outcomes. The ordinal scale endpoint includes a full range of outcomes ranked by clinical importance that are between “death” and “cure” so as to represent meaningful patient states. In this presentation we will discuss the definition of hierarchical composite endpoints (HCE) in COVID-19 setting and how they differ from WHO defined ordinal endpoints. As an analysis method we advocate the use of win ratio (WR) methods. The win ratio is a general method of comparing locations of distributions of two independent, ordinal random variables which can be estimated using distribution-free methods, based on the theory of U-statistics. We will discuss also the key considerations when designing new trials based on HCE and win ratio analysis.","tags":[],"title":"Hierarchical Composite Endpoints: Definition and Analysis Using the Win Ratio (with ties!)","type":"talk"},{"authors":["Mikhail N. Kosiborod","Russell Esterline","Remo HM Furtado","Jan Oscarsson","Samvel B. Gasparyan","Gary G. Koch","Felipe Martinez","Omar Mukhtar","Subodh Verma","Vijay Chopra","Joan Buenconsejo","Anna Maria Langkilde","Philip Ambery","Fengming Tang","Kensey Gosch","Sheryl L Windsor","Emily E Akin","Ronaldo V P Soares","Diogo D F Moia","Matthew Aboudara","Conrado Roberto","Hoffmann Filho","Audes D M Feitosa","Alberto Fonseca","Vishnu Garla","Robert A Gordon","Ali Javaheri","Cristiano P Jaeger","Paulo E Leaes","Michael Nassif","Michael Pursley","Fabio Serra Silveira","Weimar Kunz Sebba Barroso","José Roberto","Lazcano Soto","Lilia Nigro Maia","Otavio Berwanger"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"69d388418a1091780958f157b7ce2766","permalink":"https://gasparyan.co/publication/dare-19-primary/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/publication/dare-19-primary/","section":"publication","summary":"DARE-19 was a randomised, double-blind, placebo-controlled trial of patients hospitalised with COVID-19 and with at least one cardiometabolic risk factor (ie, hypertension, type 2 diabetes, atherosclerotic cardiovascular disease, heart failure, and chronic kidney disease). Patients were randomly assigned 1:1 to dapagliflozin (10 mg daily orally) or matched placebo for 30 days.","tags":["Source Themes"],"title":"Dapagliflozin in patients with cardiometabolic risk factors hospitalised with COVID-19 (DARE-19)","type":"publication"},{"authors":["Pardeep S. Jhund","Piotr Ponikowski","Kieran F. Docherty","Samvel B. Gasparyan","Michael Böhm","Chern-En Chiang","Akshay S. Desai","Jonathon Howlett","Masafumi Kitakaze","Mark C. Petrie","Subodh Verma","Olof Bengtsson","Anna-Maria Langkilde","Mikaela Sjöstrand","Silvio E. Inzucchi","Lars Køber","Mikhail N. Kosiborod","Felipe A. Martinez","Marc S. Sabatine","Scott D. Solomon","John J.V. McMurray"],"categories":null,"content":"","date":1617926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617926400,"objectID":"5359322fc04514a369eaf3c8ed60afba","permalink":"https://gasparyan.co/publication/dapa-hf-recurrent/","publishdate":"2021-04-09T00:00:00Z","relpermalink":"/publication/dapa-hf-recurrent/","section":"publication","summary":"Dapagliflozin reduced the risk of total (first and repeat) HF hospitalizations and cardiovascular death. Time-to-first event analysis underestimated the benefit of dapagliflozin in HF and reduced ejection fraction.","tags":["Source Themes"],"title":"Dapagliflozin and Recurrent Heart Failure Hospitalizations in Heart Failure With Reduced Ejection Fraction: An Analysis of DAPA-HF","type":"publication"},{"authors":["Mikhail N. Kosiborod","Otavio Berwanger","Gary G. Koch","Felipe Martinez","Subodh Verma","Vijay Chopra","Ali Javaheri","Philip Ambery","Samvel B. Gasparyan","Joan Buenconsejo","David Sjöström","Anna Maria Langkilde","Jan Oscarsson","Russell Esterline"],"categories":null,"content":"","date":1607990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990400,"objectID":"fd143bb0ccb7b53e9ea08c4d525d7847","permalink":"https://gasparyan.co/publication/dare-19-design/","publishdate":"2020-12-15T00:00:00Z","relpermalink":"/publication/dare-19-design/","section":"publication","summary":"DARE-19 will evaluate whether dapagliflozin can prevent COVID-19-related complications and all-cause mortality, or improve clinical recovery, and assess the safety profile of dapagliflozin in this patient population. Currently, DARE-19 is the first large randomized controlled trial investigating use of sodium-glucose cotransporter 2 inhibitors in patients with COVID-19.","tags":["Source Themes"],"title":"Effects of dapagliflozin on prevention of major clinical events and recovery in patients with respiratory failure because of COVID‐19: Design and rationale for the DARE‐19 study","type":"publication"},{"authors":null,"categories":"Statistics","content":"\r\rCEO Salary Estimation Problem\rConsider the following problem. An investigative reporter wants to figure out how much salary makes the CEO of an investment bank X. For this he conducts interviews with some of the employees of that bank and writes down their salaries, which forms the following sample\n\\[X^n=(X_1,\\cdots,X_n).\\]\rThe reporter knows only that the salaries in that bank can range from 0 (unpaid interns) to \\(\\theta\\) which is the salary of the CEO that the reporter wants to estimate. Since he has no information about the structure of salaries in the bank X, he assumes that the salaries have uniform distribution in the interval \\([0,\\theta],\\)\r\\[X^n=(X_1,\\cdots,X_n),\\ \\ X_i\\sim {\\mathbb U}(0,\\theta),\\,\\theta\u0026gt;0.\\]\nThe uniform distribution is the maximum entropy probability distribution under no constraint other than that it is contained in the distribution’s support (according to the principle of maximum entropy, if nothing is known about a distribution except that it belongs to a certain class (usually defined in terms of specified properties or measures), then the distribution with the largest entropy should be chosen as the least-informative default.)\nFrequentist and Bayesian Estimation\rSince no other information is known about the possible values of the CEO’s salary, the reporter needs to estimate the unknown \\(\\theta\\) using the sample \\(X^n.\\) For the uniform distribution the maximum likelihood estimator (MLE) for \\(\\theta\\) will be\r\\[\\hat\\theta_n=X_{(n)}=\\max(X_1,\\cdots,X_n).\\]\rTherefore, the reporter needs to ask for salary information from as many employees of bank X as possible and take the maximum of these values, which will serve as an estimate for the CEO’s salary.\nNow suppose that the investigative reporter wants to get a Pulitzer prize for his reporting and remembers that he has a minor in Statistics. He reads economics literature and finds out that economists established that nationally the salaries of CEOs of banks follow the Pareto distribution \\(\\theta\\sim Pa(\\theta_0, a)\\), because of the Pareto principle, which states that a large portion of wealth of CEOs is held by a small fraction of them. Using this prior distribution, a Bayesian estimator for the salary of CEO of bank X will be\n\\[\\hat\\theta^B_n=\\frac{a+n}{a+n-1}\\max(\\theta_0,X_1,\\cdots,X_n).\\]\rHere \\(a\\) and \\(\\theta_0\\) are unknown as well and can be estimated using the available data of CEO salaries of other banks. Therefore, the investigative reporter decides to use the work of his colleagues - other investigative reporters - who conducted studies in other banks and reported estimates for salaries of CEOs. Denote this new sample of salaries of other CEOs as\r\\[\\vartheta^m=(\\vartheta_1,\\cdots,\\vartheta_m).\\]\rSince \\(\\theta\\) follows the Pareto distribution then the parameters \\(\\theta_0\\) and \\(a\\) can be estimated as follows:\r\\[\\hat\\theta_0=\\vartheta_{(1)}=\\min(\\vartheta_1,\\cdots,\\vartheta_m),\\ \\ \\hat a=\\frac{m}{\\sum_{i=1}^m\\ln\\frac{\\vartheta_i}{\\vartheta_{(1)}}}.\\]\rTherefore, drawing on reports from other investigations and his own survey, the investigative reporter can obtain the following estimator of the salary of the CEO\n\\[\\hat\\theta^B_n=\\frac{\\hat a+n}{\\hat a+n-1}\\max(\\vartheta_{(1)},X_1,\\cdots,X_n).\\]\rTherefore, each time a new salary of some CEO is reported \\((\\vartheta_{m+1}),\\) this new data can be used by the investigative reporter to update the estimate of the salary of the CEO of bank X.\n\r\rConjugate priors\rIn the example above the prior distribution of the unknown parameter \\(\\theta\\) was chosen because of theoretical considerations, based on an economic law (the Pareto rule). In most cases, it is not possible to give preference to one prior over the others based on some principle, conjugate priors are selected for computational simplicity.\n\rThe prior distribution \\(\\theta\\sim\\pi(\\theta),\\,\\theta\\in\\Theta\\) (with the density \\(p(\\theta)\\)) is called conjugate prior for the likelihood function \\(f(x|\\theta)\\) if its posterior density function \\(f(\\theta|x)\\) is from the same family as the likelihood function.\r\rThe Bayes’ theorem specifies the following relationship between the likelihood function \\(f(x|\\theta)\\) and the posterior function \\(f(\\theta|x),\\) for the given prior density \\(p(\\theta)\\)\n\\[f(\\theta|x)=\\frac{f(x|\\theta) p(\\theta)}{\\int_\\Theta f(x|\\vartheta) p(\\vartheta) d\\vartheta}.\\]\rTherefore, the density \\(p(\\theta)\\) is a conjugate prior for \\(f(x|\\theta),\\) if \\(f(x|\\theta)\\) and \\(f(\\theta|x)\\) are from the same family.\n\rAppendix\rPareto distributions\rThe random variable \\(\\xi\\) has a Pareto distribution with parameters \\(\\theta\u0026gt;0\\) and \\(a\u0026gt;0\\), \\(\\xi\\sim Pa(\\theta,a)\\) if the distribution function is given by the formula\r\\[F(x)=1-\\left(\\frac{\\theta}{x}\\right)^a,\\,x\u0026gt;\\theta,\\]\rand \\(F(x)=0,\\,x\\leq \\theta.\\) The density function will have the form\r\\[f(x)=a\\frac{\\theta^a}{x^{a+1}},\\,x\u0026gt;\\theta.\\]\rIn the case of \\(a\u0026gt;1\\) the expectation of a Pareto random variable is\r\\[E\\xi=\\frac{a}{a-1}\\theta.\\]\rIf \\(a\u0026gt;2\\) then the variance exists as well and equals to\r\\[Var(\\xi)=\\frac{a}{(a-1)^2(a-2)}\\theta^2.\\]\n\rProblems\rShow that if \\(\\theta\\) follows the Pareto distribution \\(\\theta\\sim Pa(\\theta_0, a),\\) then the parameters \\(\\theta_0\\) and \\(a\\) can be estimated as follows:\r\\[\\hat\\theta_0=\\vartheta_{(1)}=\\min(\\vartheta_1,\\cdots,\\vartheta_m),\\ \\ \\hat a=\\frac{m}{\\sum_{i=1}^m\\ln\\frac{\\vartheta_i}{\\vartheta_{(1)}}},\\]\rwhere \\(\\vartheta^m=(\\vartheta_1,\\cdots,\\vartheta_m)\\) is an i.i.d. sample from \\(Pa(\\theta_0,a).\\)\n\rSuppose that an i.i.d. sample is observed\r\\[X^n=(X_1,\\cdots,X_n),\\,X_i\\sim F(x,\\theta),\\,\\theta\\in\\Theta,\\,x\\in R.\\]\rConsider an estimator for \\(\\theta\\) based on the sample \\(X^n\\)\r\\[\\hat\\theta_n(X^n)=\\hat\\theta_n(X_1,\\cdots,X_n).\\]\rMean Squared Error (MSE, \\(L(\\theta,\\hat\\theta)\\)) for this estimator is defined as\r\\[E_\\theta(\\hat\\theta_n-\\theta)^2=\\int_{R^n}(\\hat\\theta_n(x^n)-\\theta)^2dF(x_1,\\theta)\\cdots dF(x_n,\\theta).\\]\rIf the prior distribution of the unknown parameter \\(\\theta\\) is given \\(\\theta\\sim \\pi(\\theta),\\,\\theta\\in\\Theta,\\) then the Bayesian risk of the estimator \\(\\hat\\theta_n\\) is defined as\r\\[E_\\pi L(\\theta,\\hat\\theta)=\\int_\\Theta E_\\theta(\\hat\\theta_n-\\theta)^2d \\pi(\\theta).\\]\rShow that the Bayes estimator, defined as the one which minimizes the Bayesian risk, has the form\r\\[\\hat\\theta_n^B=\\arg_{\\hat\\theta_n}\\min_{\\theta\\in\\Theta} E_\\pi L(\\theta,\\hat\\theta)=\\int_{\\Theta}\\theta f(\\theta|x)d\\theta,\\]\rwhere \\(f(\\theta|x)\\) is the posterior distribution of \\(\\theta.\\)\n\rSuppose that \\(X_i\\sim {\\mathbb U}(0,\\theta)\\) and \\(\\theta\\sim Pa(\\theta_0, a)\\). Show that the Bayes estimator has the form\n\r\r\\[\\hat\\theta^B_n=\\frac{a+n}{a+n-1}\\max(\\theta_0,X_1,\\cdots,X_n).\\]\n\r\r","date":1605330000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605330000,"objectID":"c2eab7ca1d63e92e4eb5301f0667a74a","permalink":"https://gasparyan.co/post/bayes/2020-11-14-r-rmarkdown/","publishdate":"2020-11-14T09:00:00+04:00","relpermalink":"/post/bayes/2020-11-14-r-rmarkdown/","section":"post","summary":"CEO Salary Estimation Problem\rConsider the following problem. An investigative reporter wants to figure out how much salary makes the CEO of an investment bank X. For this he conducts interviews with some of the employees of that bank and writes down their salaries, which forms the following sample","tags":["Statistics","Consistency","Asymptotic Normality","MLE","Method of Moments","Bayes"],"title":"Bayesian Estimation","type":"post"},{"authors":["Samvel B. Gasparyan","Folke Folkvaljon","Olof Bengtsson","Joan Buenconsejo","Gary G. Koch"],"categories":null,"content":"","date":1595721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595721600,"objectID":"89ad83a0e9386d222fa1455301cc3975","permalink":"https://gasparyan.co/publication/adjusted-win-ratio/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/publication/adjusted-win-ratio/","section":"publication","summary":"The win ratio is a general method of comparing locations of distributions of two independent, ordinal random variables, and it can be estimated without distributional assumptions. In this paper we provide a unified theory of win ratio estimation in the presence of stratification and adjustment by a numeric variable.","tags":["Source Themes"],"title":"Adjusted win ratio with stratification: Calculation methods and interpretation","type":"publication"},{"authors":null,"categories":["R"],"content":"\r\rMann-Whitney estimate for the win probability\rConsider two independent, continuous RVs (random variables) \\(\\xi\\) and \\(\\eta\\). The following probability is called the win probability of RV \\(\\eta\\) against the RV \\(\\xi\\)\n\\[\\begin{align*}\r\\theta=P(\\eta\u0026gt;\\xi).\r\\end{align*}\\]\nGiven an i.i.d. (independent, identically distributed) random sample from \\(\\xi,\\) denoted by \\(X=(X_1,\\cdots,X_{n_1})\\) and an i.i.d. sample from \\(\\eta\\) denoted by \\(Y=(Y_1,\\cdots,Y_{n_2})\\) we are interested in estimating the unknown parameter \\(\\theta.\\)\nThe following estimator is called the Mann-Whitney estimator (or, the win proportion) for the win probability\n\\[\\begin{align*}\r\\hat\\theta_N=\\frac{1}{n_1n_2}\\sum_{i=1}^{n_1}\\sum_{j=1}^{n_2}I(X_i\u0026lt;Y_j).\r\\end{align*}\\]\rHere \\(N=n_1+n_2\\) is the total sample size, whereas \\(I(\\cdot)\\) is the indicator function which takes the value 1 if the underlying inequality is true and 0 otherwise. When \\(n_1\\rightarrow+\\infty,\\,n_2\\rightarrow+\\infty\\) then the win proportion is a consistent estimator (convergence in probability) for the win probability\r\\[\\begin{align*}\r\\hat\\theta_N\\longrightarrow\\theta.\r\\end{align*}\\]\rIf \\(\\frac{n_1}{N}\\rightarrow \\alpha,\\) as \\(n_1\\rightarrow+\\infty,\\,n_2\\rightarrow+\\infty,\\) then the win proportion is also asymptotically normal\r\\[\\begin{align*}\r\\sqrt{N}(\\hat\\theta_N-\\theta)\\Longrightarrow{\\mathcal N}\\left(0,\\frac{1}{1-\\alpha}\\sigma_{10}^2+\\frac{1}{\\alpha}\\sigma_{01}^2\\right).\r\\end{align*}\\]\rHere,\r\\[\\begin{align*}\r\\sigma_{10}^2=Cov(I(\\xi\u0026lt;\\eta),I(\\xi\u0026#39;\u0026lt;\\eta))=P(\\xi\u0026lt;\\eta,\\xi\u0026#39;\u0026lt;\\eta)-P(\\xi\u0026lt;\\eta)^2,\\\\\r\\sigma_{01}^2=Cov(I(\\xi\u0026lt;\\eta),I(\\xi\u0026lt;\\eta\u0026#39;))=P(\\xi\u0026lt;\\eta,\\xi\u0026lt;\\eta\u0026#39;)-P(\\xi\u0026lt;\\eta)^2,\r\\end{align*}\\]\rwhere \\(\\xi\u0026#39;\\) has the same distribution as \\(\\xi\\), \\(\\eta\u0026#39;\\) has the same distribution as \\(\\eta\\). All \\(\\xi,\\xi\u0026#39;,\\eta,\\eta\u0026#39;\\) are independent.\n\rApplication to exponential distributions\rSuppose now that \\(\\xi\\sim{\\mathbb E}(\\lambda)\\) and \\(\\eta\\sim{\\mathbb E}(\\mu).\\) In this case,\n\\[\\begin{align*}\r\\sigma_{10}^2=\\frac{2\\lambda}{(\\lambda+\\mu)(2\\lambda+\\mu)}-\\frac{\\lambda^2}{(\\lambda+\\mu)^2}=\\frac{\\lambda^2\\mu}{(\\lambda+\\mu)^2(2\\lambda+\\mu)}\\\\\r\\sigma_{01}^2=\\frac{\\lambda}{\\lambda+2\\mu}-\\frac{\\lambda^2}{(\\lambda+\\mu)^2}=\\frac{\\lambda\\mu^2}{(\\lambda+\\mu)^2(\\lambda+2\\mu)},\r\\end{align*}\\]\rtherefore, the asymptotic variance of the win proportion will be\r\\[\\begin{align*}\r\\sigma^2=\\frac{\\lambda\\mu}{(\\lambda+\\mu)^2}\\left(\\frac{1}{1-\\alpha}\\frac{\\lambda}{2\\lambda+\\mu}+\\frac{1}{\\alpha}\\frac{\\mu}{\\lambda+2\\mu}\\right).\r\\end{align*}\\]\nWe can check this by the following simulations\nn1 \u0026lt;- 700\rn2 \u0026lt;- 100\rN \u0026lt;- n1 + n2\rm \u0026lt;- 1000\rlambda \u0026lt;- 2\rmu \u0026lt;- 10\ralpha \u0026lt;- n1/(n1+n2)\rk \u0026lt;- lambda/(lambda+mu)\rWR \u0026lt;-NULL\rfor(i in 1:m){\rset.seed(i)\rX1 \u0026lt;- rexp(n1,lambda)\rX2 \u0026lt;- rexp(n2,mu)\rd \u0026lt;- expand.grid(x=X1,y=X2)\rd$w \u0026lt;- ifelse(d$y\u0026gt;d$x,1,ifelse(d$y==d$x,0.5,0))\rWR[i]\u0026lt;-sqrt(N)*(mean(d$w)-k)\r}\rx0\u0026lt;-3\rint \u0026lt;- seq(-x0,x0,0.001)\rCoeff0 \u0026lt;- mu*lambda/(lambda+mu)^2\rCoeff \u0026lt;- Coeff0*(1/(1-alpha)*lambda/(2*lambda+mu)+1/alpha*mu/(lambda+2*mu))\rhist(WR, nclass = 20, freq=FALSE, xlim=c(-x0,x0), ylim=c(0, 1.1), col = \u0026quot;lightblue\u0026quot;, border = \u0026quot;blue\u0026quot;)\rlines(int,dnorm(int, mean = 0, sd=sqrt(Coeff)), col=\u0026quot;2\u0026quot;)\r\rDefinition of the win odds\rConsider two independent, continuous RVs (random variables) \\(\\xi\\) and \\(\\eta\\). The odds of the win probability is called the win odds of RV \\(\\eta\\) against the RV \\(\\xi\\)\n\\[\\begin{align*}\r\\omega=\\frac{P(\\eta\u0026gt;\\xi)}{P(\\eta\u0026lt;\\xi)}=\\frac{\\theta}{1-\\theta}.\r\\end{align*}\\]\nThe Mann-Whitney estimate of the win probability can be transformed by the function \\(f(x)=\\frac{x}{1-x},\\ \\ x\\in(0,1)\\) to get an estimate for the win odds. Using the same transformation and the asymptotic normality of the Mann-Whitney estimate it is possible to construct asymptotic confidence intervals for the win odds, for a given asymptotic confidence level.\nIf \\(\\omega=1\\) then the random variables \\(\\xi\\) and \\(\\eta\\) are stochastically equivalent, while \\(\\omega\u0026gt;1\\) means that \\(\\eta\\) is stochastically greater than (wins against) \\(\\eta.\\) The case \\(\\omega\u0026lt;1\\) means that \\(\\eta\\) loses against \\(\\xi\\). The asymptotic confidence interval of the win odds can be used to test the hypothesis whether \\(\\omega=1\\).\nTo use the asymptotic normality result described above we need to estimate the asymptotic variance. The package sanon in R allows to estimate the asymptotic standard error of the Mann-Whitney estimate.\n\rThe package sanon\r#install.packages(\u0026quot;sanon\u0026quot;)\rlibrary(sanon)\rThe dataset resp contains data from a randomized clinical trial to compare a test treatment to placebo for a respiratory disorder.\ndata(resp, package = \u0026quot;sanon\u0026quot;)\rhead(resp)\r## center treatment sex age baseline visit1 visit2 visit3 visit4\r## 1 1 A F 32 1 2 2 4 2\r## 2 2 A F 37 1 3 4 4 4\r## 3 1 A F 47 2 2 3 4 4\r## 4 2 A F 39 2 3 4 4 4\r## 5 1 A M 11 4 4 4 4 2\r## 6 2 A F 60 4 4 3 3 4\rThe column visit4 is a numeric vector for patient global ratings of symptom control according to 5 categories (4 = excellent, 3 = good, 2 = fair, 1 = poor, 0 = terrible), measured at visit 4. To compare the effect of active treatment against the placebo we will use the win probability, which, as we defined previously, is an unknown theoretical quantity. The null hypothesis is that there is no treatment difference which can be written in terms of the win probability as \\(\\theta=0.5.\\) The Mann-Whitney estimate of the win probability can be calculated as follows\nfit \u0026lt;- sanon(visit4 ~ grp(treatment, ref=\u0026quot;P\u0026quot;), data = resp)\rfit\r## Call:\r## sanon.formula(formula = visit4 ~ grp(treatment, ref = \u0026quot;P\u0026quot;), data = resp)\r## ## Sample size: 111\r## ## Response levels:\r## [visit4; 5 levels] (lower) 0, 1, 2, 3, 4 (higher)\r## ## Design Matrix:\r## [,1]\r## visit4 1\r## ## Mann-Whitney Estimate ## for comparison [ A / P ] :\r## visit4 ## 0.6174\rconfint(fit)\r## M-W Estimate and 95% Confidence Intervals ## :\r## Estimate Lower Upper\r## visit4 0.6174 0.5173 0.7176\rfit$p\r## [,1]\r## [1,] 0.02150601\rThe p-value based on the asymptotic confidence interval of level 0.05 is less than 0.05, hence the null hypothesis of no treatment difference is rejected. The Mann-Whitney estimate can be transformed to get an estimate for the win odds.\nconfint(fit)$ci/(1-confint(fit)$ci)\r## Estimate Lower Upper\r## visit4 1.614013 1.071762 2.540746\rFor the win odds the null hypothesis of no treatment difference is \\(\\omega=1.\\) The win odds 1.61 characterizes the treatment effect difference.\n\r","date":1578636000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578636000,"objectID":"52dc476d8ecad85c0b199b6c4135de6e","permalink":"https://gasparyan.co/post/wrci/2020-01-10-r-rmarkdown/","publishdate":"2020-01-10T10:00:00+04:00","relpermalink":"/post/wrci/2020-01-10-r-rmarkdown/","section":"post","summary":"Mann-Whitney estimate for the win probability\rConsider two independent, continuous RVs (random variables) \\(\\xi\\) and \\(\\eta\\). The following probability is called the win probability of RV \\(\\eta\\) against the RV \\(\\xi\\)","tags":["R","Statistics","simulations","sanon"],"title":"Win Odds Confidence Intervals in R","type":"post"},{"authors":null,"categories":["R","Statisics"],"content":"\r\rIntroduction\rThis is a short reminder of some simple properties of exponential distributions.\nThe continuous random variable (RV) \\(\\xi\\) has an exponential distribution with the rate \\(\\lambda\u0026gt;0\\) if its CMD (cumulative distribution function) has the following form\n\\[F_\\xi(x)=P(\\xi\u0026lt;x)=\\left\\{\\begin{matrix}\r\u0026amp;1-e^{-\\lambda x}, \u0026amp;x\\geq 0.\\\\\r\u0026amp;0 \u0026amp;x\u0026lt;0.\r\\end{matrix}\\right.\r\\]\rThis entails that an exponential RV is with 1 probability positive and has the PDF (probability density function)\r\\[f_\\xi(x)=F_\\xi\u0026#39;(x)=\\left\\{\\begin{matrix}\r\u0026amp;\\lambda e^{-\\lambda x}, \u0026amp;x\u0026gt;0,\\\\\r\u0026amp;0 \u0026amp;x\u0026lt;0.\r\\end{matrix}\\right.\r\\]\rIf \\(\\xi\\) has an exponential distribution with the rate \\(\\lambda\\) we will denote this by \\({\\mathbb E}(\\lambda).\\)\nIn \\(R\\) the following functions can be used to, correspondingly, generate numbers from \\({\\mathbb E}(\\lambda)\\), calculate values of CDF, calculate values of PDF\nrexp(n=2, rate=1)\r## [1] 0.06697548 0.25507292\rpexp(q=1, rate=1)\r## [1] 0.6321206\rdexp(x=1, rate=1)\r## [1] 0.3678794\rThe histogram is a non-parametric estimator for the PDF. Hence we can simulate data from an exponential distribution and show that the histogram based on the data fits the PDF. Consider the case of \\({\\mathbb E}(2)\\) and simulate a sample of size \\(n=10000.\\)\nrm(list=ls())\rn \u0026lt;- 10000\rlambda \u0026lt;- 2\rX \u0026lt;- rexp(n = n, rate = lambda)\rint \u0026lt;- seq(0, max(X), max(X)/100)\rhist(X, freq = FALSE, nclass = 50, col = \u0026quot;lightblue\u0026quot;, border = \u0026quot;blue\u0026quot;, ylim = c(0, 2), main = \u0026quot;\u0026quot;)\rlines(int, dexp(int, rate = lambda), col = \u0026quot;red\u0026quot;, lty = 2, lwd = 2)\rThis is a very useful technique to check whether the RVs have the given PDF. We will use this technique below.\n\rExample 1\rSuppose \\(\\xi\\sim {\\mathbb E}(\\lambda)\\) and \\(\\eta\\sim {\\mathbb E}(\\mu)\\) are independent. Calculate the PDF of the RV \\(\\zeta=\\eta-\\xi.\\)\nFirst consider the case of \\(z\u0026gt;0.\\) Since \\(\\xi\\) and \\(\\eta\\) are independent, then the joint PDF of these RVs will be the product of individual PDFs, that is \\(f_{(\\eta,\\xi)}(x,y)=f_{\\xi}(x)f_{\\eta}(y).\\) Therefore,\n\\[F_\\zeta(z)=P(\\eta-\\xi\u0026lt;z)=\\int_0^{+\\infty}\\int_0^{+\\infty}I_{\\{y-x\\leq z\\}}(x,y)\\lambda\\mu e^{-\\lambda x-\\mu y}d x dy.\\]\rMaking the following variable change \\(u=y-x\\) will give\r\\[F_\\zeta(z)=P(\\eta-\\xi\u0026lt;z)=\\lambda\\mu \\int_0^{+\\infty}e^{-(\\lambda + \\mu) x}\\left(\\int_{-x}^{z}e^{-\\mu u}d u\\right) dx=1-\\frac{\\lambda}{\\lambda+\\mu}e^{-\\mu z},\\ \\ z\u0026gt;0.\\]\rIf \\(z\u0026lt;0\\) then\r\\[P(\\eta-\\xi\u0026lt;z)=P(\\xi-\\eta\u0026gt;-z)=1-P(\\xi-\\eta\u0026lt;-z)=1-\\left(1-\\frac{\\mu}{\\mu+\\lambda}e^{\\lambda z}\\right)=\\frac{\\mu}{\\lambda+\\mu}e^{\\lambda z},\\ \\ z\u0026lt;0.\\]\rTherefore,\r\\[F_\\zeta(z)=\\left\\{\\begin{matrix}\r\u0026amp;1-\\frac{\\lambda}{\\lambda+\\mu} e^{-\\mu z}, \u0026amp;z\\geq 0,\\\\\r\u0026amp;\\frac{\\mu}{\\lambda+\\mu} e^{\\lambda z} \u0026amp;z\u0026lt;0.\r\\end{matrix}\\right.\\]\rThe PDF will be\r\\[f_\\zeta(z)=\\frac{\\lambda\\mu}{\\lambda+\\mu}\\left\\{\\begin{matrix}\r\u0026amp;e^{-\\mu z}, \u0026amp;z\\geq 0,\\\\\r\u0026amp;e^{\\lambda z} \u0026amp;z\u0026lt;0.\r\\end{matrix}\\right.\\]\rThis formula can be checked using simulations.\nrm(list=ls())\rn \u0026lt;- 20000\rm \u0026lt;- 10000\rlambda \u0026lt;- 2\rmu \u0026lt;- 5\rX \u0026lt;- rexp(n,lambda)\rY \u0026lt;- rexp(m, mu)\rZ \u0026lt;- Y-X\rint \u0026lt;- seq(min(Z),max(Z),(max(Z)-min(Z))/100)\rdens \u0026lt;- function(z) (lambda*mu)/(lambda+mu)*ifelse(z\u0026gt;=0,exp(-mu*z),exp(lambda*z))\rhist(Z, nclass = 100, freq=FALSE,ylim=c(0,1.5),\rxlim = c(min(Z),max(Z)), main=\u0026quot;\u0026quot;, col = \u0026quot;lightblue\u0026quot;, border = \u0026quot;blue\u0026quot;)\rlines(int,dens(int), col=\u0026quot;2\u0026quot;)\rFrom the PDF we can calculate also\r\\[E(\\zeta)=\\frac{\\lambda-\\mu}{\\lambda\\mu},\\ \\ P(\\eta\u0026gt;\\xi)=\\frac{\\lambda}{\\lambda+\\mu}.\\]\n\rExample 2\rSuppose that \\(\\eta\\sim{\\mathbb E}(\\mu)\\) and \\(\\eta\u0026#39;\\sim{\\mathbb E}(\\mu\u0026#39;)\\) are independent. Find the distribution of the RVs \\(\\min(\\eta,\\eta\u0026#39;)\\) and \\(\\max(\\eta,\\eta\u0026#39;).\\)\nFor all \\(z\u0026gt;0\\) we have\r\\[\\begin{align*}\rP(\\min(\\eta,\\eta\u0026#39;)\u0026lt;z)=1-P(\\min(\\eta,\\eta\u0026#39;)\\geq z)=1-P(\\eta\\geq z,\\eta\u0026#39;\\geq z)=1-e^{-(\\mu+\\mu\u0026#39;) z},\\,z\u0026gt;0,\r\\end{align*}\\]\rand \\(P(\\min(\\eta,\\eta\u0026#39;)\u0026lt;z)=0,\\ \\ z\\leq 0.\\) Therefore, \\(\\min(\\eta,\\eta\u0026#39;)\\sim{\\mathbb E}(\\mu+\\mu\u0026#39;)\\), that is, the minimum of two exponentially distributed RVs is an exponentially distributed RV as well, with the rate being equal to the sum of the rates of the given two RVs.\nOn the other hand,\r\\[P(\\max(\\eta, \\eta\u0026#39;) \u0026lt; z)= P(\\eta \u0026lt; z, \\eta\u0026#39; \u0026lt;z)=(1-e^{-\\mu z})(1-e^{-\\mu\u0026#39; z}),\\,z\u0026gt;0.\\]\rFor \\(\\mu=\\mu\u0026#39;\\) we have\n\\[P(\\max(\\eta, \\eta\u0026#39;) \u0026lt; z) = (1-e^{-\\mu z})^2,\\,z\u0026gt;0,\\]\rwith PDF being equal to \\(f(z)=2\\mu(1-e^{-\\mu z})e^{-\\mu z},\\,z\u0026gt;0\\) and \\(f(z)=0,\\,z\u0026lt;0.\\)\n\rExample 3\rFor given three independent RVs such that \\(\\xi\\sim{\\mathbb E}(\\lambda)\\) and \\(\\eta,\\eta\u0026#39;\\sim{\\mathbb E}(\\mu),\\) calculate the following probability\r\\(\\theta=P(\\xi\u0026lt;\\eta,\\xi\u0026lt;\\eta\u0026#39;).\\)\nWe can reformulate this problem as follows\r\\[\\theta=P(\\xi\u0026lt;\\eta,\\xi\u0026lt;\\eta\u0026#39;)=P(\\xi\u0026lt;\\min(\\eta,\\eta\u0026#39;))=P(\\xi-\\min(\\eta,\\eta\u0026#39;)\u0026lt;0).\\]\rHence, denoting by \\(\\zeta=\\xi-\\min(\\eta,\\eta\u0026#39;),\\) in the first step we can calculate the distribution function of the RV \\(\\zeta.\\) We have \\(\\xi\\sim{\\mathbb E}(\\xi)\\) and \\(\\min(\\eta,\\eta\u0026#39;)\\sim{\\mathbb E}(2\\mu)\\) (see Example 2), hence, using the Example 1 we obtain\n\\[F_\\zeta(z)=\\left\\{\\begin{matrix}\r\u0026amp;1-\\frac{2\\mu}{\\lambda+2\\mu} e^{-\\lambda z}, \u0026amp;z\\geq 0,\\\\\r\u0026amp;\\frac{\\lambda}{\\lambda+2\\mu} e^{2\\mu z} \u0026amp;z\u0026lt;0.\r\\end{matrix}\\right.\\]\rFor the PDF we have\r\\[f_\\zeta(z)=\\left\\{\\begin{matrix}\r\u0026amp;\\frac{2\\lambda\\mu}{\\lambda+2\\mu} e^{-\\lambda z}, \u0026amp;z\u0026gt; 0,\\\\\r\u0026amp;\\frac{2\\lambda\\mu}{\\lambda+2\\mu} e^{2\\mu z} \u0026amp;z\u0026lt;0.\r\\end{matrix}\\right.\\]\rWe can, again, check this using simulations.\nrm(list=ls())\rn \u0026lt;- 200000\rm \u0026lt;- 100000\rlambda \u0026lt;- 2.5\rmu \u0026lt;- 1.3\rX1 \u0026lt;- rexp(n,mu)\rX2 \u0026lt;- rexp(n,mu)\rY \u0026lt;- rexp(m, lambda)\rZ \u0026lt;- Y - ifelse(X1\u0026gt;=X2,X2,X1)\rCoeff \u0026lt;- 2*lambda*mu/(lambda+2*mu)\rint \u0026lt;- seq(min(Z), max(Z), (max(Z)-min(Z))/100)\rdens \u0026lt;- function(z) Coeff*ifelse(z\u0026gt;=0, exp(-lambda*z), exp(2*mu*z))\rhist(Z, nclass = 100, freq=FALSE, ylim=c(0,1.5), xlim=c(min(Z), max(Z)), main = \u0026quot;\u0026quot;, border = \u0026quot;blue\u0026quot;, col = \u0026quot;lightblue\u0026quot;)\rlines(int,dens(int), col=\u0026quot;2\u0026quot;)\rTherefore,\r\\[\\theta=F_\\zeta(0)=\\frac{\\lambda}{\\lambda+2\\mu}.\\]\n\rExample 4\rFor given three independent RVs such that \\(\\xi,\\xi\u0026#39;\\sim{\\mathbb E}(\\lambda)\\) and \\(\\eta\\sim{\\mathbb E}(\\mu),\\) calculate the following probability\r\\(\\vartheta=P(\\xi\u0026lt;\\eta,\\xi\u0026#39;\u0026lt;\\eta).\\)\nWe can reformulate this problem as follows\r\\[\\vartheta=P(\\xi\u0026lt;\\eta,\\xi\u0026#39;\u0026lt;\\eta)=P(\\max(\\xi,\\xi\u0026#39;)\u0026lt;\\eta)=P(\\eta-\\max(\\xi,\\xi\u0026#39;)\u0026gt;0).\\]\rHence, denoting by \\(\\kappa=\\eta-\\max(\\xi,\\xi\u0026#39;),\\) in the first step we can calculate the distribution function of the RV \\(\\kappa.\\)\n\\[\\begin{align*}\rP(\\kappa\u0026lt;z)=\\int_0^{+\\infty}\\int_0^{+\\infty}I(y-x\\leq z)2\\lambda\\mu(1-e^{-\\lambda x})e^{-\\lambda x-\\mu y}d x d y.\r\\end{align*}\\]\rIf \\(z\u0026gt;0\\) then, denoting \\(y-x=u\\) we get \\(y=u+x,\\,u\\in[-x,\\infty)\\ \\ d y= d u.\\) Therefore,\n\\[\\begin{align*}\rP(\\kappa\u0026lt;z)\u0026amp;=2\\lambda\\mu\\int_0^{+\\infty}(1-e^{-\\lambda x})e^{-\\lambda x-\\mu x}\\int_{-x}^{z}e^{-\\mu u}d u d x=\\\\\r\u0026amp;=-2\\lambda\\int_0^{+\\infty}(e^{-(\\lambda+\\mu) x -\\mu z}-e^{-(2\\lambda+\\mu) x -\\mu z}-e^{-\\lambda x }+e^{-2\\lambda x }) d x=\\\\\r\u0026amp;=1-\\frac{2\\lambda^2}{(\\lambda+\\mu)(2\\lambda+\\mu)}e^{-\\mu z },\\ \\ z\u0026gt;0.\r\\end{align*}\\]\rFor \\(z\u0026gt;0\\) calculate also (using the notation \\(x-y=u\\), which entails \\(x=y+u,\\,d x=d u,\\) where \\(u\\in[-y,+\\infty).\\))\r\\[\\begin{align*}\rP(\\max(\\xi,\\xi\u0026#39;)-\\eta\u0026lt;z)\u0026amp;=\\int_0^{+\\infty}\\int_0^{+\\infty}I(x-y\\leq z)2\\lambda\\mu(1-e^{-\\lambda x})e^{-\\lambda x-\\mu y}d x d y=\\\\\r\u0026amp;=\\int_0^{+\\infty}\\int_{-y}^{z}(e^{-\\lambda(y+u) - \\mu y} - e^{-2\\lambda(y+u) -\\mu y}d u dy)=\\\\\r\u0026amp;=1+\\mu\\left[\\frac{e^{-2\\lambda z}}{2\\lambda+\\mu}-\\frac{2e^{-\\lambda z}}{\\lambda+\\mu}\\right],\\ \\ z\\geq 0.\r\\end{align*}\\]\nTherefore, for \\(z\u0026gt;0\\) we have\r\\[\\begin{align*}\rP(\\kappa\u0026lt;-z)\u0026amp;=1-P(\\max(\\xi,\\xi\u0026#39;)-\\eta\u0026lt;z)=\\\\\r\u0026amp;=-\\mu\\left[\\frac{e^{-2\\lambda z}}{2\\lambda+\\mu}-\\frac{2e^{-\\lambda z}}{\\lambda+\\mu}\\right],\\ \\ z\\geq 0\r\\end{align*}\\]\rFinally we obtain\r\\[F_\\kappa(z)=\\left\\{\\begin{matrix}\r\u0026amp;1-\\frac{2\\lambda^2}{(\\lambda+\\mu)(2\\lambda+\\mu)} e^{-\\mu z}, \u0026amp;z\\geq 0,\\\\\r\u0026amp;-\\mu\\left[\\frac{e^{2\\lambda z}}{2\\lambda+\\mu}-\\frac{2e^{\\lambda z}}{\\lambda+\\mu}\\right] \u0026amp;z\u0026lt;0.\r\\end{matrix}\\right.\\]\rFor the PDF we have\r\\[f_\\kappa(z)=\\left\\{\\begin{matrix}\r\u0026amp;\\frac{2\\lambda^2\\mu}{(\\lambda+\\mu)(2\\lambda+\\mu)} e^{-\\mu z}, \u0026amp;z\u0026gt; 0,\\\\\r\u0026amp;-2\\lambda\\mu\\left[\\frac{e^{2\\lambda z}}{2\\lambda+\\mu}-\\frac{e^{\\lambda z}}{\\lambda+\\mu}\\right] \u0026amp;z\u0026lt;0.\r\\end{matrix}\\right.\\]\rTo check this formula we can make the following simulations.\nrm(list=ls())\rn \u0026lt;- 200000\rm \u0026lt;- 100000\rlambda \u0026lt;- 2.5\rmu \u0026lt;- 5.5\rX1 \u0026lt;- rexp(n,lambda)\rX2 \u0026lt;- rexp(n,lambda)\rY \u0026lt;- rexp(m, mu)\rZ \u0026lt;- Y - ifelse(X1 \u0026gt;= X2, X1, X2)\rCoeff \u0026lt;- 2*mu*lambda^2/((lambda+mu)*(2*lambda+mu))\rCoeff1 \u0026lt;- -2*lambda*mu/(2*lambda+mu)\rCoeff2 \u0026lt;- -2*lambda*mu/(lambda+mu)\rint \u0026lt;- seq(min(Z), max(Z), (max(Z)-min(Z))/100)\rdens \u0026lt;- function(z) ifelse(z\u0026gt;=0, Coeff*exp(-mu*z),\rCoeff1*exp(2*lambda*z)-Coeff2*exp(lambda*z))\rhist(Z, nclass = 100, freq=FALSE,ylim=c(0, 1.5), xlim=c(min(Z), max(Z)),\rmain = \u0026quot;\u0026quot;, col = \u0026quot;lightblue\u0026quot;, border = \u0026quot;blue\u0026quot;)\rlines(int,dens(int), col=\u0026quot;2\u0026quot;)\rTherefore,\r\\[\\vartheta=P(\\xi\u0026lt;\\eta, \\xi\u0026#39;\u0026lt;\\eta)=P(\\kappa\u0026gt;0)=\\frac{2\\lambda^2}{(\\lambda+\\mu)(2\\lambda+\\mu)}.\\]\n\r","date":1576821600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576821600,"objectID":"f027dbe70f94092bdf2734a4e52b7873","permalink":"https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown/","publishdate":"2019-12-20T10:00:00+04:00","relpermalink":"/post/exp/2019-12-20-r-rmarkdown/","section":"post","summary":"Introduction\rThis is a short reminder of some simple properties of exponential distributions.\nThe continuous random variable (RV) \\(\\xi\\) has an exponential distribution with the rate \\(\\lambda\u0026gt;0\\) if its CMD (cumulative distribution function) has the following form","tags":["R","Statistics","Simulations"],"title":"Exponential Distribution","type":"post"},{"authors":null,"categories":null,"content":"This project is dedicated to promoting Statistics and Data Science texts in Armenian language. Follow the Rmenia link above to access R programming lecture notes in Armenian. Follow the Armdown link above to access the configuration files which will help to produce Armenian texts using Markdown.\n","date":1576368000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576368000,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://gasparyan.co/project/internal-project/","publishdate":"2019-12-15T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"Producing Scientific Documents in Armenian.","tags":["Science in Armenian"],"title":"Armdown","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://gasparyan.co/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["Statistics","R"],"content":"\r\r\rIntroduction\rAbstract\r\rIn clinical research the dose–response relationship is often non-linear, therefore advanced fitting models are needed to capture their behavior. The talk will concentrate on fitting non-linear parametric models to the dose–response data and will explain some specificities of this problem.\n\rThe talk is based on the book by Christian Ritz, Jens Carl Streibig “Nonlinear regression with R”.\n\r\r\r\rPurpose of Dose-Response Information\rICH E4 Harmonized Tripartite Guideline\rKnowledge of the relationships among dose, drug-concentration in blood, and clinical\rresponse (effectiveness and undesirable effects) is important for the safe and effective\ruse of drugs in individual patients. It helps identify\n\ran appropriate starting dose,\n\rthe best way to adjust dosage to the needs of a particular patient,\n\ra dose beyond which increases would be unlikely to provide added benefit or would\rproduce unacceptable side effects.\n\r\r\rExample\rThere are other fields of studies were concentration analysis can be used.\nConsider the following relationship (Chwirut2 is included in the package NISTnls),\rwhere the response variable is ultrasonic response, and the predictor variable is metal distance.\n#library(NISTnls)\rdim(Chwirut2)\r## [1] 54 2\rhead(Chwirut2)\r## y x\r## 1 92.9000 0.500\r## 2 57.1000 1.000\r## 3 31.0500 1.750\r## 4 11.5875 3.750\r## 5 8.0250 5.750\r## 6 63.6000 0.875\rA distance-amplitude relationship of attenuation that an ultrasound beam experiences traveling through a medium\n#library(ggplot2)\rg \u0026lt;- ggplot(data=Chwirut2,aes(x=x,y=y)) + geom_point() + xlab(\u0026quot;Metal distance\u0026quot;) + ylab(\u0026quot;Ultrasonic respons\u0026quot;)\rg\r\r\rThe built-in function nls()\rFitting a parametric family\rSuppose we observe the following pair of data points \\((x_i,y_i),\\,i=1,\\cdots,n.\\) Consider a parametric family (usually non-linear) that we want to fit to the data\r\\[f(x,(\\theta_1,\\cdots,\\theta_k)).\\]\rThe hypothesis is that there is the following relationship between observed points\n\\[y_i=f(x_i,(\\theta_1,\\cdots,\\theta_k))+\\varepsilon_i,\\ \\ i=1,\\cdots,n.\\]\nThe fitting method that we are going to use is the Non-linear Least Squares (NLS) method. NLS estimates can be found using this minimization problem.\n\\[(\\hat\\theta_1,\\cdots,\\hat\\theta_k)=\\arg\\min_{\\theta_1,\\cdots,\\theta_k}\\sum_{i=1}^n(y_i-f(x_i,(\\theta_1,\\cdots,\\theta_k)))^2.\\]\n\rGauss - Newton algorithm\rThe Gauss - Newton algorithm is used to solve the optimization problem described above.\nIf we denote \\(g_i(\\theta)=y_i-f(x_i,\\theta), \\ \\ \\theta=(\\theta_1,\\cdots,\\theta_k),\\)\rthen the minimization problem is the following\r\\[\\min_{\\theta\\in R^k}\\sum_{i=1}^ng_i^2(\\theta)=\\min_{\\theta\\in R^k}||g(\\theta)||^2.\\]\rChoose an initial value \\(\\theta_0\\) and linearize the function \\(g(\\theta)\\) around \\(\\theta_0\\) as follows\n\\[\\hat g(\\theta,\\theta_0)=g(\\theta_0)+Dg(\\theta_0)(\\theta-\\theta_0).\\]\rSolving \\(\\min_{\\theta\\in R^k}||g(\\theta_0)+Dg(\\theta_0)(\\theta-\\theta_0)||^2,\\) where \\(m\\times n\\) matrix \\(Dg(\\theta)\\) is the Jacobian, we get \\(\\theta_1\\) and continuing this process interatively, we get the sequence \\((\\theta_m),\\,m=0,1,\\cdots.\\) Moreover, if \\(Dg(\\theta_k)\\) has linearly independent columns,\n\\[\\theta_{k+1}=\\theta_k-\\left([Dg(\\theta_k)]^TDg(\\theta_k)\\right)^{-1}[Dg(\\theta_k)]^Tg(\\theta_k).\\]\rThe question is - how to choose the initial value \\(\\theta_0\\) of iteration?\n\rThe parametric family\rThe following parametric family was suggested to fit the distance-amplitude relationship\n\\[f(x,(\\beta_1,\\beta_2,\\beta_3))=\\frac{\\exp(-\\beta_1x)}{\\beta_2+\\beta_3x}\\]\rAssumptions:\ncorrect mean function \\(f\\)\n\rvariance homogeneity (homoscedasticity)\n\rnormally distributed measurements errors\n\rmutually independent measurement errors \\(\\varepsilon_i\\)\n\r\r\rIf we take \\(x=0\\) then we get \\(f(0,(\\beta_1,\\beta_2,\\beta_3))=\\frac{1}{\\beta_2}\\) so the initial estimate of \\(\\beta_2\\) can be the reciprocal of the response value closest to the \\(y\\) axis, that is, roughly, \\(\\frac{1}{100}=0.01.\\) For the\r\r\rFitting non-linear parametric family\rexpFct \u0026lt;- function(x,beta1,beta2=0.01,beta3) exp(-beta1*x)/(beta2+beta3*x)\rl \u0026lt;- list(beta1=0.1, beta2=0.01, beta3=0.001)\rfit \u0026lt;- nls(data=Chwirut2, y ~ expFct(x, beta1, beta2, beta3), start=l)\rfit\r## Nonlinear regression model\r## model: y ~ expFct(x, beta1, beta2, beta3)\r## data: Chwirut2\r## beta1 beta2 beta3 ## 0.166576 0.005165 0.012150 ## residual sum-of-squares: 513\r## ## Number of iterations to convergence: 9 ## Achieved convergence tolerance: 7.467e-07\rtidy(fit)\r## # A tibble: 3 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 beta1 0.167 0.0383 4.35 6.56e- 5\r## 2 beta2 0.00517 0.000666 7.75 3.54e-10\r## 3 beta3 0.0122 0.00153 7.94 1.81e-10\r\rPlotting the fitted non-linear function\rHence, we can plot the fitted curve\n#library(broom)\rvalues \u0026lt;- tidy(fit)$estimate\rg + stat_function(fun=expFct, args=values, colour=\u0026quot;blue\u0026quot;, lwd=1)\r\r\rThe package nls2\rSearching a grid\rThe procedure of finding initial points for the minimization iteration can be automated in the following way. In case the range of the parameter estimates can be envisaged grid search can be carried out:\n\r\rthe residual sums-of-squares \\(RSS(\\beta),\\,\\beta=(\\beta_1,\\cdots,\\beta_k)\\) is calculated for all the values in the intervals\r\r\r\r\rstarting values are chosen in the way to provide the smallest value of \\(RSS(\\beta)\\).\r\r\rIn our example, having \\(\\beta_2=0.01,\\) for other two parameters, knowing only that they are positive numbers, we can search in the interval \\([0.1,1]\\) taking the step size equal to \\(0.1.\\)\nCreate a data.frame containing all possible combinations of suggested initial values for the parameters\nbeta1 \u0026lt;- seq(0.1, 1, by = 0.1)\rbeta2 \u0026lt;- 0.01\rbeta3 \u0026lt;- seq(0.1, 1, by = 0.1)\rgrid.Chwirut2 \u0026lt;- expand.grid(list(beta1 = beta1, beta2 = beta2, beta3 = beta3))\rhead(grid.Chwirut2)\r## beta1 beta2 beta3\r## 1 0.1 0.01 0.1\r## 2 0.2 0.01 0.1\r## 3 0.3 0.01 0.1\r## 4 0.4 0.01 0.1\r## 5 0.5 0.01 0.1\r## 6 0.6 0.01 0.1\r\rThe nls2() function\rnls2() function differs from the original function by a way that it provides the possibility to specify a data frame of values of starting points. Its argument algorithm=“brute-force” evaluates RSS for the parameter values provided through the start argument.\nlibrary(nls2)\rfit2 \u0026lt;- nls2(data = Chwirut2, y ~ expFct(x, beta1, beta2, beta3), start = grid.Chwirut2, algorithm = \u0026quot;brute-force\u0026quot;)\rfit2\r## Nonlinear regression model\r## model: y ~ expFct(x, beta1, beta2, beta3)\r## data: Chwirut2\r## beta1 beta2 beta3 ## 0.10 0.01 0.10 ## residual sum-of-squares: 60696\r## ## Number of iterations to convergence: 100 ## Achieved convergence tolerance: NA\r\rFitting using the function nls2()\rFitting procedure can be continued this way: replace the value of the start argument by fit2 and leave out the argument algorithm\ntidy(nls2(data = Chwirut2, y ~ expFct(x, beta1, beta2, beta3), start=fit2))\r## # A tibble: 3 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 beta1 0.167 0.0383 4.35 6.56e- 5\r## 2 beta2 0.00517 0.000666 7.75 3.54e-10\r## 3 beta3 0.0121 0.00153 7.94 1.81e-10\r\r\rSelf-Starter functions\rAutomating further the search for initial points\r\r\rSelf-starter functions are special type of functions. They are implementations of specific given mean functions and designed in a way to calculate starting values for a given dataset. They can be thought as the definition of the parametric family combined with the logic of how to choose initial values based on the given dataset.\r\r\r\r\rWe have already encountered such example: for the parametric family\r\\[f(x,(\\beta_1,\\beta_2,\\beta_3))=\\frac{\\exp(-\\beta_1x)}{\\beta_2+\\beta_3x}\\]\rwe said that the initial estimate of \\(\\beta_2\\) can be the reciprocal of the response value closest to the \\(y\\) axis. So, if we find also logic of choosing initial values for \\(\\beta_1\\) and \\(\\beta_3,\\) then we can add these logics into the definition of this parametric family and get a Self-Starter function.\r\r\r\r\rWe will construct a Self-Starter function later, but before that we will explore the built-in Self-Starter functions.\r\r\r\rBuilt-in Self-Starters\rConsider the following dataset\ndat \u0026lt;- data.frame(\rconc = c(2.856829, 5.005303, 7.519473, 22.101664, 27.769976, 39.198025, 45.483269, 203.784238),\rrate = c(14.58342, 24.74123, 31.34551, 72.96985, 77.50099, 96.08794, 96.96624, 108.88374)\r)\rdat\r## conc rate\r## 1 2.856829 14.58342\r## 2 5.005303 24.74123\r## 3 7.519473 31.34551\r## 4 22.101664 72.96985\r## 5 27.769976 77.50099\r## 6 39.198025 96.08794\r## 7 45.483269 96.96624\r## 8 203.784238 108.88374\rg \u0026lt;- ggplot(data = dat, aes(x = conc, y = rate)) + geom_point()\rg\r\rThe Michaelis-Menten model\rWe are going to fit this data using the parametric family of Michaelis-Menten functions\n\\[f(x,(V_m,k))=\\frac{V_m x}{K+x}.\\]\n\r\rHere the constants \\(V_m\\) and \\(K\\) are positive. \\(V_{m}\\) represents the maximum rate achieved by the system, at saturating substrate concentration. The constant \\(K\\) is the substrate concentration at which the reaction rate is half of \\(V_m.\\)\r\r\r\r\rAgain we can interactively find the initial estimates for the parameters to fit the NLS curve. But there is a built-in Self-Starter function called SSmicmen() which has a logic inscribed in it which lets it work with nls() without specifying starting points.\r\r\rfit3 \u0026lt;- nls(rate ~ SSmicmen(conc, Vm, K), data = dat)\rtidy(fit3)\r## # A tibble: 2 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Vm 126. 7.17 17.6 0.00000218\r## 2 K 17.1 2.95 5.78 0.00117\rg + stat_function(fun = SSmicmen, args = list(Vm = tidy(fit3)$estimate[1],\rK=tidy(fit3)$estimate[2]), colour = \u0026quot;darkgreen\u0026quot;)\r\r\rDefining Self-Starter functions\rA simple example\rConsider the following parametric family\n\\[f(x,(b,y_0))=y_0 e^{\\frac{x}{b}}.\\]\n\r\rThis model can be used to describe radioactive decay. The parameter \\(y_0\\) is\rthe initial amount of the radioactive substance (at time \\(x = 0\\)). The rate of\rdecay is governed by the second parameter \\(b\\) (the inverse decay constant).\r\r\r\r\rThe definition of a self-starter contains the formula of the parametric family, a logic of constructing initial values for parameters, a function taking the dataset as argument and other arguments ensuring that the logic of construction of initial values, the definition of parametric family and nls() are tied together.\r\r\r\rThe logic of construction of initial values\rApply the log transform on the given mean function\n\\[\\log f(x,(b,y_0))=\\log y_0+\\frac{x}{b}.\\]\nHence, if we apply the log transform on the response data, we can apply linear regression techniques to estimate the slope and the intercept, then, from the equalities\n\\[\\log\\tilde y_0={\\beta_0}, \\ \\ \\tilde \\beta_1=\\frac{1}{b}\\]\rwe get\n\\[\\tilde y_0=e^{\\tilde \\beta_0}, \\ \\ \\tilde b=\\frac{1}{\\tilde\\beta_1}.\\]\rThis is the logic of construction of initial parameter values that we need to include in the definition of a Self-Starter.\n(We are not interested in the error structure that will change after the transformation, because we need only initial estimates.)\n\rThe match.call() function\rThe function match.call() can be used in the definition of the function to store the call in the resulting object. In addition, if that function is called without specifying the arguments, the function match.call() matches the arguments to their names by their positions. Observe below\nFunc \u0026lt;- function(input, parameter){\rvalue \u0026lt;- input^parameter\rattr(value,\u0026quot;call\u0026quot;) \u0026lt;- match.call()\rvalue\r}\rpower \u0026lt;- Func(2,16) #observe that we did not specify the names of parameters\rpower\r## [1] 65536\r## attr(,\u0026quot;call\u0026quot;)\r## Func(input = 2, parameter = 16)\rHence, their argument values can be extracted from the call by their names as from a list\nattr(power,\u0026quot;call\u0026quot;)$parameter\r## [1] 16\r\rThree steps\r\r\rDefine the mean function\r\r\rexpModel \u0026lt;- function(predictor,b, y0) {\ry0 * exp(predictor/b)\r}\r\r\rDefine the function with the initialization logic\r\r\rexpModelInit \u0026lt;- function(mCall, LHS, data) {\rxy \u0026lt;- sortedXyData(mCall[[\u0026quot;predictor\u0026quot;]], LHS, data)\rlmFit \u0026lt;- lm(log(xy[, \u0026quot;y\u0026quot;]) ~ xy[, \u0026quot;x\u0026quot;])\rcoefs \u0026lt;- coef(lmFit)\ry0 \u0026lt;- exp(coefs[1])\rb \u0026lt;- 1/coefs[2]\rvalue \u0026lt;- c(b, y0)\rnames(value) \u0026lt;- c(\u0026quot;b\u0026quot;,\u0026quot;y0\u0026quot;)\rvalue\r}\r\r\rDefine the Self-Starter function\rSpecificities of working in a Markdown Environment\rSSexp \u0026lt;- selfStart(expModel, expModelInit, c(\u0026quot;b\u0026quot;, \u0026quot;y0\u0026quot;))\rclass(SSexp)\r## [1] \u0026quot;selfStart\u0026quot;\rRStudio actually creates a separate R session to render the document. This causes problem with the getInitial() function which searches for user created SS functions in the Global environment and cannot find them because they are created in a new environment only for the Markdown use. Hence, we need to ensure that our created Self-Starter is defined in the Global environment.\n#Only for Markdown execution\rl \u0026lt;- list(SSexp = SSexp)\rlist2env(l, envir = .GlobalEnv)\r## \u0026lt;environment: R_GlobalEnv\u0026gt;\r\rGetting the inital values\rSelf-starters can be passed as an argument to the function getInitial() to obtain the initial values. Consider the following dataset\nload(file = \u0026quot;RGRcurve.rda\u0026quot;)\rhead(RGRcurve)\r## Day RGR\r## 1 0 0.169\r## 2 0 0.164\r## 3 0 0.210\r## 4 0 0.215\r## 5 0 0.183\r## 6 0 0.181\rThen, the initial values, determined according to our logic, will be\ngetInitial(RGR ~ SSexp(Day, b,y0), data = RGRcurve)\r## b y0 ## 3.8450187 0.1674235\r\rFitting by user defined SS\rfit4 \u0026lt;- nls(RGR ~ SSexp(Day, b,y0), data = RGRcurve)\rgetInitial(RGR ~ SSexp(Day, b,y0), data = RGRcurve)\r## b y0 ## 3.8450187 0.1674235\r# Compare the initial values with the estimated values\rtidy(fit4)\r## # A tibble: 2 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 b 3.76 0.175 21.5 3.40e-23\r## 2 y0 0.164 0.0142 11.5 4.03e-14\rg \u0026lt;- ggplot(data = RGRcurve, aes(x = Day, y = RGR)) + geom_point()\rg + stat_function(fun = SSexp, args=tidy(fit4)$estimate, colour=\u0026quot;darkgreen\u0026quot;)\r\r\rDifficult Example\rReturning to the difficult example\rConsidering once again the parametric family\n\\[f(x,\\beta)=\\frac{\\exp(-\\beta_1x)}{\\beta_2+\\beta_3x},\\ \\ \\beta=(\\beta_1,\\beta_2,\\beta_3).\\]\rWe can use Taylor’s formula\n\\[f(x,\\beta)\\approx f(0,\\beta)+f\u0026#39;(0,\\beta)x,\\ \\ x\\approx 0.\\]\rSimplifying this and remembering that \\(f(0,\\beta)=\\frac{1}{\\beta_2}\\) we get\r\\[f(x,\\beta)\\approx \\frac{1}{\\beta_2}-\\frac{1}{\\beta_2}\\left(\\beta_1+\\frac{\\beta_3}{\\beta_2}\\right)x.\\]\rWhich in turn gives the equality\r\\[1-\\beta_2 f(x,\\beta)=\\left(\\beta_1+\\frac{\\beta_3}{\\beta_2}\\right)x,\\]\rhence, transforming the data according to the left hand side and fitting a linear model without an intercept we can estimate \\(\\left(\\beta_1+\\frac{\\beta_3}{\\beta_2}\\right).\\) For two parameters out of three we have a rule to find starting values, for the third one we still need a guess. Taking \\(\\beta_1=0\\) may work.\n\r\rexpFctInit \u0026lt;- function(mCall, LHS, data){\rxy \u0026lt;- sortedXyData(mCall[[\u0026quot;x\u0026quot;]], LHS, data)\rbeta2 \u0026lt;- 1/xy[1,\u0026quot;y\u0026quot;]\rcoefs \u0026lt;- coef(lm(1-beta2*xy[,\u0026quot;y\u0026quot;]~xy[,\u0026quot;x\u0026quot;]+0))\rbeta1 \u0026lt;- 0\rbeta3 \u0026lt;- coefs*beta2\rvalue \u0026lt;- c(beta1, beta2, beta3)\rnames(value) \u0026lt;- c(\u0026quot;beta1\u0026quot;, \u0026quot;beta2\u0026quot;, \u0026quot;beta3\u0026quot;)\rvalue\r}\rSSexpFct \u0026lt;- selfStart(expFct, expFctInit, c(\u0026quot;beta1\u0026quot;, \u0026quot;beta2\u0026quot;, \u0026quot;beta3\u0026quot;))\rl \u0026lt;- list(SSexpFct=SSexpFct)\rlist2env(l, envir = .GlobalEnv)#Only for Markdown execution\r## \u0026lt;environment: R_GlobalEnv\u0026gt;\rgetInitial(data = Chwirut2, y ~ SSexpFct(x, beta1, beta2, beta3))\r## beta1 beta2 beta3 ## 0.000000000 0.012140834 0.002537341\rfit0 \u0026lt;- nls(data = Chwirut2, y ~ SSexpFct(x, beta1, beta2, beta3))\rfit0_tidy \u0026lt;- tidy(fit0)\rfit0_tidy$p.value \u0026lt;- format.pval(fit0_tidy$p.value,eps=0.001,digits=3)\rCI \u0026lt;- as.data.frame(suppressMessages(confint(fit0)))\rCI$term \u0026lt;- row.names(CI)\rfit0_tidy \u0026lt;- merge(fit0_tidy,CI,by=\u0026quot;term\u0026quot;)\rfit0_tidy[,!names(fit0_tidy)%in%c(\u0026quot;term\u0026quot;, \u0026quot;p.value\u0026quot;)]\u0026lt;-\rround(fit0_tidy[,!names(fit0_tidy)%in%c(\u0026quot;term\u0026quot;, \u0026quot;p.value\u0026quot;)], 3)\rfit0_tidy\r## term estimate std.error statistic p.value 2.5% 97.5%\r## 1 beta1 0.167 0.038 4.349 \u0026lt;0.001 0.094 0.254\r## 2 beta2 0.005 0.001 7.753 \u0026lt;0.001 0.004 0.006\r## 3 beta3 0.012 0.002 7.939 \u0026lt;0.001 0.009 0.015\rHere, we used the generic function \\(confint()\\) on the object of class \\(nls\\) and got the confidence intervals for the parameters estimates. In addition, the function \\(format.pval()\\) was used to simplify the presentation of p.values.\nvalues \u0026lt;- tidy(fit0)$estimate\rg \u0026lt;- ggplot(data = Chwirut2, aes(x = x, y = y))+\rgeom_point() + xlab(\u0026quot;Metal distance\u0026quot;) + ylab(\u0026quot;Ultrasonic respons\u0026quot;)\rg + stat_function(fun = expFct, args = values,\rcolour=\u0026quot;blue\u0026quot;, lwd=1)\rpredict(fit0, newdata = data.frame(x=6:7))\r## [1] 4.715006 3.453957\r\rFurther Reading\r\rThe package drc (Dose-Response curves) contains much more prespecified parametric families (22, to be exact - run drc::getMeanFunctions() to see the list).\n\rThe package nlme (Linear and Nonlinear Mixed Effects Models) which allows the inclusion of the donor effect as a random effect in dose-response studies.\n\rMore robust algorithms for fitting (The Levenberg-Marquardt curve-fitting method)\n\r\r\rThe Levenberg-Marquardt method\r\rThe Levenberg-Marquardt curve-fitting method is a combination of the two other minimization methods: the gradient descent method and the Gauss-Newton method.\n\rIn the gradient descent method, the sum of the squared errors is reduced by updating the parameters in the steepest-descent direction.\n\rIn the Gauss-Newton method, the sum of the squared errors is reduced by assuming the least squares function is locally quadratic, and finding the minimum of the quadratic.\n\rThe Levenberg-Marquardt method acts more like a gradient-descent method when the parameters are far from their optimal value, and acts more like the Gauss-Newton method when the parameters are close to their optimal value.\n\rHenri P. Gavin, The Levenberg-Marquardt method for nonlinear least squares curve-fitting problems\n\r\r\r\rPresentation\rBelow is a presentation version of this lecture which contains also interactive applications for curve fitting.\n\r","date":1536814800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536814800,"objectID":"9706718afdff97222a4548e412309806","permalink":"https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown/","publishdate":"2018-09-13T09:00:00+04:00","relpermalink":"/post/drc/2018-03-19-r-rmarkdown/","section":"post","summary":"Introduction\rAbstract\r\rIn clinical research the dose–response relationship is often non-linear, therefore advanced fitting models are needed to capture their behavior. The talk will concentrate on fitting non-linear parametric models to the dose–response data and will explain some specificities of this problem.","tags":["drc"],"title":"Dose-Response Curves in R","type":"post"},{"authors":[],"categories":null,"content":"","date":1536789600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536789600,"objectID":"3069d3401342d4ccf0587deb17503b9a","permalink":"https://gasparyan.co/talk/aua2018/","publishdate":"2018-09-13T00:00:00+02:00","relpermalink":"/talk/aua2018/","section":"talk","summary":"","tags":["R","drc"],"title":"Dose Response Curves in R","type":"talk"},{"authors":null,"categories":null,"content":"","date":1534723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534723200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://gasparyan.co/project/external-project/","publishdate":"2018-08-20T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"EasyStat App is an `R` based FREE web application. This platform can help you to independently conduct research data analyses.","tags":["Reproducible Research"],"title":"EasySTAT Project","type":"project"},{"authors":null,"categories":["Statistics"],"content":"\r\r\rConsistent Estimators\rIn the estimation problem of a one-dimensional parameter \\(\\theta\\) from an i.i.d. sample \\[X^n=(X_1,\\cdots,X_n),\\ \\ X_i\\sim F(x,\\theta),\\,\\theta\\in\\Theta\\subset R\\]\rwe introduced the notion of consistency of an estimator.\rThis means that whatever the unknown value of \\(\\theta\\) is, this estimator is going to be close to that value in higher probability as \\(n\\) increases\r\\[\\hat\\theta_n\\approx\\theta, \\text{ for large } n.\\]\rNow our goal is to quantify this closeness.\n\rAsymptotically Normal Estimators\rWe say that the estimator \\(\\hat\\theta_n\\) is asymptotically normal if\r\\[\\frac{\\hat\\theta_n-\\theta}{\\sqrt{Var(\\hat\\theta_n)}}\\stackrel{d}{\\rightarrow}N (0,1),\\]\rwhich means that\r\\[P\\left(\\frac{\\hat\\theta_n-\\theta}{\\sqrt{Var(\\hat\\theta_n)}}\u0026lt;x\\right)\\rightarrow P(\\xi\u0026lt;x),\\,x\\in R,\\,\\xi\\sim N(0,1).\\]\n\rAsymptotically Normal Estimators (Example 1)\rSuppose that\n\\[X^n=(X_1,\\cdots,X_n),\\ \\ X_i\\sim N(\\theta,\\sigma^2),\\,\\theta\\in R,\\,\\sigma\u0026gt;0.\\]\rWe know that the estimator \\(\\hat\\theta_n=\\bar X_n=\\frac{1}{n}\\sum_{i=1}^nX_i\\) is an unbiased, consistent estimator for \\(\\theta.\\) By central limit theorem, as \\(n\\rightarrow+\\infty,\\)\r\\[\\sqrt{n}\\frac{\\bar X_n-\\theta}{\\sigma}\\stackrel{d}{\\rightarrow}N(0,1),\\]\rhence this estimator is also asymptotically normal. The convergence above can be written as (when \\(n\\rightarrow+\\infty\\))\n\\[\\sqrt{n}(\\bar X_n-\\theta)\\stackrel{d}{\\rightarrow}N(0,\\sigma^2).\\]\rHere \\(\\sqrt{n}\\) is rate of convergence of the estimator and \\(\\sigma^2\\) is the asymptotic variance.\rHence, higher the rate of convergence or smaller the asymptotic variance, better is the estimator.\n\rMethod of Moments\rConsider again the estimation problem of a one-dimensional parameter \\(\\theta\\) from an i.i.d. sample \\[X^n=(X_1,\\cdots,X_n),\\ \\ X_i\\sim F(x,\\theta),\\,\\theta\\in\\Theta\\subset R.\\]\rThe idea of the method of moments is based on the fact that we can estimate the mathematical expectation, that is, for any given function \\(g(\\cdot),\\) so that \\(E|g(X_1)|\u0026lt;+\\infty,\\)\rthe following convergence is true\r\\[\\begin{align*}\r\\frac{1}{n}\\sum_{j=1}^n g(X_j)\\stackrel{P}{\\rightarrow} E g(X_1).\r\\end{align*}\\]\rHence, for the estimation of the parameter \\(\\theta,\\) for some \\(g(\\cdot)\\) we can calculate \\(E_\\theta g(X_1)=T(\\theta).\\) That will ensure the convergence\r\\[\r\\frac{1}{n}\\sum_{j=1}^n g(X_j)\\stackrel{P}{\\rightarrow}T(\\theta).\r\\]\n\rMethod of Moments (Page 2)\rTherefore, if the function \\(T(\\cdot)\\) has continuous inverse function \\(h=T^{-1},\\) then \\(\\hat\\theta_n\\) will be a consistent estimator\r\\[\\hat\\theta_n=h\\left(\\frac{1}{n}\\sum_{j=1}^n g(X_j)\\right)\\stackrel{P}{\\rightarrow}\\theta.\\]\rFurthermore, if the function \\(h(\\cdot)\\) is also differentiable and \\(E_\\theta g^2(X_1)\u0026lt;+\\infty,\\) then the delta method ensures that the MM estimator is also asymptotically normal\r\\[\\sqrt{n}(\\hat\\theta_n-\\theta)\\stackrel{d}{\\rightarrow}N\\left(0,[h\u0026#39;(E_\\theta(X_1))]^2 Var_\\theta(g(X_1))\\right).\\]\n\rMethod of Moments (Example 2)\rConsider the problem of parameter estimation in uniform distribution\n\\[X_1,\\cdots,X_n,\\ \\ X_i\\sim U[0,\\theta],\\,\\theta\u0026gt;0.\\]\rConstruct MM estimators using the functions \\(g(x)=x,\\ \\ g(x)=x^2.\\)\nFor \\(g(x)=x\\) we have \\(E_\\theta X_1=\\frac{\\theta}{2}=t,\\) were the last equality is a notation. Then, \\(\\theta=2t=h(t),\\) again, the last equality is a notation and\r\\[\\frac{1}{n}\\sum_{i=1}^n X_i\\stackrel{P_\\theta}{\\rightarrow}E_\\theta X_1=t.\\]\rTherefore,\r\\[\\hat\\theta_n^1=\\frac{2}{n}\\sum_{i=1}^n X_i\\stackrel{P_\\theta}{\\rightarrow}2t=\\theta,\\]\rand\r\\[\\sqrt{n}(\\hat\\theta_n^1-\\theta)\\stackrel{d}{\\rightarrow}N\\left(0,\\frac{\\theta^2}{3}\\right).\\]\n\rMethod of Moments (Example 2, Page 2)\rFor \\(g(x)=x^2\\) we have \\(E_\\theta X_1^2=\\frac{\\theta^2}{3}=t,\\) were the last equality is a notation. Then, \\(\\theta=\\sqrt{3t}=h(t), \\ \\ (\\theta\u0026gt;0)\\) again, the last equality is a notation and\r\\[\\frac{1}{n}\\sum_{i=1}^n X_i^2\\stackrel{P_\\theta}{\\rightarrow}E_\\theta X_1^2=t.\\]\rTherefore,\r\\[\\hat\\theta_n^2=\\sqrt{\\frac{3}{n}\\sum_{i=1}^n X_i^2}\\stackrel{P_\\theta}{\\rightarrow}\\sqrt{3t}=\\theta,\\]\rand\r\\[\\sqrt{n}(\\hat\\theta_n^2-\\theta)\\stackrel{d}{\\rightarrow}N\\left(0,\\frac{\\theta^2}{5}\\right).\\]\n\rMethod of Moments (Example 2, Page 3)\rHence, in the estimation problem of a one-dimensional parameter \\(\\theta\\) from a uniform distribution\n\\[X_1,\\cdots,X_n,\\ \\ X_i\\sim U[0,\\theta],\\,\\theta\u0026gt;0,\\]\rwe have constructed two estimators using the MM\r\\[\\hat\\theta_n^1=\\frac{2}{n}\\sum_{i=1}^n X_i, \\ \\ \\hat\\theta_n^2=\\sqrt{\\frac{3}{n}\\sum_{i=1}^n X_i^2},\\]\rwith the following properties\r\\[\r\\sqrt{n}(\\hat\\theta_n^1-\\theta)\\stackrel{d}{\\rightarrow}N\\left(0,\\frac{\\theta^2}{3}\\right), \\ \\\r\\sqrt{n}(\\hat\\theta_n^2-\\theta)\\stackrel{d}{\\rightarrow}N\\left(0,\\frac{\\theta^2}{5}\\right).\\]\nBoth have the same rate of convergence, but the second one has smaller asymptotic variance, so, asymptotically, the second one is better, although the first one is computationally easier to implement (an important trade-off in Statistics).\n\rMethod of Moments (Page 2)\rTherefore, if the function \\(T(\\cdot)\\) has continuous inverse function \\(h=T^{-1},\\) then \\(\\hat\\theta_n\\) will be a consistent estimator\r\\[\\hat\\theta_n=h\\left(\\frac{1}{n}\\sum_{j=1}^n g(X_j)\\right)\\stackrel{P}{\\rightarrow}\\theta.\\]\rFurthermore, if the function \\(h(\\cdot)\\) is also differentiable and \\(E_\\theta g^2(X_1)\u0026lt;+\\infty,\\) then the delta method ensures that the MM estimator is also asymptotically normal\r\\[\\sqrt{n}(\\hat\\theta_n-\\theta)\\stackrel{d}{\\rightarrow}N\\left(0,[h\u0026#39;(E_\\theta(X_1))]^2Var_\\theta(g(X_1))\\right).\\]\n\rMethod of Moments (Example 2)\rConsider the problem of parameter estimation in uniform distribution\n\\[X_1,\\cdots,X_n,\\ \\ X_i\\sim U[0,\\theta],\\,\\theta\u0026gt;0.\\]\rConstruct MM estimators using the functions \\(g(x)=x,\\ \\ g(x)=x^2.\\)\nFor \\(g(x)=x\\) we have \\(E_\\theta X_1=\\frac{\\theta}{2}=t,\\) were the last equality is a notation. Then, \\(\\theta=2t=h(t),\\) again, the last equality is a notation and\r\\[\\frac{1}{n}\\sum_{i=1}^n X_i\\stackrel{P_\\theta}{\\rightarrow}E_\\theta X_1=t.\\]\rTherefore,\r\\[\\hat\\theta_n^1=\\frac{2}{n}\\sum_{i=1}^n X_i\\stackrel{P_\\theta}{\\rightarrow}2t=\\theta,\\]\rand\r\\[\\sqrt{n}(\\hat\\theta_n^1-\\theta)\\stackrel{d}{\\rightarrow}N\\left(0,\\frac{\\theta^2}{3}\\right).\\]\n\rMethod of Moments (Example 2, Page 2)\rFor \\(g(x)=x^2\\) we have \\(E_\\theta X_1^2=\\frac{\\theta^2}{3}=t,\\) were the last equality is a notation. Then, \\(\\theta=\\sqrt{3t}=h(t), \\ \\ (\\theta\u0026gt;0)\\) again, the last equality is a notation and\r\\[\\frac{1}{n}\\sum_{i=1}^n X_i^2\\stackrel{P_\\theta}{\\rightarrow}E_\\theta X_1^2=t.\\]\rTherefore,\r\\[\\hat\\theta_n^2=\\sqrt{\\frac{3}{n}\\sum_{i=1}^n X_i^2}\\stackrel{P_\\theta}{\\rightarrow}\\sqrt{3t}=\\theta,\\]\rand\r\\[\\sqrt{n}(\\hat\\theta_n^2-\\theta)\\stackrel{d}{\\rightarrow}N\\left(0,\\frac{\\theta^2}{5}\\right).\\]\n\rMethod of Moments (Example 2, Page 3)\rHence, in the estimation problem of a one-dimensional parameter \\(\\theta\\) from a uniform distribution\n\\[X_1,\\cdots,X_n,\\ \\ X_i\\sim U[0,\\theta],\\,\\theta\u0026gt;0,\\]\rwe have constructed two estimators using the MM\r\\[\\hat\\theta_n^1=\\frac{2}{n}\\sum_{i=1}^n X_i, \\ \\ \\hat\\theta_n^2=\\sqrt{\\frac{3}{n}\\sum_{i=1}^n X_i^2},\\]\rwith the following properties\r\\[\r\\sqrt{n}(\\hat\\theta_n^1-\\theta)\\stackrel{d}{\\rightarrow}N\\left(0,\\frac{\\theta^2}{3}\\right), \\ \\\r\\sqrt{n}(\\hat\\theta_n^2-\\theta)\\stackrel{d}{\\rightarrow}N\\left(0,\\frac{\\theta^2}{5}\\right).\\]\rBoth have the same rate of convergence, but the second one has smaller asymptotic variance, so, asymptotically, the second one is better, although the first one is computationally easier to implement (an important trade-off in Statistics).\n\rThe Maximum Likelihood Estimator\rConsider again the estimation problem of a one-dimensional parameter \\(\\theta\\) from an i.i.d. sample \\[X^n=(X_1,\\cdots,X_n),\\ \\ X_i\\sim F(x,\\theta),\\,\\theta\\in\\Theta\\subset R.\\]\nThe second method to construct estimators is the Maximum Likelihood Estimator. Here we need the existence of the density functions \\(\\{f(x,\\theta,\\,\\theta\\in\\Theta)\\}.\\)\rThe following function is called the Likelihood function of the above model\n\\[L(X^n,\\theta)=\\prod_{i=1}^n f(X_i,\\theta).\\]\nThe Maximum Likelihood Estimator (the MLE) is defined as the point of maximum of the likelihood function\r\\[\\hat\\theta_n^{MLE}=\\arg\\max_{\\theta\\in\\Theta}L(X^n,\\theta).\\]\n\rRegular Models\rWe call the model \\(\\{f(x,\\theta),\\,\\theta\\in\\Theta\\}\\) a regular if the derivative of the density functions \\(f(\\cdot,\\theta)\\) exists with respect to \\(\\theta.\\)\nBeware that there are other technical conditions as well in the definition of regular models, but we will check only the condition above.\nExample The uniform distribution has the density function\r\\[f(x,\\theta)=\\frac{1}{\\theta}I_{[0,\\theta]}(x)=\\frac{1}{\\theta}I_{[0,+\\infty)}(x)I_{[x,+\\infty)}(\\theta).\\]\rThis function is not differentiable w.r.t. the unknown parameter \\(\\theta,\\) hence this is not a regular model.\n\rThe Properties of the MLE\rSince the natural logarithm is an increasing function, then we can define (if \\(f(x,\\theta)\u0026gt;0,\\,(x,\\theta)\\in\\Theta\\times R\\))\r\\[V(X^n,\\theta)=\\ln L(X^n,\\theta)=\\sum_{i=1}^n\\ln f(X_i,\\theta)\\]\rand for the MLE we will have\r\\[\\hat\\theta_n^{MLE}=\\arg\\max_{\\theta\\in\\Theta}V(X^n,\\theta)=\\arg\\max_{\\theta\\in\\Theta}\\sum_{i=1}^n\\ln f(X_i,\\theta).\\]\r\r\rThe Properties of the MLE (Page 2)\rIn the regular models the MLE is consistent and asymptotically normal with the asymptotic variance equal to the inverse of the Fisher information \\(I^{-1}(\\theta)\\)\r\\[\\sqrt{n}(\\hat\\theta_n^{MLE}-\\theta)\\stackrel{d}{\\rightarrow}N(0,I^{-1}(\\theta)),\\]\rwhere the Fisher information is defined as\r\\[I(\\theta)=\\int_{R}\\left[\\frac{\\partial(\\ln f(x,\\theta))}{\\partial\\theta}\\right]^2f(x,\\theta){\\rm d} x.\\]\n\rIf the second derivative of the density functions \\(f(x,\\theta)\\) with respect to \\(\\theta\\) exists then the Fisher information can be calculated by the following formula\n\r\r\\[I(\\theta)=-\\int_{R}\\left[\\frac{\\partial^2(\\ln f(x,\\theta))}{\\partial\\theta^2}\\right]f(x,\\theta){\\rm d} x.\\]\n\rAn Example of a Non-Regular Model\rConsider again the problem of parameter estimation in uniform distribution\r\\[X_1,\\cdots,X_n,\\ \\ X_i\\sim U[0,\\theta],\\,\\theta\u0026gt;0.\\]\rThe density function of the uniform distribution is given by the formula\r\\[f(x,\\theta)=\\frac{1}{\\theta}I_{[0,\\theta]}(x),\\]\rhence the likelihood function will be\r\\[\\begin{align*}\rL(X^n,\\theta)\u0026amp;=\\frac{1}{\\theta^n}\\prod_{i=1}^n I_{[0,\\theta]}(X_i)=\\frac{1}{\\theta^n}I_{[0,+\\infty)}(X_{(1)})I_{[0,\\theta]}(X_{(n)})=\\\\\r\u0026amp;=\\frac{1}{\\theta^n}I_{[0,+\\infty)}(X_{(1)})I_{[X_{(n)},+\\infty)}(\\theta)\\stackrel{a.s.}{=}\\frac{1}{\\theta^n}I_{[X_{(n)},+\\infty)}(\\theta),\r\\end{align*}\\]\rwhich attains its maximum at the point \\(\\hat\\theta_n^\\ast=X_{(n)},\\) so that is the MLE.\n\rAn Example of a Non-Regular Model (Page 2)\rCalculate the distribution function of this estimator\r\\[\\begin{align*}\rP(X_{(n)}\u0026lt; x)\u0026amp;=P(\\max\\{X_1,\\cdots,X_n\\}\u0026lt;x)=P(X_1\u0026lt;x,\\cdots,X_n\u0026lt;x)=\\\\\r\u0026amp;=P(X_1\u0026lt;x)\\cdots P(X_n\u0026lt;x)=F^n(x),\r\\end{align*}\\]\rhence\r\\[P(n(\\theta-X_{(n)})\u0026lt;x)=P\\left(\\theta-\\frac{x}{n}\u0026lt;X_{(n)}\\right)=1-F^n\\left(\\theta-\\frac{x}{n}\\right),\\]\rtherefore, \\(P(n(\\theta-X_{(n)})\u0026lt;x)=0,\\) for \\(x\\leq0,\\) and for \\(x\u0026gt;0\\)\r\\[\\begin{align*}\r\u0026amp;P(n(\\theta-X_{(n)})\u0026lt;x)=P\\left(\\theta-\\frac{x}{n}\u0026lt;X_{(n)}\\right)=1-F^n\\left(\\theta-\\frac{x}{n}\\right)=\\\\\r\u0026amp;=1-\\left(\\frac{\\theta-\\frac{x}{n}}{\\theta}\\right)^n=1-\\left[\\left(1-\\frac{x}{\\theta n}\\right)^{-\\frac{n\\theta}{x}}\\right]^{-\\frac{x}{\\theta}}\\rightarrow1-e^{-\\frac{x}{\\theta}}.\r\\end{align*}\\]\n\rAn Example of a Non-Regular Model (Page 3)\rSo, for the MLE we have the following convergence to the exponential distribution\r\\[n(\\theta-\\hat\\theta_n^\\ast)\\stackrel{d}{\\rightarrow}E\\left(\\frac{1}{\\theta}\\right),\\]\rwhich means that the rate of convergence for the MLE is \\(n,\\) unlike the two previous estimators constructed by the method of moments for which the rate of convergence was \\(\\sqrt{n}.\\) In fact, we can show that even non-asymptotically\n\\[\\begin{align*}\rE_\\theta(\\hat\\theta^\\ast_n-\\theta)^2=\\frac{2\\theta^2}{(n+1)(n+2)}\u0026lt;\\frac{\\theta^2}{3n}=E_\\theta(\\hat\\theta^{1}_n-\\theta)^2,\\,n\u0026gt;2,\r\\end{align*}\\]\rwhere \\(\\hat\\theta^1_n=2\\bar X_n,\\) which entails that even for small sample sizes the MLE is better.\n\rAn Example of a Non-Regular Model (Page 4, Simulations)\rset.seed(1)\rn\u0026lt;-1000\rtheta\u0026lt;-2\rX\u0026lt;-runif(n,0,theta)\rth1\u0026lt;-2*cumsum(X)/(1:n)\rth2\u0026lt;-sqrt(3*cumsum(X^2)/(1:n))\rth\u0026lt;-numeric()\rfor(i in 1:n){\rth[i]\u0026lt;-max(X[1:i])\r}\rplot(1:n,th1,type=\u0026quot;l\u0026quot;,col=1,ylim=c(1.8,2.2),\rxlab=\u0026quot;Sample Size\u0026quot;,ylab=\u0026quot;Estimators\u0026quot;)\rlines(1:n,th2,type=\u0026quot;l\u0026quot;,col=2)\rlines(1:n,th,type=\u0026quot;l\u0026quot;,col=3)\rabline(h=theta,col=4,lty=2)\rlegend(500,2.2,col=c(1,2,3,4),lty=c(1,1,1,2),\rlegend=c(\u0026quot;MM1\u0026quot;,\u0026quot;MM2\u0026quot;,\u0026quot;MLE\u0026quot;,\u0026quot;True Value\u0026quot;))\r\rAn Example of a Non-Regular Model (Page 5)\r\rAn Example of a Non-Regular Model (Page 6)\rn\u0026lt;-10000\rm\u0026lt;-1000\rtheta\u0026lt;-2\rth1\u0026lt;-numeric()\rth2\u0026lt;-numeric()\rth\u0026lt;-numeric()\rfor(i in 1:m){set.seed(i);X\u0026lt;-runif(n,0,theta)\rth1[i] \u0026lt;- 2*mean(X)\rth2[i] \u0026lt;- sqrt(3*mean(X^2))\rth[i] \u0026lt;- max(X) }\r\rAn Example of a Non-Regular Model (Page 7)\rpar(mfrow=c(1,3))\rx\u0026lt;-seq(-4,4,0.001)\rhist(sqrt(n)*(th1-theta),nclass=50,freq=FALSE,\rcol=\u0026quot;lightblue\u0026quot;, border=\u0026quot;blue\u0026quot;,ylab=\u0026quot;\u0026quot;,\rxlab=\u0026quot;\u0026quot;,main=\u0026quot;MM1\u0026quot;,xlim=c(-4,4),ylim=c(0,0.5))\rlines(x,dnorm(x,0,theta^2/3),col=2)\rhist(sqrt(n)*(th2-theta),nclass=50,freq=FALSE,\rcol=\u0026quot;lightblue\u0026quot;, border=\u0026quot;blue\u0026quot;,ylab=\u0026quot;\u0026quot;,\rxlab=\u0026quot;\u0026quot;,main=\u0026quot;MM2\u0026quot;,xlim=c(-4,4),ylim=c(0,0.5))\rlines(x,dnorm(x,0,theta^2/5),col=2)\rx\u0026lt;-seq(0,10,0.001)\rhist(n*(theta-th),nclass=50,freq=FALSE,col=\u0026quot;lightblue\u0026quot;,\rborder=\u0026quot;blue\u0026quot;,ylab=\u0026quot;\u0026quot;,xlab=\u0026quot;\u0026quot;,main=\u0026quot;MLE\u0026quot;,\rxlim=c(0,10),ylim=c(0,0.5))\rlines(x,dexp(x,1/theta),col=2)\r\rAn Example of a Non-Regular Model (Page 7)\r\r","date":1521435600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521435600,"objectID":"0130c4f965577a1027f2cfd9d7abe991","permalink":"https://gasparyan.co/post/regmod/2018-09-13-r-rmarkdown/","publishdate":"2018-03-19T09:00:00+04:00","relpermalink":"/post/regmod/2018-09-13-r-rmarkdown/","section":"post","summary":"Consistent Estimators\rIn the estimation problem of a one-dimensional parameter \\(\\theta\\) from an i.i.d. sample \\[X^n=(X_1,\\cdots,X_n),\\ \\ X_i\\sim F(x,\\theta),\\,\\theta\\in\\Theta\\subset R\\]\rwe introduced the notion of consistency of an estimator.","tags":["Statistics","Consistency","Asymptotic Normality","MLE","Method of Moments"],"title":"Optimality of Estimators in Regular Models","type":"post"},{"authors":[],"categories":null,"content":"","date":1481497200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481497200,"objectID":"4880730fed432b8eef1e5673f6a740f7","permalink":"https://gasparyan.co/talk/defense2016/","publishdate":"2016-12-12T00:00:00+01:00","relpermalink":"/talk/defense2016/","section":"talk","summary":"","tags":[],"title":"Two problems of statistical estimation for stochastic processes","type":"talk"},{"authors":[],"categories":null,"content":"","date":1473372000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473372000,"objectID":"f75869195ee661d8b743510d1d62495d","permalink":"https://gasparyan.co/talk/aua2016/","publishdate":"2016-09-09T00:00:00+02:00","relpermalink":"/talk/aua2016/","section":"talk","summary":"","tags":[],"title":"Some Problems of Statistical Estimation","type":"talk"},{"authors":[],"categories":null,"content":"","date":1465509600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1465509600,"objectID":"34605ab71f2f5aeb71dd168ebcec0443","permalink":"https://gasparyan.co/talk/dynstoch2016/","publishdate":"2016-06-10T00:00:00+02:00","relpermalink":"/talk/dynstoch2016/","section":"talk","summary":"","tags":[],"title":"Second Order Asymptotic Efficiency for a Poisson process","type":"talk"},{"authors":["Samvel B. Gasparyan","Yury Kutoyants"],"categories":null,"content":"","date":1355270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1355270400,"objectID":"fd508e6469e500a5d1e1e86af5ef80b5","permalink":"https://gasparyan.co/publication/phd-thesis/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/phd-thesis/","section":"publication","summary":"This work is devoted to the questions of the statistics of stochastic processes. Particularly, the first chapter is devoted to a non-parametric estimation problem for an inhomogeneous Poisson process. The second chapter is dedicated to a problem of estimation of the solution of a Backward Stochastic Differential Equation (BSDE).","tags":["Source Themes"],"title":"Publications related to the PhD thesis","type":"publication"}]