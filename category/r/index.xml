<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Biostatistician</title>
    <link>https://gasparyan.co/category/r/</link>
      <atom:link href="https://gasparyan.co/category/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2022 Samvel B. Gasparyan</copyright><lastBuildDate>Thu, 30 Dec 2021 10:00:00 +0400</lastBuildDate>
    <image>
      <url>https://gasparyan.co/images/icon_hu37ae94fde7f6135a8e8cfd653ea9ade8_11929814_512x512_fill_lanczos_center_2.png</url>
      <title>R</title>
      <link>https://gasparyan.co/category/r/</link>
    </image>
    
    <item>
      <title>Neyman-Pearson and some other Uniformly Most Powerful Tests</title>
      <link>https://gasparyan.co/post/neymanpearson/2021-12-30-r-rmarkdown/</link>
      <pubDate>Thu, 30 Dec 2021 10:00:00 +0400</pubDate>
      <guid>https://gasparyan.co/post/neymanpearson/2021-12-30-r-rmarkdown/</guid>
      <description>
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Suppose data consisting of i.i.d. observations &lt;span class=&#34;math inline&#34;&gt;\(X^n=(X_1,X_2,\cdots,X_n)\)&lt;/span&gt; are available from a distribution &lt;span class=&#34;math inline&#34;&gt;\(F(x,\theta),\,\theta\in\Theta\subset\mathbf{R}.\)&lt;/span&gt; The exact value &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; corresponding to the distribution that generated the observations is unknown. The problem is, using the available data &lt;span class=&#34;math inline&#34;&gt;\(X^n,\)&lt;/span&gt; construct tests for making decisions on the possible value of unknown parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Unlike the estimation problems where an estimator is constructed based on data which can be used as an approximate value of the unknown parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the hypothesis testing deals with decisions, for example, whether the unknown parameter is in a given subset (the null hypothesis)
&lt;span class=&#34;math display&#34;&gt;\[{\mathcal H}_0:\ \ \theta\in\Theta_0\subset\Theta,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or, alternatively, in its supplement
&lt;span class=&#34;math display&#34;&gt;\[{\mathcal H}_a:\ \ \theta\in\Theta\setminus\Theta_0.\]&lt;/span&gt;
Therefore, hypothesis testing is interested in knowing whether the unknown value is in a given set &lt;span class=&#34;math inline&#34;&gt;\(\Theta_0\)&lt;/span&gt;. We may define this set as containing only one value &lt;span class=&#34;math inline&#34;&gt;\(\Theta_0=\{\theta_0\}\)&lt;/span&gt; in which case the test will be whether the unknown value is equal to the given known value &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt;. The statistical tests that make the decisions are based on the data and the construction of statistical tests can be formalized as follows.&lt;/p&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(\psi:\mathbf{R^n}\rightarrow\{0,1\}\)&lt;/span&gt; is a measurable function defined for all observations &lt;span class=&#34;math inline&#34;&gt;\(X^n\)&lt;/span&gt; and takes only the values 0 and 1. Any such function will be called a &lt;em&gt;statistical test.&lt;/em&gt; We will use the convention that the value 1 corresponds to the decision of rejecting the null hypothesis (hence the alternative hypothesis should be accepted), while the value 0 means a decision that the null hypothesis should be accepted. Hence using the available observations &lt;span class=&#34;math inline&#34;&gt;\(X^n\)&lt;/span&gt; we can make a decision based on the value of &lt;span class=&#34;math inline&#34;&gt;\(\psi(X^n).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As we have seen from the definition of a statistical test, any measurable function is a test, including the functions that are constant &lt;span class=&#34;math inline&#34;&gt;\(\psi\equiv1\)&lt;/span&gt; (data independent tests), which are not good tests at all since those will always give the same answer regardless of the data, and hence, will very likely be wrong in most cases. Therefore, we need to define tests that have good properties (give reliable answers), and before this we need to define what a good test should be in a formal way. We will be dealing only with small sample statistical tests, meaning the sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is fixed and the properties of statistical test will be considered under this condition only (unlike the asymptotic theory, where a large sample inference is done under the condition when &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow+\infty\)&lt;/span&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;type-i-and-ii-errors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Type I and II errors&lt;/h3&gt;
&lt;p&gt;For each statistical test &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; we may either make a correct decision (correctly identify the set to which the unknown value &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; belongs) or commit one of two errors: reject the null hypothesis when it is true (type I error) or accept when it is false (type II error). If the sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is fixed, it is impossible to construct a test with both types of errors being low, hence the strategy is to fix some level for the type I error (&lt;em&gt;level of significance&lt;/em&gt;) and among those tests find a test with the lowest type II error.&lt;/p&gt;
&lt;p&gt;Indeed, consider the type I error of a given statistical test &lt;span class=&#34;math inline&#34;&gt;\(\psi.\)&lt;/span&gt; The type I error, denoting it by &lt;span class=&#34;math inline&#34;&gt;\(\alpha(\psi),\)&lt;/span&gt; will be
&lt;span class=&#34;math display&#34;&gt;\[\alpha(\psi,\theta) = P_\theta(\psi=1)=E_\theta\psi,\ \ \theta\in\Theta_0.\]&lt;/span&gt;
That is, the probability of rejecting that &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is in &lt;span class=&#34;math inline&#34;&gt;\(\Theta_0\)&lt;/span&gt; (the decision is &lt;span class=&#34;math inline&#34;&gt;\(\psi=1\)&lt;/span&gt;) while &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is indeed in &lt;span class=&#34;math inline&#34;&gt;\(\Theta_0.\)&lt;/span&gt; For a given &lt;em&gt;significance level&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\alpha\in(0,1)\)&lt;/span&gt;, we consider only tests &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; such that
&lt;span class=&#34;math display&#34;&gt;\[\alpha(\psi,\theta)\leq\alpha,\ \ \theta\in\Theta_0.\]&lt;/span&gt;
Among these tests we will try to find the one with the lowest type II error. Or, equivalently, if we denote by &lt;span class=&#34;math inline&#34;&gt;\(\pi(\psi,\theta)=P_\theta(\psi=1)=E_\theta\psi,\ \ \theta\in\Theta\setminus\Theta_0,\)&lt;/span&gt; the &lt;em&gt;power function&lt;/em&gt; of the test &lt;span class=&#34;math inline&#34;&gt;\(\psi,\)&lt;/span&gt; then the problem above can be formulated as finding a test with the highest power in the region &lt;span class=&#34;math inline&#34;&gt;\(\Theta\setminus\Theta_0\)&lt;/span&gt; among the tests with the given significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; in the region &lt;span class=&#34;math inline&#34;&gt;\(\Theta_0.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The hypothesis testing will be called &lt;em&gt;simple&lt;/em&gt; if both &lt;span class=&#34;math inline&#34;&gt;\(\Theta_0\)&lt;/span&gt; and its complement consist of only single values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;neyman-pearson-test&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Neyman-Pearson test&lt;/h3&gt;
&lt;p&gt;Consider the case of simple hypothesis testing. We observe from a random variable &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; which has a distribution function &lt;span class=&#34;math inline&#34;&gt;\(F(x),\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(X\sim F(x)\)&lt;/span&gt;. The simple hypothesis to be tested is the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\mathcal H}_0:\ \ F(x)=F_0(x),\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the alternative hypothesis is
&lt;span class=&#34;math display&#34;&gt;\[{\mathcal H}_a:\ \ F(x)=F_1(x).\]&lt;/span&gt;
Here &lt;span class=&#34;math inline&#34;&gt;\(F_0(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_1(x)\)&lt;/span&gt; are given distribution functions.&lt;/p&gt;
&lt;p&gt;Suppose the distribution function &lt;span class=&#34;math inline&#34;&gt;\(F_0(x)\)&lt;/span&gt; has a density &lt;span class=&#34;math inline&#34;&gt;\(f_0(x)\)&lt;/span&gt; with respect to some measure &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, while &lt;span class=&#34;math inline&#34;&gt;\(F_1(x)\)&lt;/span&gt; has a density &lt;span class=&#34;math inline&#34;&gt;\(f_1(x),\)&lt;/span&gt; with respect to the same measure. Such a measure always exists since we can take the measure generated by the distribution function &lt;span class=&#34;math inline&#34;&gt;\(\tilde F(x)=\frac{F_0(x)+F_1(x)}{2}\)&lt;/span&gt;. The &lt;strong&gt;Neyman-Pearson&lt;/strong&gt; fundamental lemma &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lehmann2005testing&#34; role=&#34;doc-biblioref&#34;&gt;Lehmann and Romano 2005&lt;/a&gt;)&lt;/span&gt; says that:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;For a given significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\in(0,1)\)&lt;/span&gt; there exists a value &lt;span class=&#34;math inline&#34;&gt;\(c_0\in\mathbf{R}\)&lt;/span&gt; such that the following &lt;em&gt;Neyman-Pearson (NP)&lt;/em&gt; test
&lt;span class=&#34;math display&#34;&gt;\[\tilde\psi_{c_0}(x)=\left\{
\begin{matrix}
1, &amp;amp; x\in\{x:\,f_1(x)&amp;gt;c_0f_0(x)\},\\
\frac{\alpha-\alpha(c_0)}{\alpha(c_0-0)-\alpha(c_0)}, &amp;amp; x\in\{x:\,f_1(x)=c_0f_0(x)\},\\
0, &amp;amp; x\in\{x:\,f_1(x)&amp;lt;c_0f_0(x)\},
\end{matrix}\right.
\]&lt;/span&gt;
satisfies the equality &lt;span class=&#34;math inline&#34;&gt;\(E_{\theta_0}\tilde\psi_{c_0}(X)=\alpha.\)&lt;/span&gt; Here
&lt;span class=&#34;math display&#34;&gt;\[\alpha(c)=P_0(f_1(X)&amp;gt;cf_0(X)),\]&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(c_0\)&lt;/span&gt; is such that &lt;span class=&#34;math inline&#34;&gt;\(\alpha(c_0)\leq \alpha\leq\alpha(c_0-0).\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The test &lt;span class=&#34;math inline&#34;&gt;\(\tilde\psi_c\)&lt;/span&gt; is most powerful at the significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha.\)&lt;/span&gt; Meaning that for any test &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; which is of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; level, that is, &lt;span class=&#34;math inline&#34;&gt;\(E_{0}(X)\psi\leq \alpha,\)&lt;/span&gt; the power of that test does not exceed the power of the test &lt;span class=&#34;math inline&#34;&gt;\(\tilde\psi_{c_0}\)&lt;/span&gt;, &lt;span class=&#34;math display&#34;&gt;\[E_{1}\tilde\psi_{c_0}(X)-E_{1}\psi(X)\geq \int\left[\tilde\psi_{c_0}(x)-\psi(x)\right]f_1(x)d \mu\geq 0.\]&lt;/span&gt;
Indeed, if &lt;span class=&#34;math inline&#34;&gt;\(\tilde \psi_{c_0}(x)&amp;gt;\psi(x)\geq 0,\)&lt;/span&gt; then necessarily &lt;span class=&#34;math inline&#34;&gt;\(\tilde \psi_{c_0}(x)\neq 0\)&lt;/span&gt; hence &lt;span class=&#34;math inline&#34;&gt;\(f_1(x)\geq c_0f_0(x).\)&lt;/span&gt; While, in the same way, if &lt;span class=&#34;math inline&#34;&gt;\(\tilde \psi_{c_0}(x)&amp;lt;\psi(x)\geq 1,\)&lt;/span&gt; then necessarily &lt;span class=&#34;math inline&#34;&gt;\(\tilde \psi_{c_0}(x)\neq 1\)&lt;/span&gt; hence &lt;span class=&#34;math inline&#34;&gt;\(f_1(x)\leq c_0f_0(x).\)&lt;/span&gt; Therefore,
&lt;span class=&#34;math display&#34;&gt;\[\int\left(\tilde\psi_{c_0}(x)-\psi(x)\right)(f_1(x)-c_0f_0(x))d\mu\geq 0.\]&lt;/span&gt;
Which entails that
&lt;span class=&#34;math display&#34;&gt;\[\int\left(\tilde\psi_{c_0}(x)-\psi(x)\right)f_1(x)d\mu\geq c_0\int\left(\tilde\psi_{c_0}(x)-\psi(x)\right)f_0(x)d\mu=c_0[a-E_0\psi(X)]\geq 0.\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a test &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; is most powerful at level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; for testing &lt;span class=&#34;math inline&#34;&gt;\(f_0(x)\)&lt;/span&gt; against &lt;span class=&#34;math inline&#34;&gt;\(f_1(x)\)&lt;/span&gt;, then for some &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; it can be written as &lt;span class=&#34;math inline&#34;&gt;\(\psi=\tilde\psi_c,\)&lt;/span&gt; almost everywhere on the set &lt;span class=&#34;math inline&#34;&gt;\(\{f_1(x)\neq c_0 f_0(x)\}\)&lt;/span&gt;. Furthermore, for the most powerful test &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt; the equality &lt;span class=&#34;math inline&#34;&gt;\(E_{\theta_0}\psi(X)=\alpha\)&lt;/span&gt; will be satisfied unless there exists a test of size &lt;span class=&#34;math inline&#34;&gt;\(&amp;lt;\alpha\)&lt;/span&gt; and with power 1.
Since the &lt;em&gt;NP&lt;/em&gt; test always exists and is most powerful, this third point essentially means the uniqueness (almost everywhere) of most powerful tests, except possibly on the set &lt;span class=&#34;math inline&#34;&gt;\(\{f_1(x)= c_0 f_0(x)\}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Remark.&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For a given &lt;span class=&#34;math inline&#34;&gt;\(\alpha\in(0,1)\)&lt;/span&gt; the value &lt;span class=&#34;math inline&#34;&gt;\(c_0\)&lt;/span&gt; always exits since &lt;span class=&#34;math inline&#34;&gt;\(1-\alpha(c)\)&lt;/span&gt; is a distribution function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The constructed test is &lt;em&gt;randomized&lt;/em&gt;, meaning that it does not take only the values &lt;span class=&#34;math inline&#34;&gt;\({0,1},\)&lt;/span&gt; but can take also a value between 0 and 1, which can be interpreted as the probability of rejecting the null hypothesis. Hence, as a result of this statistical test, the decision to reject or accept the null hypothesis sometimes may not be made, but a probability is assigned to rejecting the null hypothesis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the set &lt;span class=&#34;math inline&#34;&gt;\(\{x:\,f_1(x)=c_0f_0(x)\}\)&lt;/span&gt; has the &lt;span class=&#34;math inline&#34;&gt;\(\mu-\)&lt;/span&gt;measure zero, then the most powerful test is determined uniquely (up to sets of measure zero) by the &lt;em&gt;Neyman-Pearson&lt;/em&gt; lemma. This will happen if, for example, both &lt;span class=&#34;math inline&#34;&gt;\(f_1(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f_0(x)\)&lt;/span&gt; are continuous and &lt;span class=&#34;math inline&#34;&gt;\(f_0(x)&amp;gt;0,\)&lt;/span&gt; almost everywhere.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In practice randomization is not considered acceptable and hence an &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; value is selected so that a &lt;em&gt;non-randomized&lt;/em&gt; test exists.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Example.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Consider a single observation &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; from a &lt;em&gt;Poisson distribution&lt;/em&gt;, that is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(k,\theta)=P(X=k)=\frac{\theta^k}{k!}e^{-\theta},\ \ k=0,1,2,\cdots.\]&lt;/span&gt;
We are testing the simple hypothesis&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[{\mathcal H}_0:\ \ \theta=\theta_0,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;against the alternative hypothesis
&lt;span class=&#34;math display&#34;&gt;\[{\mathcal H}_a:\ \ \theta=\theta_1&amp;gt;\theta_0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case
&lt;span class=&#34;math display&#34;&gt;\[\frac{f(X,\theta_1)}{f(X,\theta_0)}=\left(\frac{\theta_1}{\theta_0}\right)^Xe^{-(\theta_1-\theta_0)}&amp;gt;\tilde c\]&lt;/span&gt;
is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(X&amp;gt;c,\)&lt;/span&gt; because of the fact that &lt;span class=&#34;math inline&#34;&gt;\(\theta_1&amp;gt;\theta_0.\)&lt;/span&gt; Hence the most powerful test will be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tilde\psi_{c_0}(X)=\left\{
\begin{matrix}
&amp;amp;1, &amp;amp; X&amp;gt;c_0,\\
&amp;amp;\frac{F(c_0,\theta_0)-(1-\alpha)}{F(c_0,\theta_0)-F(c_0-0,\theta_0)}, &amp;amp; X=c_0,\\
&amp;amp;0, &amp;amp; X&amp;lt;c_0.
\end{matrix}\right.
\]&lt;/span&gt;
Here &lt;span class=&#34;math inline&#34;&gt;\(c_0\)&lt;/span&gt; is such that &lt;span class=&#34;math inline&#34;&gt;\(F(c_0-0,\theta_0)\leq 1-\alpha\leq F(c_0,\theta_0).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As noted above, to avoid randomized tests we can select the significance level in a way so that the set &lt;span class=&#34;math inline&#34;&gt;\(\{X=c_0\}\)&lt;/span&gt; has the measure zero. This can be achieved by replacing the given significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; by a more conservative (lower) significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(1-\alpha_0=F(c_0,\theta_0).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Take the case of &lt;span class=&#34;math inline&#34;&gt;\(\theta_0=1,\,\theta_1=2,\,\alpha=0.05.\)&lt;/span&gt; In this case,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[F(3-0, 1)\leq 1-\alpha\leq F(3,1),\]&lt;/span&gt;
therefore, &lt;span class=&#34;math inline&#34;&gt;\(c_0=3.\)&lt;/span&gt; This value can be found as follows&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha &amp;lt;- 0.05
theta0 &amp;lt;- 1
theta1 &amp;lt;- 2
Y &amp;lt;- ppois(1:100, theta0)
Z &amp;lt;- which(Y &amp;gt; 1- alpha, Y)
c0 &amp;lt;- Z[1]
c0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we test at the given significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0.05,\)&lt;/span&gt; then the Neyman-Pearson test will be randomized and on the set &lt;span class=&#34;math inline&#34;&gt;\(\{X=3\}\)&lt;/span&gt; it will have the following value&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(ppois(c0,theta0)-(1-alpha))/(ppois(c0,theta0)-ppois(c0-1,theta0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5057936&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hence, on this set the probability of rejecting the null hypothesis is around 0.5, hence no decision can be made. On the other hand, if we take a more conservative significance level &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0\)&lt;/span&gt; as follows&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha0 &amp;lt;- 1-ppois(c0,theta0)
alpha0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01898816&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case we can get a non-randomized test with the power equal to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ppois(c0,theta1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8571235&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-lehmann2005testing&#34; class=&#34;csl-entry&#34;&gt;
Lehmann, Erich Leo, and Joseph P Romano. 2005. &lt;em&gt;Testing Statistical Hypotheses&lt;/em&gt;. 3rd ed. Vol. 3. Springer.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Win Odds Confidence Intervals in R</title>
      <link>https://gasparyan.co/post/wrci/2020-01-10-r-rmarkdown/</link>
      <pubDate>Fri, 10 Jan 2020 10:00:00 +0400</pubDate>
      <guid>https://gasparyan.co/post/wrci/2020-01-10-r-rmarkdown/</guid>
      <description>
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;mann-whitney-estimate-for-the-win-probability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mann-Whitney estimate for the win probability&lt;/h2&gt;
&lt;p&gt;Consider two independent, continuous RVs (random variables) &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;. The following probability is called the &lt;em&gt;win probability&lt;/em&gt; of RV &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; against the RV &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\theta=P(\eta&amp;gt;\xi).
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given an i.i.d. (independent, identically distributed) random sample from &lt;span class=&#34;math inline&#34;&gt;\(\xi,\)&lt;/span&gt; denoted by &lt;span class=&#34;math inline&#34;&gt;\(X=(X_1,\cdots,X_{n_1})\)&lt;/span&gt; and an i.i.d. sample from &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; denoted by &lt;span class=&#34;math inline&#34;&gt;\(Y=(Y_1,\cdots,Y_{n_2})\)&lt;/span&gt; we are interested in estimating the unknown parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The following estimator is called the &lt;em&gt;Mann-Whitney&lt;/em&gt; estimator (or, the &lt;em&gt;win proportion&lt;/em&gt;) for the win probability&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\hat\theta_N=\frac{1}{n_1n_2}\sum_{i=1}^{n_1}\sum_{j=1}^{n_2}I(X_i&amp;lt;Y_j).
\end{align*}\]&lt;/span&gt;
Here &lt;span class=&#34;math inline&#34;&gt;\(N=n_1+n_2\)&lt;/span&gt; is the total sample size, whereas &lt;span class=&#34;math inline&#34;&gt;\(I(\cdot)\)&lt;/span&gt; is the indicator function which takes the value 1 if the underlying inequality is true and 0 otherwise. When &lt;span class=&#34;math inline&#34;&gt;\(n_1\rightarrow+\infty,\,n_2\rightarrow+\infty\)&lt;/span&gt; then the win proportion is a consistent estimator (convergence in probability) for the win probability
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\hat\theta_N\longrightarrow\theta.
\end{align*}\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_1}{N}\rightarrow \alpha,\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n_1\rightarrow+\infty,\,n_2\rightarrow+\infty,\)&lt;/span&gt; then the win proportion is also asymptotically normal
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\sqrt{N}(\hat\theta_N-\theta)\Longrightarrow{\mathcal N}\left(0,\frac{1}{1-\alpha}\sigma_{10}^2+\frac{1}{\alpha}\sigma_{01}^2\right).
\end{align*}\]&lt;/span&gt;
Here,
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\sigma_{10}^2=Cov(I(\xi&amp;lt;\eta),I(\xi&amp;#39;&amp;lt;\eta))=P(\xi&amp;lt;\eta,\xi&amp;#39;&amp;lt;\eta)-P(\xi&amp;lt;\eta)^2,\\
\sigma_{01}^2=Cov(I(\xi&amp;lt;\eta),I(\xi&amp;lt;\eta&amp;#39;))=P(\xi&amp;lt;\eta,\xi&amp;lt;\eta&amp;#39;)-P(\xi&amp;lt;\eta)^2,
\end{align*}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\xi&amp;#39;\)&lt;/span&gt; has the same distribution as &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\eta&amp;#39;\)&lt;/span&gt; has the same distribution as &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;. All &lt;span class=&#34;math inline&#34;&gt;\(\xi,\xi&amp;#39;,\eta,\eta&amp;#39;\)&lt;/span&gt; are independent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-to-exponential-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application to exponential distributions&lt;/h2&gt;
&lt;p&gt;Suppose now that &lt;span class=&#34;math inline&#34;&gt;\(\xi\sim{\mathbb E}(\lambda)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\sim{\mathbb E}(\mu).\)&lt;/span&gt; In this case,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\sigma_{10}^2=\frac{2\lambda}{(\lambda+\mu)(2\lambda+\mu)}-\frac{\lambda^2}{(\lambda+\mu)^2}=\frac{\lambda^2\mu}{(\lambda+\mu)^2(2\lambda+\mu)}\\
\sigma_{01}^2=\frac{\lambda}{\lambda+2\mu}-\frac{\lambda^2}{(\lambda+\mu)^2}=\frac{\lambda\mu^2}{(\lambda+\mu)^2(\lambda+2\mu)},
\end{align*}\]&lt;/span&gt;
therefore, the asymptotic variance of the win proportion will be
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\sigma^2=\frac{\lambda\mu}{(\lambda+\mu)^2}\left(\frac{1}{1-\alpha}\frac{\lambda}{2\lambda+\mu}+\frac{1}{\alpha}\frac{\mu}{\lambda+2\mu}\right).
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can check this by the following simulations&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n1 &amp;lt;- 700
n2 &amp;lt;- 100
N &amp;lt;- n1 + n2
m &amp;lt;- 1000
lambda &amp;lt;- 2
mu &amp;lt;- 10
alpha &amp;lt;- n1/(n1+n2)


k &amp;lt;- lambda/(lambda+mu)
WR &amp;lt;-NULL

for(i in 1:m){
  set.seed(i)
  X1 &amp;lt;- rexp(n1,lambda)
  X2 &amp;lt;- rexp(n2,mu)
  d &amp;lt;- expand.grid(x=X1,y=X2)
  d$w &amp;lt;- ifelse(d$y&amp;gt;d$x,1,ifelse(d$y==d$x,0.5,0))
  WR[i]&amp;lt;-sqrt(N)*(mean(d$w)-k)
}


x0&amp;lt;-3
int &amp;lt;- seq(-x0,x0,0.001)

Coeff0 &amp;lt;- mu*lambda/(lambda+mu)^2
Coeff &amp;lt;- Coeff0*(1/(1-alpha)*lambda/(2*lambda+mu)+1/alpha*mu/(lambda+2*mu))


hist(WR, nclass = 20, freq=FALSE, xlim=c(-x0,x0), 
     ylim=c(0, 1.1), col = &amp;quot;lightblue&amp;quot;, border = &amp;quot;blue&amp;quot;)
lines(int,dnorm(int, mean = 0, sd=sqrt(Coeff)), col=&amp;quot;2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/wrci/2020-01-10-r-rmarkdown_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;definition-of-the-win-odds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Definition of the win odds&lt;/h2&gt;
&lt;p&gt;Consider two independent, continuous RVs (random variables) &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;. The odds of the win probability is called the &lt;em&gt;win odds&lt;/em&gt; of RV &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; against the RV &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\omega=\frac{P(\eta&amp;gt;\xi)}{P(\eta&amp;lt;\xi)}=\frac{\theta}{1-\theta}.
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Mann-Whitney estimate of the win probability can be transformed by the function &lt;span class=&#34;math inline&#34;&gt;\(f(x)=\frac{x}{1-x},\ \ x\in(0,1)\)&lt;/span&gt; to get an estimate for the win odds. Using the same transformation and the asymptotic normality of the Mann-Whitney estimate it is possible to construct asymptotic confidence intervals for the win odds, for a given asymptotic confidence level.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\omega=1\)&lt;/span&gt; then the random variables &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; are stochastically equivalent, while &lt;span class=&#34;math inline&#34;&gt;\(\omega&amp;gt;1\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is stochastically greater than (wins against) &lt;span class=&#34;math inline&#34;&gt;\(\eta.\)&lt;/span&gt; The case &lt;span class=&#34;math inline&#34;&gt;\(\omega&amp;lt;1\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; loses against &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;. The asymptotic confidence interval of the win odds can be used to test the hypothesis whether &lt;span class=&#34;math inline&#34;&gt;\(\omega=1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To use the asymptotic normality result described above we need to estimate the asymptotic variance. The package &lt;em&gt;sanon&lt;/em&gt; in &lt;em&gt;R&lt;/em&gt; allows to estimate the asymptotic standard error of the Mann-Whitney estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-package-sanon&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The package sanon&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install.packages(&amp;quot;sanon&amp;quot;)
library(sanon)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset &lt;em&gt;resp&lt;/em&gt; contains data from a randomized clinical trial to compare a test treatment to placebo for a respiratory disorder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(resp, package = &amp;quot;sanon&amp;quot;)

head(resp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   center treatment sex age baseline visit1 visit2 visit3 visit4
## 1      1         A   F  32        1      2      2      4      2
## 2      2         A   F  37        1      3      4      4      4
## 3      1         A   F  47        2      2      3      4      4
## 4      2         A   F  39        2      3      4      4      4
## 5      1         A   M  11        4      4      4      4      2
## 6      2         A   F  60        4      4      3      3      4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The column &lt;em&gt;visit4&lt;/em&gt; is a numeric vector for patient global ratings of symptom control according to 5 categories (4 = excellent, 3 = good, 2 = fair, 1 = poor, 0 = terrible), measured at visit 4. To compare the effect of active treatment against the placebo we will use the win probability, which, as we defined previously, is an unknown theoretical quantity. The null hypothesis is that there is no treatment difference which can be written in terms of the win probability as &lt;span class=&#34;math inline&#34;&gt;\(\theta=0.5.\)&lt;/span&gt; The Mann-Whitney estimate of the win probability can be calculated as follows&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- sanon(visit4 ~ grp(treatment, ref=&amp;quot;P&amp;quot;), data = resp)

fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## sanon.formula(formula = visit4 ~ grp(treatment, ref = &amp;quot;P&amp;quot;), data = resp)
## 
## Sample size: 111
## 
## Response levels:
## [visit4; 5 levels] (lower) 0, 1, 2, 3, 4 (higher)
## 
## Design Matrix:
##        [,1]
## visit4    1
## 
## Mann-Whitney Estimate 
##  for comparison [ A / P ] :
## visit4 
## 0.6174&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## M-W Estimate and 95% Confidence Intervals 
## :
##        Estimate  Lower  Upper
## visit4   0.6174 0.5173 0.7176&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit$p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]
## [1,] 0.02150601&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value based on the asymptotic confidence interval of level 0.05 is less than 0.05, hence the null hypothesis of no treatment difference is rejected. The Mann-Whitney estimate can be transformed to get an estimate for the win odds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(fit)$ci/(1-confint(fit)$ci)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Estimate    Lower    Upper
## visit4 1.614013 1.071762 2.540746&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the win odds the null hypothesis of no treatment difference is &lt;span class=&#34;math inline&#34;&gt;\(\omega=1.\)&lt;/span&gt; The win odds 1.61 characterizes the treatment effect difference.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exponential Distribution</title>
      <link>https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown/</link>
      <pubDate>Fri, 20 Dec 2019 10:00:00 +0400</pubDate>
      <guid>https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown/</guid>
      <description>
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;This is a short reminder of some simple properties of exponential distributions.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The continuous random variable (RV) &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; has an exponential distribution with the rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;gt;0\)&lt;/span&gt; if its CMD (cumulative distribution function) has the following form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[F_\xi(x)=P(\xi&amp;lt;x)=\left\{\begin{matrix}
&amp;amp;1-e^{-\lambda x}, &amp;amp;x\geq 0.\\
&amp;amp;0 &amp;amp;x&amp;lt;0.
\end{matrix}\right.
\]&lt;/span&gt;
This entails that an exponential RV is with 1 probability positive and has the PDF (probability density function)
&lt;span class=&#34;math display&#34;&gt;\[f_\xi(x)=F_\xi&amp;#39;(x)=\left\{\begin{matrix}
&amp;amp;\lambda e^{-\lambda x}, &amp;amp;x&amp;gt;0,\\
&amp;amp;0 &amp;amp;x&amp;lt;0.
\end{matrix}\right.
\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; has an exponential distribution with the rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; we will denote this by &lt;span class=&#34;math inline&#34;&gt;\({\mathbb E}(\lambda).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; the following functions can be used to, correspondingly, generate numbers from &lt;span class=&#34;math inline&#34;&gt;\({\mathbb E}(\lambda)\)&lt;/span&gt;, calculate values of CDF, calculate values of PDF&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rexp(n=2, rate=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.06697548 0.25507292&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pexp(q=1, rate=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6321206&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dexp(x=1, rate=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3678794&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;histogram&lt;/em&gt; is a non-parametric estimator for the PDF. Hence we can simulate data from an exponential distribution and show that the histogram based on the data fits the PDF. Consider the case of &lt;span class=&#34;math inline&#34;&gt;\({\mathbb E}(2)\)&lt;/span&gt; and simulate a sample of size &lt;span class=&#34;math inline&#34;&gt;\(n=10000.\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())

n &amp;lt;- 10000
lambda &amp;lt;- 2

X &amp;lt;- rexp(n = n, rate = lambda)
int &amp;lt;- seq(0, max(X), max(X)/100)

hist(X, freq = FALSE, nclass = 50, col = &amp;quot;lightblue&amp;quot;, 
     border = &amp;quot;blue&amp;quot;, ylim = c(0, 2), main = &amp;quot;&amp;quot;)
lines(int, dexp(int, rate = lambda), col = &amp;quot;red&amp;quot;, lty = 2, lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a very useful technique to check whether the RVs have the given PDF. We will use this technique below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(\xi\sim {\mathbb E}(\lambda)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\sim {\mathbb E}(\mu)\)&lt;/span&gt; are independent. Calculate the PDF of the RV &lt;span class=&#34;math inline&#34;&gt;\(\zeta=\eta-\xi.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;First consider the case of &lt;span class=&#34;math inline&#34;&gt;\(z&amp;gt;0.\)&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; are independent, then the joint PDF of these RVs will be the product of individual PDFs, that is &lt;span class=&#34;math inline&#34;&gt;\(f_{(\eta,\xi)}(x,y)=f_{\xi}(x)f_{\eta}(y).\)&lt;/span&gt; Therefore,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[F_\zeta(z)=P(\eta-\xi&amp;lt;z)=\int_0^{+\infty}\int_0^{+\infty}I_{\{y-x\leq z\}}(x,y)\lambda\mu e^{-\lambda x-\mu y}d x dy.\]&lt;/span&gt;
Making the following variable change &lt;span class=&#34;math inline&#34;&gt;\(u=y-x\)&lt;/span&gt; will give
&lt;span class=&#34;math display&#34;&gt;\[F_\zeta(z)=P(\eta-\xi&amp;lt;z)=\lambda\mu \int_0^{+\infty}e^{-(\lambda + \mu) x}\left(\int_{-x}^{z}e^{-\mu u}d u\right) dx=1-\frac{\lambda}{\lambda+\mu}e^{-\mu z},\ \ z&amp;gt;0.\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(z&amp;lt;0\)&lt;/span&gt; then
&lt;span class=&#34;math display&#34;&gt;\[P(\eta-\xi&amp;lt;z)=P(\xi-\eta&amp;gt;-z)=1-P(\xi-\eta&amp;lt;-z)=1-\left(1-\frac{\mu}{\mu+\lambda}e^{\lambda z}\right)=\frac{\mu}{\lambda+\mu}e^{\lambda z},\ \ z&amp;lt;0.\]&lt;/span&gt;
Therefore,
&lt;span class=&#34;math display&#34;&gt;\[F_\zeta(z)=\left\{\begin{matrix}
&amp;amp;1-\frac{\lambda}{\lambda+\mu} e^{-\mu z}, &amp;amp;z\geq 0,\\
&amp;amp;\frac{\mu}{\lambda+\mu} e^{\lambda z} &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
The PDF will be
&lt;span class=&#34;math display&#34;&gt;\[f_\zeta(z)=\frac{\lambda\mu}{\lambda+\mu}\left\{\begin{matrix}
&amp;amp;e^{-\mu z}, &amp;amp;z\geq 0,\\
&amp;amp;e^{\lambda z} &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
This formula can be checked using simulations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())

n &amp;lt;- 20000
m &amp;lt;- 10000
lambda &amp;lt;- 2
mu &amp;lt;- 5


  X &amp;lt;- rexp(n,lambda)
  Y &amp;lt;- rexp(m, mu)
  Z &amp;lt;- Y-X
  
int &amp;lt;- seq(min(Z),max(Z),(max(Z)-min(Z))/100)
dens &amp;lt;- function(z) (lambda*mu)/(lambda+mu)*ifelse(z&amp;gt;=0,exp(-mu*z),exp(lambda*z))
hist(Z, nclass = 100, freq=FALSE,ylim=c(0,1.5),
     xlim = c(min(Z),max(Z)), main=&amp;quot;&amp;quot;, col = &amp;quot;lightblue&amp;quot;, border = &amp;quot;blue&amp;quot;)
lines(int,dens(int), col=&amp;quot;2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the PDF we can calculate also
&lt;span class=&#34;math display&#34;&gt;\[E(\zeta)=\frac{\lambda-\mu}{\lambda\mu},\ \ P(\eta&amp;gt;\xi)=\frac{\lambda}{\lambda+\mu}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;p&gt;Suppose that &lt;span class=&#34;math inline&#34;&gt;\(\eta\sim{\mathbb E}(\mu)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta&amp;#39;\sim{\mathbb E}(\mu&amp;#39;)\)&lt;/span&gt; are independent. Find the distribution of the RVs &lt;span class=&#34;math inline&#34;&gt;\(\min(\eta,\eta&amp;#39;)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\max(\eta,\eta&amp;#39;).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For all &lt;span class=&#34;math inline&#34;&gt;\(z&amp;gt;0\)&lt;/span&gt; we have
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
P(\min(\eta,\eta&amp;#39;)&amp;lt;z)=1-P(\min(\eta,\eta&amp;#39;)\geq z)=1-P(\eta\geq z,\eta&amp;#39;\geq z)=1-e^{-(\mu+\mu&amp;#39;) z},\,z&amp;gt;0,
\end{align*}\]&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(P(\min(\eta,\eta&amp;#39;)&amp;lt;z)=0,\ \ z\leq 0.\)&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\min(\eta,\eta&amp;#39;)\sim{\mathbb E}(\mu+\mu&amp;#39;)\)&lt;/span&gt;, that is, the minimum of two exponentially distributed RVs is an exponentially distributed RV as well, with the rate being equal to the sum of the rates of the given two RVs.&lt;/p&gt;
&lt;p&gt;On the other hand,
&lt;span class=&#34;math display&#34;&gt;\[P(\max(\eta, \eta&amp;#39;) &amp;lt; z)= P(\eta &amp;lt; z, \eta&amp;#39; &amp;lt;z)=(1-e^{-\mu z})(1-e^{-\mu&amp;#39; z}),\,z&amp;gt;0.\]&lt;/span&gt;
For &lt;span class=&#34;math inline&#34;&gt;\(\mu=\mu&amp;#39;\)&lt;/span&gt; we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\max(\eta, \eta&amp;#39;) &amp;lt; z) = (1-e^{-\mu z})^2,\,z&amp;gt;0,\]&lt;/span&gt;
with PDF being equal to &lt;span class=&#34;math inline&#34;&gt;\(f(z)=2\mu(1-e^{-\mu z})e^{-\mu z},\,z&amp;gt;0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f(z)=0,\,z&amp;lt;0.\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 3&lt;/h2&gt;
&lt;p&gt;For given three independent RVs such that &lt;span class=&#34;math inline&#34;&gt;\(\xi\sim{\mathbb E}(\lambda)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta,\eta&amp;#39;\sim{\mathbb E}(\mu),\)&lt;/span&gt; calculate the following probability
&lt;span class=&#34;math inline&#34;&gt;\(\theta=P(\xi&amp;lt;\eta,\xi&amp;lt;\eta&amp;#39;).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can reformulate this problem as follows
&lt;span class=&#34;math display&#34;&gt;\[\theta=P(\xi&amp;lt;\eta,\xi&amp;lt;\eta&amp;#39;)=P(\xi&amp;lt;\min(\eta,\eta&amp;#39;))=P(\xi-\min(\eta,\eta&amp;#39;)&amp;lt;0).\]&lt;/span&gt;
Hence, denoting by &lt;span class=&#34;math inline&#34;&gt;\(\zeta=\xi-\min(\eta,\eta&amp;#39;),\)&lt;/span&gt; in the first step we can calculate the distribution function of the RV &lt;span class=&#34;math inline&#34;&gt;\(\zeta.\)&lt;/span&gt; We have &lt;span class=&#34;math inline&#34;&gt;\(\xi\sim{\mathbb E}(\xi)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\min(\eta,\eta&amp;#39;)\sim{\mathbb E}(2\mu)\)&lt;/span&gt; (see Example 2), hence, using the Example 1 we obtain&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[F_\zeta(z)=\left\{\begin{matrix}
&amp;amp;1-\frac{2\mu}{\lambda+2\mu} e^{-\lambda z}, &amp;amp;z\geq 0,\\
&amp;amp;\frac{\lambda}{\lambda+2\mu} e^{2\mu z} &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
For the PDF we have
&lt;span class=&#34;math display&#34;&gt;\[f_\zeta(z)=\left\{\begin{matrix}
&amp;amp;\frac{2\lambda\mu}{\lambda+2\mu} e^{-\lambda z}, &amp;amp;z&amp;gt; 0,\\
&amp;amp;\frac{2\lambda\mu}{\lambda+2\mu} e^{2\mu z} &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
We can, again, check this using simulations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())

n &amp;lt;- 200000
m &amp;lt;- 100000
lambda &amp;lt;- 2.5
mu &amp;lt;- 1.3


X1 &amp;lt;- rexp(n,mu)
X2 &amp;lt;- rexp(n,mu)
Y &amp;lt;- rexp(m, lambda)
Z &amp;lt;- Y - ifelse(X1&amp;gt;=X2,X2,X1)
Coeff &amp;lt;- 2*lambda*mu/(lambda+2*mu)

int &amp;lt;- seq(min(Z), max(Z), (max(Z)-min(Z))/100)
dens &amp;lt;- function(z) Coeff*ifelse(z&amp;gt;=0, exp(-lambda*z), exp(2*mu*z))


hist(Z, nclass = 100, freq=FALSE, ylim=c(0,1.5), 
     xlim=c(min(Z), max(Z)), main = &amp;quot;&amp;quot;, border = &amp;quot;blue&amp;quot;, col = &amp;quot;lightblue&amp;quot;)
lines(int,dens(int), col=&amp;quot;2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Therefore,
&lt;span class=&#34;math display&#34;&gt;\[\theta=F_\zeta(0)=\frac{\lambda}{\lambda+2\mu}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 4&lt;/h2&gt;
&lt;p&gt;For given three independent RVs such that &lt;span class=&#34;math inline&#34;&gt;\(\xi,\xi&amp;#39;\sim{\mathbb E}(\lambda)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\sim{\mathbb E}(\mu),\)&lt;/span&gt; calculate the following probability
&lt;span class=&#34;math inline&#34;&gt;\(\vartheta=P(\xi&amp;lt;\eta,\xi&amp;#39;&amp;lt;\eta).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can reformulate this problem as follows
&lt;span class=&#34;math display&#34;&gt;\[\vartheta=P(\xi&amp;lt;\eta,\xi&amp;#39;&amp;lt;\eta)=P(\max(\xi,\xi&amp;#39;)&amp;lt;\eta)=P(\eta-\max(\xi,\xi&amp;#39;)&amp;gt;0).\]&lt;/span&gt;
Hence, denoting by &lt;span class=&#34;math inline&#34;&gt;\(\kappa=\eta-\max(\xi,\xi&amp;#39;),\)&lt;/span&gt; in the first step we can calculate the distribution function of the RV &lt;span class=&#34;math inline&#34;&gt;\(\kappa.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
P(\kappa&amp;lt;z)=\int_0^{+\infty}\int_0^{+\infty}I(y-x\leq z)2\lambda\mu(1-e^{-\lambda x})e^{-\lambda x-\mu y}d x d y.
\end{align*}\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(z&amp;gt;0\)&lt;/span&gt; then, denoting &lt;span class=&#34;math inline&#34;&gt;\(y-x=u\)&lt;/span&gt; we get &lt;span class=&#34;math inline&#34;&gt;\(y=u+x,\,u\in[-x,\infty)\ \ d y= d u.\)&lt;/span&gt; Therefore,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
P(\kappa&amp;lt;z)&amp;amp;=2\lambda\mu\int_0^{+\infty}(1-e^{-\lambda x})e^{-\lambda x-\mu x}\int_{-x}^{z}e^{-\mu u}d u d x=\\
&amp;amp;=-2\lambda\int_0^{+\infty}(e^{-(\lambda+\mu) x -\mu z}-e^{-(2\lambda+\mu) x -\mu z}-e^{-\lambda x }+e^{-2\lambda x }) d x=\\
&amp;amp;=1-\frac{2\lambda^2}{(\lambda+\mu)(2\lambda+\mu)}e^{-\mu z },\ \ z&amp;gt;0.
\end{align*}\]&lt;/span&gt;
For &lt;span class=&#34;math inline&#34;&gt;\(z&amp;gt;0\)&lt;/span&gt; calculate also (using the notation &lt;span class=&#34;math inline&#34;&gt;\(x-y=u\)&lt;/span&gt;, which entails &lt;span class=&#34;math inline&#34;&gt;\(x=y+u,\,d x=d u,\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(u\in[-y,+\infty).\)&lt;/span&gt;)
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
P(\max(\xi,\xi&amp;#39;)-\eta&amp;lt;z)&amp;amp;=\int_0^{+\infty}\int_0^{+\infty}I(x-y\leq z)2\lambda\mu(1-e^{-\lambda x})e^{-\lambda x-\mu y}d x d y=\\
&amp;amp;=\int_0^{+\infty}\int_{-y}^{z}(e^{-\lambda(y+u)  - \mu y} - e^{-2\lambda(y+u)  -\mu y}d u dy)=\\
&amp;amp;=1+\mu\left[\frac{e^{-2\lambda z}}{2\lambda+\mu}-\frac{2e^{-\lambda z}}{\lambda+\mu}\right],\ \ z\geq 0.
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, for &lt;span class=&#34;math inline&#34;&gt;\(z&amp;gt;0\)&lt;/span&gt; we have
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
P(\kappa&amp;lt;-z)&amp;amp;=1-P(\max(\xi,\xi&amp;#39;)-\eta&amp;lt;z)=\\
&amp;amp;=-\mu\left[\frac{e^{-2\lambda z}}{2\lambda+\mu}-\frac{2e^{-\lambda z}}{\lambda+\mu}\right],\ \ z\geq 0
\end{align*}\]&lt;/span&gt;
Finally we obtain
&lt;span class=&#34;math display&#34;&gt;\[F_\kappa(z)=\left\{\begin{matrix}
&amp;amp;1-\frac{2\lambda^2}{(\lambda+\mu)(2\lambda+\mu)} e^{-\mu z}, &amp;amp;z\geq 0,\\
&amp;amp;-\mu\left[\frac{e^{2\lambda z}}{2\lambda+\mu}-\frac{2e^{\lambda z}}{\lambda+\mu}\right] &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
For the PDF we have
&lt;span class=&#34;math display&#34;&gt;\[f_\kappa(z)=\left\{\begin{matrix}
&amp;amp;\frac{2\lambda^2\mu}{(\lambda+\mu)(2\lambda+\mu)} e^{-\mu z}, &amp;amp;z&amp;gt; 0,\\
&amp;amp;-2\lambda\mu\left[\frac{e^{2\lambda z}}{2\lambda+\mu}-\frac{e^{\lambda z}}{\lambda+\mu}\right] &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
To check this formula we can make the following simulations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())

n &amp;lt;- 200000
m &amp;lt;- 100000
lambda &amp;lt;- 2.5
mu &amp;lt;- 5.5


X1 &amp;lt;- rexp(n,lambda)
X2 &amp;lt;- rexp(n,lambda)
Y &amp;lt;- rexp(m, mu)
Z &amp;lt;- Y - ifelse(X1 &amp;gt;= X2, X1, X2)
Coeff &amp;lt;- 2*mu*lambda^2/((lambda+mu)*(2*lambda+mu))
Coeff1 &amp;lt;- -2*lambda*mu/(2*lambda+mu)
Coeff2 &amp;lt;- -2*lambda*mu/(lambda+mu)

int &amp;lt;- seq(min(Z), max(Z), (max(Z)-min(Z))/100)

dens &amp;lt;- function(z) ifelse(z&amp;gt;=0, Coeff*exp(-mu*z),
                           Coeff1*exp(2*lambda*z)-Coeff2*exp(lambda*z))



hist(Z, nclass = 100, freq=FALSE,ylim=c(0, 1.5), xlim=c(min(Z), max(Z)),
     main = &amp;quot;&amp;quot;, col = &amp;quot;lightblue&amp;quot;, border = &amp;quot;blue&amp;quot;)
lines(int,dens(int), col=&amp;quot;2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Therefore,
&lt;span class=&#34;math display&#34;&gt;\[\vartheta=P(\xi&amp;lt;\eta, \xi&amp;#39;&amp;lt;\eta)=P(\kappa&amp;gt;0)=\frac{2\lambda^2}{(\lambda+\mu)(2\lambda+\mu)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dose-Response Curves in R</title>
      <link>https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown/</link>
      <pubDate>Thu, 13 Sep 2018 09:00:00 +0400</pubDate>
      <guid>https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown/</guid>
      <description>
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://gasparyan.co/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In clinical research the doseâresponse relationship is often non-linear, therefore advanced fitting models are needed to capture their behavior. The talk will concentrate on fitting non-linear parametric models to the doseâresponse data and will explain some specificities of this problem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The talk is based on the book by &lt;strong&gt;Christian Ritz, Jens Carl Streibig âNonlinear regression with Râ.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;purpose-of-dose-response-information&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Purpose of Dose-Response Information&lt;/h1&gt;
&lt;div id=&#34;ich-e4-harmonized-tripartite-guideline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ICH E4 Harmonized Tripartite Guideline&lt;/h2&gt;
&lt;p&gt;Knowledge of the relationships among dose, drug-concentration in blood, and clinical
response (effectiveness and undesirable effects) is important for the safe and effective
use of drugs in individual patients. It helps identify&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;an appropriate starting dose,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the best way to adjust dosage to the needs of a particular patient,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;a dose beyond which increases would be unlikely to provide added benefit or would
produce unacceptable side effects.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;There are other fields of studies were concentration analysis can be used.&lt;/p&gt;
&lt;p&gt;Consider the following relationship (&lt;em&gt;Chwirut2&lt;/em&gt; is included in the package &lt;strong&gt;NISTnls&lt;/strong&gt;),
where the response variable is ultrasonic response, and the predictor variable is metal distance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#library(NISTnls)
dim(Chwirut2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 54  2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(Chwirut2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         y     x
## 1 92.9000 0.500
## 2 57.1000 1.000
## 3 31.0500 1.750
## 4 11.5875 3.750
## 5  8.0250 5.750
## 6 63.6000 0.875&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;A distance-amplitude relationship of attenuation that an ultrasound beam experiences traveling through a medium&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#library(ggplot2)
g &amp;lt;- ggplot(data=Chwirut2,aes(x=x,y=y)) + 
  geom_point() + xlab(&amp;quot;Metal distance&amp;quot;) + ylab(&amp;quot;Ultrasonic respons&amp;quot;)
g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-built-in-function-nls&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The built-in function &lt;em&gt;nls()&lt;/em&gt;&lt;/h1&gt;
&lt;div id=&#34;fitting-a-parametric-family&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting a parametric family&lt;/h2&gt;
&lt;p&gt;Suppose we observe the following pair of data points &lt;span class=&#34;math inline&#34;&gt;\((x_i,y_i),\,i=1,\cdots,n.\)&lt;/span&gt; Consider a parametric family (usually non-linear) that we want to fit to the data
&lt;span class=&#34;math display&#34;&gt;\[f(x,(\theta_1,\cdots,\theta_k)).\]&lt;/span&gt;
The hypothesis is that there is the following relationship between observed points&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i=f(x_i,(\theta_1,\cdots,\theta_k))+\varepsilon_i,\ \ i=1,\cdots,n.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The fitting method that we are going to use is &lt;em&gt;the Non-linear Least Squares (NLS)&lt;/em&gt; method. NLS estimates can be found using this minimization problem.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[(\hat\theta_1,\cdots,\hat\theta_k)=\arg\min_{\theta_1,\cdots,\theta_k}\sum_{i=1}^n(y_i-f(x_i,(\theta_1,\cdots,\theta_k)))^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gauss---newton-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gauss - Newton algorithm&lt;/h2&gt;
&lt;p&gt;The Gauss - Newton algorithm is used to solve the optimization problem described above.&lt;/p&gt;
&lt;p&gt;If we denote &lt;span class=&#34;math inline&#34;&gt;\(g_i(\theta)=y_i-f(x_i,\theta), \ \ \theta=(\theta_1,\cdots,\theta_k),\)&lt;/span&gt;
then the minimization problem is the following
&lt;span class=&#34;math display&#34;&gt;\[\min_{\theta\in R^k}\sum_{i=1}^ng_i^2(\theta)=\min_{\theta\in R^k}||g(\theta)||^2.\]&lt;/span&gt;
Choose an initial value &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; and linearize the function &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt; around &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; as follows&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat g(\theta,\theta_0)=g(\theta_0)+Dg(\theta_0)(\theta-\theta_0).\]&lt;/span&gt;
Solving &lt;span class=&#34;math inline&#34;&gt;\(\min_{\theta\in R^k}||g(\theta_0)+Dg(\theta_0)(\theta-\theta_0)||^2,\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(Dg(\theta)\)&lt;/span&gt; is the Jacobian, we get &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt; and continuing this process interatively, we get the sequence &lt;span class=&#34;math inline&#34;&gt;\((\theta_m),\,m=0,1,\cdots.\)&lt;/span&gt; Moreover, if &lt;span class=&#34;math inline&#34;&gt;\(Dg(\theta_k)\)&lt;/span&gt; has linearly independent columns,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_{k+1}=\theta_k-\left([Dg(\theta_k)]^TDg(\theta_k)\right)^{-1}[Dg(\theta_k)]^Tg(\theta_k).\]&lt;/span&gt;
The question is - how to choose the initial value &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; of iteration?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-parametric-family&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The parametric family&lt;/h2&gt;
&lt;p&gt;The following parametric family was suggested to fit the distance-amplitude relationship&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x,(\beta_1,\beta_2,\beta_3))=\frac{\exp(-\beta_1x)}{\beta_2+\beta_3x}\]&lt;/span&gt;
Assumptions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;correct mean function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;variance homogeneity (homoscedasticity)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;normally distributed measurements errors&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;mutually independent measurement errors &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;If we take &lt;span class=&#34;math inline&#34;&gt;\(x=0\)&lt;/span&gt; then we get &lt;span class=&#34;math inline&#34;&gt;\(f(0,(\beta_1,\beta_2,\beta_3))=\frac{1}{\beta_2}\)&lt;/span&gt; so the initial estimate of &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; can be the reciprocal of the response value closest to the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis, that is, roughly, &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{100}=0.01.\)&lt;/span&gt; For the&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-non-linear-parametric-family&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting non-linear parametric family&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expFct &amp;lt;- function(x,beta1,beta2=0.01,beta3)  exp(-beta1*x)/(beta2+beta3*x)
l &amp;lt;- list(beta1=0.1, beta2=0.01, beta3=0.001)
fit &amp;lt;- nls(data=Chwirut2, y ~ expFct(x, beta1, beta2, beta3), start=l)
fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Nonlinear regression model
##   model: y ~ expFct(x, beta1, beta2, beta3)
##    data: Chwirut2
##    beta1    beta2    beta3 
## 0.166576 0.005165 0.012150 
##  residual sum-of-squares: 513
## 
## Number of iterations to convergence: 9 
## Achieved convergence tolerance: 7.467e-07&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 5
##   term  estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 beta1  0.167    0.0383        4.35 6.56e- 5
## 2 beta2  0.00517  0.000666      7.75 3.54e-10
## 3 beta3  0.0122   0.00153       7.94 1.81e-10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-the-fitted-non-linear-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting the fitted non-linear function&lt;/h2&gt;
&lt;p&gt;Hence, we can plot the fitted curve&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#library(broom)
values &amp;lt;- tidy(fit)$estimate
g + stat_function(fun=expFct, args=values, colour=&amp;quot;blue&amp;quot;, lwd=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-package-nls2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The package &lt;em&gt;nls2&lt;/em&gt;&lt;/h1&gt;
&lt;div id=&#34;searching-a-grid&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Searching a grid&lt;/h2&gt;
&lt;p&gt;The procedure of finding initial points for the minimization iteration can be automated in the following way. In case the range of the parameter estimates can be envisaged grid search can be carried out:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;the residual sums-of-squares &lt;span class=&#34;math inline&#34;&gt;\(RSS(\beta),\,\beta=(\beta_1,\cdots,\beta_k)\)&lt;/span&gt; is calculated for all the values in the intervals&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;starting values are chosen in the way to provide the smallest value of &lt;span class=&#34;math inline&#34;&gt;\(RSS(\beta)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;In our example, having &lt;span class=&#34;math inline&#34;&gt;\(\beta_2=0.01,\)&lt;/span&gt; for other two parameters, knowing only that they are positive numbers, we can search in the interval &lt;span class=&#34;math inline&#34;&gt;\([0.1,1]\)&lt;/span&gt; taking the step size equal to &lt;span class=&#34;math inline&#34;&gt;\(0.1.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Create a &lt;em&gt;data.frame&lt;/em&gt; containing all possible combinations of suggested initial values for the parameters&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta1 &amp;lt;- seq(0.1, 1, by = 0.1)
beta2 &amp;lt;- 0.01
beta3 &amp;lt;- seq(0.1, 1, by = 0.1)
grid.Chwirut2 &amp;lt;- expand.grid(list(beta1 = beta1, beta2 = beta2, beta3 = beta3))
head(grid.Chwirut2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   beta1 beta2 beta3
## 1   0.1  0.01   0.1
## 2   0.2  0.01   0.1
## 3   0.3  0.01   0.1
## 4   0.4  0.01   0.1
## 5   0.5  0.01   0.1
## 6   0.6  0.01   0.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-nls2-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;em&gt;nls2()&lt;/em&gt; function&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;nls2()&lt;/em&gt; function differs from the original function by a way that it provides the possibility to specify a data frame of values of starting points. Its argument &lt;em&gt;algorithm=âbrute-forceâ&lt;/em&gt; evaluates &lt;em&gt;RSS&lt;/em&gt; for the parameter values provided through the &lt;em&gt;start&lt;/em&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nls2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- nls2(data = Chwirut2, y ~ expFct(x, beta1, beta2, beta3), 
             start = grid.Chwirut2, algorithm = &amp;quot;brute-force&amp;quot;)
fit2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Nonlinear regression model
##   model: y ~ expFct(x, beta1, beta2, beta3)
##    data: Chwirut2
## beta1 beta2 beta3 
##  0.10  0.01  0.10 
##  residual sum-of-squares: 60696
## 
## Number of iterations to convergence: 100 
## Achieved convergence tolerance: NA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-using-the-function-nls2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting using the function &lt;em&gt;nls2()&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Fitting procedure can be continued this way: replace the value of the &lt;em&gt;start&lt;/em&gt; argument by &lt;em&gt;fit2&lt;/em&gt; and leave out the argument &lt;em&gt;algorithm&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(nls2(data = Chwirut2, y ~ expFct(x, beta1, beta2, beta3), start=fit2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 5
##   term  estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 beta1  0.167    0.0383        4.35 6.56e- 5
## 2 beta2  0.00517  0.000666      7.75 3.54e-10
## 3 beta3  0.0121   0.00153       7.94 1.81e-10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;self-starter-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Self-Starter functions&lt;/h1&gt;
&lt;div id=&#34;automating-further-the-search-for-initial-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Automating further the search for initial points&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Self-starter functions are special type of functions. They are implementations of specific given mean functions and designed in a way to calculate starting values for a given dataset. They can be thought as the definition of the parametric family combined with the logic of how to choose initial values based on the given dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;We have already encountered such example: for the parametric family
&lt;span class=&#34;math display&#34;&gt;\[f(x,(\beta_1,\beta_2,\beta_3))=\frac{\exp(-\beta_1x)}{\beta_2+\beta_3x}\]&lt;/span&gt;
we said that the initial estimate of &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; can be the reciprocal of the response value closest to the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis. So, if we find also logic of choosing initial values for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3,\)&lt;/span&gt; then we can add these logics into the definition of this parametric family and get a Self-Starter function.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;We will construct a Self-Starter function later, but before that we will explore the built-in Self-Starter functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;built-in-self-starters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Built-in Self-Starters&lt;/h2&gt;
&lt;p&gt;Consider the following dataset&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- data.frame(
  conc = c(2.856829,  5.005303,  7.519473,  22.101664,  
         27.769976,  39.198025,  45.483269, 203.784238),
rate = c(14.58342, 24.74123, 31.34551, 72.96985, 77.50099, 
         96.08794, 96.96624, 108.88374)
)
dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         conc      rate
## 1   2.856829  14.58342
## 2   5.005303  24.74123
## 3   7.519473  31.34551
## 4  22.101664  72.96985
## 5  27.769976  77.50099
## 6  39.198025  96.08794
## 7  45.483269  96.96624
## 8 203.784238 108.88374&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- ggplot(data = dat, aes(x = conc, y = rate)) + geom_point()
g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-michaelis-menten-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Michaelis-Menten model&lt;/h2&gt;
&lt;p&gt;We are going to fit this data using the parametric family of Michaelis-Menten functions&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x,(V_m,k))=\frac{V_m x}{K+x}.\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Here the constants &lt;span class=&#34;math inline&#34;&gt;\(V_m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; are positive. &lt;span class=&#34;math inline&#34;&gt;\(V_{m}\)&lt;/span&gt; represents the maximum rate achieved by the system, at saturating substrate concentration. The constant &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is the substrate concentration at which the reaction rate is half of &lt;span class=&#34;math inline&#34;&gt;\(V_m.\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Again we can interactively find the initial estimates for the parameters to fit the NLS curve. But there is a built-in Self-Starter function called &lt;em&gt;SSmicmen()&lt;/em&gt; which has a logic inscribed in it which lets it work with &lt;em&gt;nls()&lt;/em&gt; without specifying starting points.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3 &amp;lt;- nls(rate ~ SSmicmen(conc, Vm, K), data = dat)
tidy(fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   term  estimate std.error statistic    p.value
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1 Vm       126.       7.17     17.6  0.00000218
## 2 K         17.1      2.95      5.78 0.00117&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g + stat_function(fun = SSmicmen, args = list(Vm = tidy(fit3)$estimate[1],
                                       K=tidy(fit3)$estimate[2]), 
                  colour = &amp;quot;darkgreen&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;defining-self-starter-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Defining Self-Starter functions&lt;/h1&gt;
&lt;div id=&#34;a-simple-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A simple example&lt;/h2&gt;
&lt;p&gt;Consider the following parametric family&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x,(b,y_0))=y_0 e^{\frac{x}{b}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;This model can be used to describe radioactive decay. The parameter &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; is
the initial amount of the radioactive substance (at time &lt;span class=&#34;math inline&#34;&gt;\(x = 0\)&lt;/span&gt;). The rate of
decay is governed by the second parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; (the inverse decay constant).&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The definition of a self-starter contains the formula of the parametric family, a logic of constructing initial values for parameters, a function taking the dataset as argument and other arguments ensuring that the logic of construction of initial values, the definition of parametric family and &lt;em&gt;nls()&lt;/em&gt; are tied together.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logic-of-construction-of-initial-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The logic of construction of initial values&lt;/h2&gt;
&lt;p&gt;Apply the log transform on the given mean function&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log f(x,(b,y_0))=\log y_0+\frac{x}{b}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence, if we apply the log transform on the response data, we can apply linear regression techniques to estimate the slope and the intercept, then, from the equalities&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log\tilde y_0={\beta_0}, \ \ \tilde \beta_1=\frac{1}{b}\]&lt;/span&gt;
we get&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tilde y_0=e^{\tilde \beta_0}, \ \ \tilde b=\frac{1}{\tilde\beta_1}.\]&lt;/span&gt;
This is the logic of construction of initial parameter values that we need to include in the definition of a Self-Starter.&lt;/p&gt;
&lt;p&gt;(We are not interested in the error structure that will change after the transformation, because we need only initial estimates.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-match.call-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;em&gt;match.call()&lt;/em&gt; function&lt;/h2&gt;
&lt;p&gt;The function &lt;em&gt;match.call()&lt;/em&gt; can be used in the definition of the function to store the call in the resulting object. In addition, if that function is called without specifying the arguments, the function &lt;em&gt;match.call()&lt;/em&gt; matches the arguments to their names by their positions. Observe below&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Func &amp;lt;- function(input, parameter){
  value &amp;lt;- input^parameter
  attr(value,&amp;quot;call&amp;quot;) &amp;lt;- match.call()
  value
}
power &amp;lt;- Func(2,16) #observe that we did not specify the names of parameters
power&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 65536
## attr(,&amp;quot;call&amp;quot;)
## Func(input = 2, parameter = 16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hence, their argument values can be extracted from the call by their names as from a list&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attr(power,&amp;quot;call&amp;quot;)$parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;three-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Three steps&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Define the mean function&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expModel &amp;lt;- function(predictor,b, y0) {
  y0 * exp(predictor/b)
}&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Define the function with the initialization logic&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expModelInit &amp;lt;- function(mCall, LHS, data) {
 xy &amp;lt;- sortedXyData(mCall[[&amp;quot;predictor&amp;quot;]], LHS, data)
 lmFit &amp;lt;- lm(log(xy[, &amp;quot;y&amp;quot;]) ~ xy[, &amp;quot;x&amp;quot;])
 coefs &amp;lt;- coef(lmFit)
 y0 &amp;lt;- exp(coefs[1])
 b &amp;lt;- 1/coefs[2]
 value &amp;lt;- c(b, y0)
 names(value) &amp;lt;- c(&amp;quot;b&amp;quot;,&amp;quot;y0&amp;quot;)
 value
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;define-the-self-starter-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Define the Self-Starter function&lt;/h1&gt;
&lt;div id=&#34;specificities-of-working-in-a-markdown-environment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specificities of working in a Markdown Environment&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SSexp &amp;lt;- selfStart(expModel, expModelInit, c(&amp;quot;b&amp;quot;, &amp;quot;y0&amp;quot;))
class(SSexp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;selfStart&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;RStudio actually creates a separate R session to render the document. This causes problem with the &lt;em&gt;getInitial()&lt;/em&gt; function which searches for user created SS functions in the Global environment and cannot find them because they are created in a new environment only for the Markdown use. Hence, we need to ensure that our created Self-Starter is defined in the Global environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Only for Markdown execution
l &amp;lt;- list(SSexp = SSexp)
list2env(l, envir = .GlobalEnv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;environment: R_GlobalEnv&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-the-inital-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting the inital values&lt;/h2&gt;
&lt;p&gt;Self-starters can be passed as an argument to the function &lt;em&gt;getInitial()&lt;/em&gt; to obtain the initial values. Consider the following dataset&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(file = &amp;quot;RGRcurve.rda&amp;quot;)
head(RGRcurve)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Day   RGR
## 1   0 0.169
## 2   0 0.164
## 3   0 0.210
## 4   0 0.215
## 5   0 0.183
## 6   0 0.181&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, the initial values, determined according to our logic, will be&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getInitial(RGR ~ SSexp(Day, b,y0), data = RGRcurve)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         b        y0 
## 3.8450187 0.1674235&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-by-user-defined-ss&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting by user defined SS&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit4 &amp;lt;- nls(RGR ~ SSexp(Day, b,y0), data = RGRcurve)
getInitial(RGR ~ SSexp(Day, b,y0), data = RGRcurve)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         b        y0 
## 3.8450187 0.1674235&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compare the initial values with the estimated values
tidy(fit4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   term  estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 b        3.76     0.175       21.5 3.40e-23
## 2 y0       0.164    0.0142      11.5 4.03e-14&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- ggplot(data = RGRcurve, aes(x = Day, y = RGR)) + geom_point()
g + stat_function(fun = SSexp, args=tidy(fit4)$estimate, colour=&amp;quot;darkgreen&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;difficult-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Difficult Example&lt;/h1&gt;
&lt;div id=&#34;returning-to-the-difficult-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Returning to the difficult example&lt;/h2&gt;
&lt;p&gt;Considering once again the parametric family&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x,\beta)=\frac{\exp(-\beta_1x)}{\beta_2+\beta_3x},\ \ \beta=(\beta_1,\beta_2,\beta_3).\]&lt;/span&gt;
We can use Taylorâs formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x,\beta)\approx f(0,\beta)+f&amp;#39;(0,\beta)x,\ \ x\approx 0.\]&lt;/span&gt;
Simplifying this and remembering that &lt;span class=&#34;math inline&#34;&gt;\(f(0,\beta)=\frac{1}{\beta_2}\)&lt;/span&gt; we get
&lt;span class=&#34;math display&#34;&gt;\[f(x,\beta)\approx \frac{1}{\beta_2}-\frac{1}{\beta_2}\left(\beta_1+\frac{\beta_3}{\beta_2}\right)x.\]&lt;/span&gt;
Which in turn gives the equality
&lt;span class=&#34;math display&#34;&gt;\[1-\beta_2 f(x,\beta)=\left(\beta_1+\frac{\beta_3}{\beta_2}\right)x,\]&lt;/span&gt;
hence, transforming the data according to the left hand side and fitting a linear model without an intercept we can estimate &lt;span class=&#34;math inline&#34;&gt;\(\left(\beta_1+\frac{\beta_3}{\beta_2}\right).\)&lt;/span&gt; For two parameters out of three we have a rule to find starting values, for the third one we still need a guess. Taking &lt;span class=&#34;math inline&#34;&gt;\(\beta_1=0\)&lt;/span&gt; may work.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expFctInit &amp;lt;- function(mCall, LHS, data){
  xy &amp;lt;- sortedXyData(mCall[[&amp;quot;x&amp;quot;]], LHS, data)
  beta2 &amp;lt;- 1/xy[1,&amp;quot;y&amp;quot;]
  coefs &amp;lt;- coef(lm(1-beta2*xy[,&amp;quot;y&amp;quot;]~xy[,&amp;quot;x&amp;quot;]+0))
  beta1 &amp;lt;- 0
  beta3 &amp;lt;- coefs*beta2
  value &amp;lt;- c(beta1, beta2, beta3)
  names(value) &amp;lt;- c(&amp;quot;beta1&amp;quot;, &amp;quot;beta2&amp;quot;, &amp;quot;beta3&amp;quot;)
  value
}
SSexpFct &amp;lt;- selfStart(expFct, expFctInit, c(&amp;quot;beta1&amp;quot;, &amp;quot;beta2&amp;quot;, &amp;quot;beta3&amp;quot;))
l &amp;lt;- list(SSexpFct=SSexpFct)
list2env(l, envir = .GlobalEnv)#Only for Markdown execution&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;environment: R_GlobalEnv&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getInitial(data = Chwirut2, y ~ SSexpFct(x, beta1, beta2, beta3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       beta1       beta2       beta3 
## 0.000000000 0.012140834 0.002537341&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit0 &amp;lt;- nls(data = Chwirut2, y ~ SSexpFct(x, beta1, beta2, beta3))&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit0_tidy &amp;lt;- tidy(fit0)
fit0_tidy$p.value &amp;lt;- format.pval(fit0_tidy$p.value,eps=0.001,digits=3)
CI &amp;lt;- as.data.frame(suppressMessages(confint(fit0)))
CI$term &amp;lt;- row.names(CI)
fit0_tidy &amp;lt;- merge(fit0_tidy,CI,by=&amp;quot;term&amp;quot;)
fit0_tidy[,!names(fit0_tidy)%in%c(&amp;quot;term&amp;quot;, &amp;quot;p.value&amp;quot;)]&amp;lt;-
  round(fit0_tidy[,!names(fit0_tidy)%in%c(&amp;quot;term&amp;quot;, &amp;quot;p.value&amp;quot;)], 3)
fit0_tidy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    term estimate std.error statistic p.value  2.5% 97.5%
## 1 beta1    0.167     0.038     4.349  &amp;lt;0.001 0.094 0.254
## 2 beta2    0.005     0.001     7.753  &amp;lt;0.001 0.004 0.006
## 3 beta3    0.012     0.002     7.939  &amp;lt;0.001 0.009 0.015&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we used the generic function &lt;span class=&#34;math inline&#34;&gt;\(confint()\)&lt;/span&gt; on the object of class &lt;span class=&#34;math inline&#34;&gt;\(nls\)&lt;/span&gt; and got the confidence intervals for the parameters estimates. In addition, the function &lt;span class=&#34;math inline&#34;&gt;\(format.pval()\)&lt;/span&gt; was used to simplify the presentation of p.values.&lt;/p&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;values &amp;lt;- tidy(fit0)$estimate
g &amp;lt;- ggplot(data = Chwirut2, aes(x = x, y = y))+
  geom_point() + xlab(&amp;quot;Metal distance&amp;quot;) + ylab(&amp;quot;Ultrasonic respons&amp;quot;)
g + stat_function(fun = expFct, args = values,
                  colour=&amp;quot;blue&amp;quot;, lwd=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(fit0, newdata = data.frame(x=6:7))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.715006 3.453957&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Further Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The package &lt;em&gt;drc&lt;/em&gt; (Dose-Response curves) contains much more prespecified parametric families (22, to be exact - run &lt;em&gt;drc::getMeanFunctions()&lt;/em&gt; to see the list).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The package &lt;em&gt;nlme&lt;/em&gt; (Linear and Nonlinear Mixed Effects Models) which allows the inclusion of the donor effect as a random effect in dose-response studies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;More robust algorithms for fitting (&lt;em&gt;The Levenberg-Marquardt curve-fitting method&lt;/em&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-levenberg-marquardt-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Levenberg-Marquardt method&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The Levenberg-Marquardt curve-fitting method is a combination of the two other minimization methods: the gradient descent method and the Gauss-Newton method.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the gradient descent method, the sum of the squared errors is reduced by updating the parameters in the steepest-descent direction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the Gauss-Newton method, the sum of the squared errors is reduced by assuming the least squares function is locally quadratic, and finding the minimum of the quadratic.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Levenberg-Marquardt method acts more like a gradient-descent method when the parameters are far from their optimal value, and acts more like the Gauss-Newton method when the parameters are close to their optimal value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://people.duke.edu/~hpgavin/ce281/lm.pdf&#34;&gt;&lt;em&gt;Henri P. Gavin, The Levenberg-Marquardt method for nonlinear least squares curve-fitting problems&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;presentation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Presentation&lt;/h1&gt;
&lt;p&gt;Below is a presentation version of this lecture which contains also interactive applications for curve fitting.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
