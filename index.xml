<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Biostatistician</title>
    <link>https://gasparyan.co/</link>
      <atom:link href="https://gasparyan.co/index.xml" rel="self" type="application/rss+xml" />
    <description>Biostatistician</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Samvel B. Gasparyan</copyright><lastBuildDate>Thu, 23 Sep 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://gasparyan.co/images/icon_hu37ae94fde7f6135a8e8cfd653ea9ade8_11929814_512x512_fill_lanczos_center_2.png</url>
      <title>Biostatistician</title>
      <link>https://gasparyan.co/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>https://gasparyan.co/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://gasparyan.co/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>https://gasparyan.co/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://gasparyan.co/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Power and sample size calculation for the win odds test: application to an ordinal endpoint in COVID-19 trials</title>
      <link>https://gasparyan.co/publication/power-win-odds/</link>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://gasparyan.co/publication/power-win-odds/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hierarchical Composite Endpoints: Definition and Analysis Using the Win Ratio (with ties!)</title>
      <link>https://gasparyan.co/talk/asa2021/</link>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0200</pubDate>
      <guid>https://gasparyan.co/talk/asa2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dapagliflozin in patients with cardiometabolic risk factors hospitalised with COVID-19 (DARE-19)</title>
      <link>https://gasparyan.co/publication/dare-19-primary/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://gasparyan.co/publication/dare-19-primary/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dapagliflozin and Recurrent Heart Failure Hospitalizations in Heart Failure With Reduced Ejection Fraction: An Analysis of DAPA-HF</title>
      <link>https://gasparyan.co/publication/dapa-hf-recurrent/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://gasparyan.co/publication/dapa-hf-recurrent/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Effects of dapagliflozin on prevention of major clinical events and recovery in patients with respiratory failure because of COVID‐19: Design and rationale for the DARE‐19 study</title>
      <link>https://gasparyan.co/publication/dare-19-design/</link>
      <pubDate>Tue, 15 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://gasparyan.co/publication/dare-19-design/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Estimation</title>
      <link>https://gasparyan.co/post/bayes/2020-11-14-r-rmarkdown/</link>
      <pubDate>Sat, 14 Nov 2020 09:00:00 +0400</pubDate>
      <guid>https://gasparyan.co/post/bayes/2020-11-14-r-rmarkdown/</guid>
      <description>
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;ceo-salary-estimation-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CEO Salary Estimation Problem&lt;/h2&gt;
&lt;p&gt;Consider the following problem. An investigative reporter wants to figure out how much salary makes the CEO of an investment bank X. For this he conducts interviews with some of the employees of that bank and writes down their salaries, which forms the following sample&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X^n=(X_1,\cdots,X_n).\]&lt;/span&gt;
The reporter knows only that the salaries in that bank can range from 0 (unpaid interns) to &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; which is the salary of the CEO that the reporter wants to estimate. Since he has no information about the structure of salaries in the bank X, he assumes that the salaries have uniform distribution in the interval &lt;span class=&#34;math inline&#34;&gt;\([0,\theta],\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[X^n=(X_1,\cdots,X_n),\ \ X_i\sim {\mathbb U}(0,\theta),\,\theta&amp;gt;0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The uniform distribution is the maximum entropy probability distribution under no constraint other than that it is contained in the distribution’s support (according to the principle of maximum entropy, if nothing is known about a distribution except that it belongs to a certain class (usually defined in terms of specified properties or measures), then the distribution with the largest entropy should be chosen as the least-informative default.)&lt;/p&gt;
&lt;div id=&#34;frequentist-and-bayesian-estimation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Frequentist and Bayesian Estimation&lt;/h3&gt;
&lt;p&gt;Since no other information is known about the possible values of the CEO’s salary, the reporter needs to estimate the unknown &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; using the sample &lt;span class=&#34;math inline&#34;&gt;\(X^n.\)&lt;/span&gt; For the uniform distribution the maximum likelihood estimator (MLE) for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; will be
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n=X_{(n)}=\max(X_1,\cdots,X_n).\]&lt;/span&gt;
Therefore, the reporter needs to ask for salary information from as many employees of bank X as possible and take the maximum of these values, which will serve as an estimate for the CEO’s salary.&lt;/p&gt;
&lt;p&gt;Now suppose that the investigative reporter wants to get a Pulitzer prize for his reporting and remembers that he has a minor in Statistics. He reads economics literature and finds out that economists established that nationally the salaries of CEOs of banks follow the Pareto distribution &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim Pa(\theta_0, a)\)&lt;/span&gt;, because of the Pareto principle, which states that a large portion of wealth of CEOs is held by a small fraction of them. Using this prior distribution, a Bayesian estimator for the salary of CEO of bank X will be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\theta^B_n=\frac{a+n}{a+n-1}\max(\theta_0,X_1,\cdots,X_n).\]&lt;/span&gt;
Here &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; are unknown as well and can be estimated using the available data of CEO salaries of other banks. Therefore, the investigative reporter decides to use the work of his colleagues - other investigative reporters - who conducted studies in other banks and reported estimates for salaries of CEOs. Denote this new sample of salaries of other CEOs as
&lt;span class=&#34;math display&#34;&gt;\[\vartheta^m=(\vartheta_1,\cdots,\vartheta_m).\]&lt;/span&gt;
Since &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; follows the Pareto distribution then the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; can be estimated as follows:
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_0=\vartheta_{(1)}=\min(\vartheta_1,\cdots,\vartheta_m),\ \ \hat a=\frac{m}{\sum_{i=1}^m\ln\frac{\vartheta_i}{\vartheta_{(1)}}}.\]&lt;/span&gt;
Therefore, drawing on reports from other investigations and his own survey, the investigative reporter can obtain the following estimator of the salary of the CEO&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\theta^B_n=\frac{\hat a+n}{\hat a+n-1}\max(\vartheta_{(1)},X_1,\cdots,X_n).\]&lt;/span&gt;
Therefore, each time a new salary of some CEO is reported &lt;span class=&#34;math inline&#34;&gt;\((\vartheta_{m+1}),\)&lt;/span&gt; this new data can be used by the investigative reporter to update the estimate of the salary of the CEO of bank X.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conjugate-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conjugate priors&lt;/h2&gt;
&lt;p&gt;In the example above the prior distribution of the unknown parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; was chosen because of theoretical considerations, based on an economic law (the Pareto rule). In most cases, it is not possible to give preference to one prior over the others based on some principle, &lt;em&gt;conjugate&lt;/em&gt; priors are selected for computational simplicity.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The prior distribution &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim\pi(\theta),\,\theta\in\Theta\)&lt;/span&gt; (with the density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;) is called &lt;em&gt;conjugate&lt;/em&gt; prior for the likelihood function &lt;span class=&#34;math inline&#34;&gt;\(f(x|\theta)\)&lt;/span&gt; if its posterior density function &lt;span class=&#34;math inline&#34;&gt;\(f(\theta|x)\)&lt;/span&gt; is from the same family as the likelihood function.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;Bayes’&lt;/strong&gt; theorem specifies the following relationship between the likelihood function &lt;span class=&#34;math inline&#34;&gt;\(f(x|\theta)\)&lt;/span&gt; and the posterior function &lt;span class=&#34;math inline&#34;&gt;\(f(\theta|x),\)&lt;/span&gt; for the given prior density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(\theta|x)=\frac{f(x|\theta) p(\theta)}{\int_\Theta f(x|\vartheta) p(\vartheta) d\vartheta}.\]&lt;/span&gt;
Therefore, the density &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; is a conjugate prior for &lt;span class=&#34;math inline&#34;&gt;\(f(x|\theta),\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(f(x|\theta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f(\theta|x)\)&lt;/span&gt; are from the same family.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;appendix&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;div id=&#34;pareto-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pareto distributions&lt;/h2&gt;
&lt;p&gt;The random variable &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; has a Pareto distribution with parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta&amp;gt;0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a&amp;gt;0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\xi\sim Pa(\theta,a)\)&lt;/span&gt; if the distribution function is given by the formula
&lt;span class=&#34;math display&#34;&gt;\[F(x)=1-\left(\frac{\theta}{x}\right)^a,\,x&amp;gt;\theta,\]&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(F(x)=0,\,x\leq \theta.\)&lt;/span&gt; The density function will have the form
&lt;span class=&#34;math display&#34;&gt;\[f(x)=a\frac{\theta^a}{x^{a+1}},\,x&amp;gt;\theta.\]&lt;/span&gt;
In the case of &lt;span class=&#34;math inline&#34;&gt;\(a&amp;gt;1\)&lt;/span&gt; the expectation of a Pareto random variable is
&lt;span class=&#34;math display&#34;&gt;\[E\xi=\frac{a}{a-1}\theta.\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(a&amp;gt;2\)&lt;/span&gt; then the variance exists as well and equals to
&lt;span class=&#34;math display&#34;&gt;\[Var(\xi)=\frac{a}{(a-1)^2(a-2)}\theta^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;problems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Problems&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Show that if &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; follows the Pareto distribution &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim Pa(\theta_0, a),\)&lt;/span&gt; then the parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; can be estimated as follows:
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_0=\vartheta_{(1)}=\min(\vartheta_1,\cdots,\vartheta_m),\ \ \hat a=\frac{m}{\sum_{i=1}^m\ln\frac{\vartheta_i}{\vartheta_{(1)}}},\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\vartheta^m=(\vartheta_1,\cdots,\vartheta_m)\)&lt;/span&gt; is an i.i.d. sample from &lt;span class=&#34;math inline&#34;&gt;\(Pa(\theta_0,a).\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Suppose that an i.i.d. sample is observed
&lt;span class=&#34;math display&#34;&gt;\[X^n=(X_1,\cdots,X_n),\,X_i\sim F(x,\theta),\,\theta\in\Theta,\,x\in R.\]&lt;/span&gt;
Consider an estimator for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; based on the sample &lt;span class=&#34;math inline&#34;&gt;\(X^n\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n(X^n)=\hat\theta_n(X_1,\cdots,X_n).\]&lt;/span&gt;
Mean Squared Error (MSE, &lt;span class=&#34;math inline&#34;&gt;\(L(\theta,\hat\theta)\)&lt;/span&gt;) for this estimator is defined as
&lt;span class=&#34;math display&#34;&gt;\[E_\theta(\hat\theta_n-\theta)^2=\int_{R^n}(\hat\theta_n(x^n)-\theta)^2dF(x_1,\theta)\cdots dF(x_n,\theta).\]&lt;/span&gt;
If the prior distribution of the unknown parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is given &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim \pi(\theta),\,\theta\in\Theta,\)&lt;/span&gt; then the Bayesian risk of the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_n\)&lt;/span&gt; is defined as
&lt;span class=&#34;math display&#34;&gt;\[E_\pi L(\theta,\hat\theta)=\int_\Theta E_\theta(\hat\theta_n-\theta)^2d \pi(\theta).\]&lt;/span&gt;
Show that the Bayes estimator, defined as the one which minimizes the Bayesian risk, has the form
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n^B=\arg_{\hat\theta_n}\min_{\theta\in\Theta} E_\pi L(\theta,\hat\theta)=\int_{\Theta}\theta f(\theta|x)d\theta,\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(f(\theta|x)\)&lt;/span&gt; is the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta.\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Suppose that &lt;span class=&#34;math inline&#34;&gt;\(X_i\sim {\mathbb U}(0,\theta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta\sim Pa(\theta_0, a)\)&lt;/span&gt;. Show that the Bayes estimator has the form&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat\theta^B_n=\frac{a+n}{a+n-1}\max(\theta_0,X_1,\cdots,X_n).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Adjusted win ratio with stratification: Calculation methods and interpretation</title>
      <link>https://gasparyan.co/publication/adjusted-win-ratio/</link>
      <pubDate>Sun, 26 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://gasparyan.co/publication/adjusted-win-ratio/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Win Odds Confidence Intervals in R</title>
      <link>https://gasparyan.co/post/wrci/2020-01-10-r-rmarkdown/</link>
      <pubDate>Fri, 10 Jan 2020 10:00:00 +0400</pubDate>
      <guid>https://gasparyan.co/post/wrci/2020-01-10-r-rmarkdown/</guid>
      <description>
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;mann-whitney-estimate-for-the-win-probability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mann-Whitney estimate for the win probability&lt;/h2&gt;
&lt;p&gt;Consider two independent, continuous RVs (random variables) &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;. The following probability is called the &lt;em&gt;win probability&lt;/em&gt; of RV &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; against the RV &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\theta=P(\eta&amp;gt;\xi).
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Given an i.i.d. (independent, identically distributed) random sample from &lt;span class=&#34;math inline&#34;&gt;\(\xi,\)&lt;/span&gt; denoted by &lt;span class=&#34;math inline&#34;&gt;\(X=(X_1,\cdots,X_{n_1})\)&lt;/span&gt; and an i.i.d. sample from &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; denoted by &lt;span class=&#34;math inline&#34;&gt;\(Y=(Y_1,\cdots,Y_{n_2})\)&lt;/span&gt; we are interested in estimating the unknown parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The following estimator is called the &lt;em&gt;Mann-Whitney&lt;/em&gt; estimator (or, the &lt;em&gt;win proportion&lt;/em&gt;) for the win probability&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\hat\theta_N=\frac{1}{n_1n_2}\sum_{i=1}^{n_1}\sum_{j=1}^{n_2}I(X_i&amp;lt;Y_j).
\end{align*}\]&lt;/span&gt;
Here &lt;span class=&#34;math inline&#34;&gt;\(N=n_1+n_2\)&lt;/span&gt; is the total sample size, whereas &lt;span class=&#34;math inline&#34;&gt;\(I(\cdot)\)&lt;/span&gt; is the indicator function which takes the value 1 if the underlying inequality is true and 0 otherwise. When &lt;span class=&#34;math inline&#34;&gt;\(n_1\rightarrow+\infty,\,n_2\rightarrow+\infty\)&lt;/span&gt; then the win proportion is a consistent estimator (convergence in probability) for the win probability
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\hat\theta_N\longrightarrow\theta.
\end{align*}\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(\frac{n_1}{N}\rightarrow \alpha,\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(n_1\rightarrow+\infty,\,n_2\rightarrow+\infty,\)&lt;/span&gt; then the win proportion is also asymptotically normal
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\sqrt{N}(\hat\theta_N-\theta)\Longrightarrow{\mathcal N}\left(0,\frac{1}{1-\alpha}\sigma_{10}^2+\frac{1}{\alpha}\sigma_{01}^2\right).
\end{align*}\]&lt;/span&gt;
Here,
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\sigma_{10}^2=Cov(I(\xi&amp;lt;\eta),I(\xi&amp;#39;&amp;lt;\eta))=P(\xi&amp;lt;\eta,\xi&amp;#39;&amp;lt;\eta)-P(\xi&amp;lt;\eta)^2,\\
\sigma_{01}^2=Cov(I(\xi&amp;lt;\eta),I(\xi&amp;lt;\eta&amp;#39;))=P(\xi&amp;lt;\eta,\xi&amp;lt;\eta&amp;#39;)-P(\xi&amp;lt;\eta)^2,
\end{align*}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\xi&amp;#39;\)&lt;/span&gt; has the same distribution as &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\eta&amp;#39;\)&lt;/span&gt; has the same distribution as &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;. All &lt;span class=&#34;math inline&#34;&gt;\(\xi,\xi&amp;#39;,\eta,\eta&amp;#39;\)&lt;/span&gt; are independent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application-to-exponential-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Application to exponential distributions&lt;/h2&gt;
&lt;p&gt;Suppose now that &lt;span class=&#34;math inline&#34;&gt;\(\xi\sim{\mathbb E}(\lambda)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\sim{\mathbb E}(\mu).\)&lt;/span&gt; In this case,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\sigma_{10}^2=\frac{2\lambda}{(\lambda+\mu)(2\lambda+\mu)}-\frac{\lambda^2}{(\lambda+\mu)^2}=\frac{\lambda^2\mu}{(\lambda+\mu)^2(2\lambda+\mu)}\\
\sigma_{01}^2=\frac{\lambda}{\lambda+2\mu}-\frac{\lambda^2}{(\lambda+\mu)^2}=\frac{\lambda\mu^2}{(\lambda+\mu)^2(\lambda+2\mu)},
\end{align*}\]&lt;/span&gt;
therefore, the asymptotic variance of the win proportion will be
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\sigma^2=\frac{\lambda\mu}{(\lambda+\mu)^2}\left(\frac{1}{1-\alpha}\frac{\lambda}{2\lambda+\mu}+\frac{1}{\alpha}\frac{\mu}{\lambda+2\mu}\right).
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can check this by the following simulations&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n1 &amp;lt;- 700
n2 &amp;lt;- 100
N &amp;lt;- n1 + n2
m &amp;lt;- 1000
lambda &amp;lt;- 2
mu &amp;lt;- 10
alpha &amp;lt;- n1/(n1+n2)


k &amp;lt;- lambda/(lambda+mu)
WR &amp;lt;-NULL

for(i in 1:m){
  set.seed(i)
  X1 &amp;lt;- rexp(n1,lambda)
  X2 &amp;lt;- rexp(n2,mu)
  d &amp;lt;- expand.grid(x=X1,y=X2)
  d$w &amp;lt;- ifelse(d$y&amp;gt;d$x,1,ifelse(d$y==d$x,0.5,0))
  WR[i]&amp;lt;-sqrt(N)*(mean(d$w)-k)
}


x0&amp;lt;-3
int &amp;lt;- seq(-x0,x0,0.001)

Coeff0 &amp;lt;- mu*lambda/(lambda+mu)^2
Coeff &amp;lt;- Coeff0*(1/(1-alpha)*lambda/(2*lambda+mu)+1/alpha*mu/(lambda+2*mu))


hist(WR, nclass = 20, freq=FALSE, xlim=c(-x0,x0), 
     ylim=c(0, 1.1), col = &amp;quot;lightblue&amp;quot;, border = &amp;quot;blue&amp;quot;)
lines(int,dnorm(int, mean = 0, sd=sqrt(Coeff)), col=&amp;quot;2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/wrci/2020-01-10-r-rmarkdown_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;definition-of-the-win-odds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Definition of the win odds&lt;/h2&gt;
&lt;p&gt;Consider two independent, continuous RVs (random variables) &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;. The odds of the win probability is called the &lt;em&gt;win odds&lt;/em&gt; of RV &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; against the RV &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\omega=\frac{P(\eta&amp;gt;\xi)}{P(\eta&amp;lt;\xi)}=\frac{\theta}{1-\theta}.
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Mann-Whitney estimate of the win probability can be transformed by the function &lt;span class=&#34;math inline&#34;&gt;\(f(x)=\frac{x}{1-x},\ \ x\in(0,1)\)&lt;/span&gt; to get an estimate for the win odds. Using the same transformation and the asymptotic normality of the Mann-Whitney estimate it is possible to construct asymptotic confidence intervals for the win odds, for a given asymptotic confidence level.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\omega=1\)&lt;/span&gt; then the random variables &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; are stochastically equivalent, while &lt;span class=&#34;math inline&#34;&gt;\(\omega&amp;gt;1\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is stochastically greater than (wins against) &lt;span class=&#34;math inline&#34;&gt;\(\eta.\)&lt;/span&gt; The case &lt;span class=&#34;math inline&#34;&gt;\(\omega&amp;lt;1\)&lt;/span&gt; means that &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; loses against &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt;. The asymptotic confidence interval of the win odds can be used to test the hypothesis whether &lt;span class=&#34;math inline&#34;&gt;\(\omega=1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To use the asymptotic normality result described above we need to estimate the asymptotic variance. The package &lt;em&gt;sanon&lt;/em&gt; in &lt;em&gt;R&lt;/em&gt; allows to estimate the asymptotic standard error of the Mann-Whitney estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-package-sanon&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The package sanon&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#install.packages(&amp;quot;sanon&amp;quot;)
library(sanon)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset &lt;em&gt;resp&lt;/em&gt; contains data from a randomized clinical trial to compare a test treatment to placebo for a respiratory disorder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(resp, package = &amp;quot;sanon&amp;quot;)

head(resp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   center treatment sex age baseline visit1 visit2 visit3 visit4
## 1      1         A   F  32        1      2      2      4      2
## 2      2         A   F  37        1      3      4      4      4
## 3      1         A   F  47        2      2      3      4      4
## 4      2         A   F  39        2      3      4      4      4
## 5      1         A   M  11        4      4      4      4      2
## 6      2         A   F  60        4      4      3      3      4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The column &lt;em&gt;visit4&lt;/em&gt; is a numeric vector for patient global ratings of symptom control according to 5 categories (4 = excellent, 3 = good, 2 = fair, 1 = poor, 0 = terrible), measured at visit 4. To compare the effect of active treatment against the placebo we will use the win probability, which, as we defined previously, is an unknown theoretical quantity. The null hypothesis is that there is no treatment difference which can be written in terms of the win probability as &lt;span class=&#34;math inline&#34;&gt;\(\theta=0.5.\)&lt;/span&gt; The Mann-Whitney estimate of the win probability can be calculated as follows&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- sanon(visit4 ~ grp(treatment, ref=&amp;quot;P&amp;quot;), data = resp)

fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## sanon.formula(formula = visit4 ~ grp(treatment, ref = &amp;quot;P&amp;quot;), data = resp)
## 
## Sample size: 111
## 
## Response levels:
## [visit4; 5 levels] (lower) 0, 1, 2, 3, 4 (higher)
## 
## Design Matrix:
##        [,1]
## visit4    1
## 
## Mann-Whitney Estimate 
##  for comparison [ A / P ] :
## visit4 
## 0.6174&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## M-W Estimate and 95% Confidence Intervals 
## :
##        Estimate  Lower  Upper
## visit4   0.6174 0.5173 0.7176&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit$p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]
## [1,] 0.02150601&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The p-value based on the asymptotic confidence interval of level 0.05 is less than 0.05, hence the null hypothesis of no treatment difference is rejected. The Mann-Whitney estimate can be transformed to get an estimate for the win odds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(fit)$ci/(1-confint(fit)$ci)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Estimate    Lower    Upper
## visit4 1.614013 1.071762 2.540746&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the win odds the null hypothesis of no treatment difference is &lt;span class=&#34;math inline&#34;&gt;\(\omega=1.\)&lt;/span&gt; The win odds 1.61 characterizes the treatment effect difference.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exponential Distribution</title>
      <link>https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown/</link>
      <pubDate>Fri, 20 Dec 2019 10:00:00 +0400</pubDate>
      <guid>https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown/</guid>
      <description>
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;This is a short reminder of some simple properties of exponential distributions.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The continuous random variable (RV) &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; has an exponential distribution with the rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda&amp;gt;0\)&lt;/span&gt; if its CMD (cumulative distribution function) has the following form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[F_\xi(x)=P(\xi&amp;lt;x)=\left\{\begin{matrix}
&amp;amp;1-e^{-\lambda x}, &amp;amp;x\geq 0.\\
&amp;amp;0 &amp;amp;x&amp;lt;0.
\end{matrix}\right.
\]&lt;/span&gt;
This entails that an exponential RV is with 1 probability positive and has the PDF (probability density function)
&lt;span class=&#34;math display&#34;&gt;\[f_\xi(x)=F_\xi&amp;#39;(x)=\left\{\begin{matrix}
&amp;amp;\lambda e^{-\lambda x}, &amp;amp;x&amp;gt;0,\\
&amp;amp;0 &amp;amp;x&amp;lt;0.
\end{matrix}\right.
\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; has an exponential distribution with the rate &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; we will denote this by &lt;span class=&#34;math inline&#34;&gt;\({\mathbb E}(\lambda).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; the following functions can be used to, correspondingly, generate numbers from &lt;span class=&#34;math inline&#34;&gt;\({\mathbb E}(\lambda)\)&lt;/span&gt;, calculate values of CDF, calculate values of PDF&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rexp(n=2, rate=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.06697548 0.25507292&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pexp(q=1, rate=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6321206&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dexp(x=1, rate=1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3678794&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;em&gt;histogram&lt;/em&gt; is a non-parametric estimator for the PDF. Hence we can simulate data from an exponential distribution and show that the histogram based on the data fits the PDF. Consider the case of &lt;span class=&#34;math inline&#34;&gt;\({\mathbb E}(2)\)&lt;/span&gt; and simulate a sample of size &lt;span class=&#34;math inline&#34;&gt;\(n=10000.\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())

n &amp;lt;- 10000
lambda &amp;lt;- 2

X &amp;lt;- rexp(n = n, rate = lambda)
int &amp;lt;- seq(0, max(X), max(X)/100)

hist(X, freq = FALSE, nclass = 50, col = &amp;quot;lightblue&amp;quot;, 
     border = &amp;quot;blue&amp;quot;, ylim = c(0, 2), main = &amp;quot;&amp;quot;)
lines(int, dexp(int, rate = lambda), col = &amp;quot;red&amp;quot;, lty = 2, lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a very useful technique to check whether the RVs have the given PDF. We will use this technique below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1&lt;/h2&gt;
&lt;p&gt;Suppose &lt;span class=&#34;math inline&#34;&gt;\(\xi\sim {\mathbb E}(\lambda)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\sim {\mathbb E}(\mu)\)&lt;/span&gt; are independent. Calculate the PDF of the RV &lt;span class=&#34;math inline&#34;&gt;\(\zeta=\eta-\xi.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;First consider the case of &lt;span class=&#34;math inline&#34;&gt;\(z&amp;gt;0.\)&lt;/span&gt; Since &lt;span class=&#34;math inline&#34;&gt;\(\xi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; are independent, then the joint PDF of these RVs will be the product of individual PDFs, that is &lt;span class=&#34;math inline&#34;&gt;\(f_{(\eta,\xi)}(x,y)=f_{\xi}(x)f_{\eta}(y).\)&lt;/span&gt; Therefore,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[F_\zeta(z)=P(\eta-\xi&amp;lt;z)=\int_0^{+\infty}\int_0^{+\infty}I_{\{y-x\leq z\}}(x,y)\lambda\mu e^{-\lambda x-\mu y}d x dy.\]&lt;/span&gt;
Making the following variable change &lt;span class=&#34;math inline&#34;&gt;\(u=y-x\)&lt;/span&gt; will give
&lt;span class=&#34;math display&#34;&gt;\[F_\zeta(z)=P(\eta-\xi&amp;lt;z)=\lambda\mu \int_0^{+\infty}e^{-(\lambda + \mu) x}\left(\int_{-x}^{z}e^{-\mu u}d u\right) dx=1-\frac{\lambda}{\lambda+\mu}e^{-\mu z},\ \ z&amp;gt;0.\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(z&amp;lt;0\)&lt;/span&gt; then
&lt;span class=&#34;math display&#34;&gt;\[P(\eta-\xi&amp;lt;z)=P(\xi-\eta&amp;gt;-z)=1-P(\xi-\eta&amp;lt;-z)=1-\left(1-\frac{\mu}{\mu+\lambda}e^{\lambda z}\right)=\frac{\mu}{\lambda+\mu}e^{\lambda z},\ \ z&amp;lt;0.\]&lt;/span&gt;
Therefore,
&lt;span class=&#34;math display&#34;&gt;\[F_\zeta(z)=\left\{\begin{matrix}
&amp;amp;1-\frac{\lambda}{\lambda+\mu} e^{-\mu z}, &amp;amp;z\geq 0,\\
&amp;amp;\frac{\mu}{\lambda+\mu} e^{\lambda z} &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
The PDF will be
&lt;span class=&#34;math display&#34;&gt;\[f_\zeta(z)=\frac{\lambda\mu}{\lambda+\mu}\left\{\begin{matrix}
&amp;amp;e^{-\mu z}, &amp;amp;z\geq 0,\\
&amp;amp;e^{\lambda z} &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
This formula can be checked using simulations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())

n &amp;lt;- 20000
m &amp;lt;- 10000
lambda &amp;lt;- 2
mu &amp;lt;- 5


  X &amp;lt;- rexp(n,lambda)
  Y &amp;lt;- rexp(m, mu)
  Z &amp;lt;- Y-X
  
int &amp;lt;- seq(min(Z),max(Z),(max(Z)-min(Z))/100)
dens &amp;lt;- function(z) (lambda*mu)/(lambda+mu)*ifelse(z&amp;gt;=0,exp(-mu*z),exp(lambda*z))
hist(Z, nclass = 100, freq=FALSE,ylim=c(0,1.5),
     xlim = c(min(Z),max(Z)), main=&amp;quot;&amp;quot;, col = &amp;quot;lightblue&amp;quot;, border = &amp;quot;blue&amp;quot;)
lines(int,dens(int), col=&amp;quot;2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the PDF we can calculate also
&lt;span class=&#34;math display&#34;&gt;\[E(\zeta)=\frac{\lambda-\mu}{\lambda\mu},\ \ P(\eta&amp;gt;\xi)=\frac{\lambda}{\lambda+\mu}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;p&gt;Suppose that &lt;span class=&#34;math inline&#34;&gt;\(\eta\sim{\mathbb E}(\mu)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta&amp;#39;\sim{\mathbb E}(\mu&amp;#39;)\)&lt;/span&gt; are independent. Find the distribution of the RVs &lt;span class=&#34;math inline&#34;&gt;\(\min(\eta,\eta&amp;#39;)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\max(\eta,\eta&amp;#39;).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For all &lt;span class=&#34;math inline&#34;&gt;\(z&amp;gt;0\)&lt;/span&gt; we have
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
P(\min(\eta,\eta&amp;#39;)&amp;lt;z)=1-P(\min(\eta,\eta&amp;#39;)\geq z)=1-P(\eta\geq z,\eta&amp;#39;\geq z)=1-e^{-(\mu+\mu&amp;#39;) z},\,z&amp;gt;0,
\end{align*}\]&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(P(\min(\eta,\eta&amp;#39;)&amp;lt;z)=0,\ \ z\leq 0.\)&lt;/span&gt; Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\min(\eta,\eta&amp;#39;)\sim{\mathbb E}(\mu+\mu&amp;#39;)\)&lt;/span&gt;, that is, the minimum of two exponentially distributed RVs is an exponentially distributed RV as well, with the rate being equal to the sum of the rates of the given two RVs.&lt;/p&gt;
&lt;p&gt;On the other hand,
&lt;span class=&#34;math display&#34;&gt;\[P(\max(\eta, \eta&amp;#39;) &amp;lt; z)= P(\eta &amp;lt; z, \eta&amp;#39; &amp;lt;z)=(1-e^{-\mu z})(1-e^{-\mu&amp;#39; z}),\,z&amp;gt;0.\]&lt;/span&gt;
For &lt;span class=&#34;math inline&#34;&gt;\(\mu=\mu&amp;#39;\)&lt;/span&gt; we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\max(\eta, \eta&amp;#39;) &amp;lt; z) = (1-e^{-\mu z})^2,\,z&amp;gt;0,\]&lt;/span&gt;
with PDF being equal to &lt;span class=&#34;math inline&#34;&gt;\(f(z)=2\mu(1-e^{-\mu z})e^{-\mu z},\,z&amp;gt;0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f(z)=0,\,z&amp;lt;0.\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 3&lt;/h2&gt;
&lt;p&gt;For given three independent RVs such that &lt;span class=&#34;math inline&#34;&gt;\(\xi\sim{\mathbb E}(\lambda)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta,\eta&amp;#39;\sim{\mathbb E}(\mu),\)&lt;/span&gt; calculate the following probability
&lt;span class=&#34;math inline&#34;&gt;\(\theta=P(\xi&amp;lt;\eta,\xi&amp;lt;\eta&amp;#39;).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can reformulate this problem as follows
&lt;span class=&#34;math display&#34;&gt;\[\theta=P(\xi&amp;lt;\eta,\xi&amp;lt;\eta&amp;#39;)=P(\xi&amp;lt;\min(\eta,\eta&amp;#39;))=P(\xi-\min(\eta,\eta&amp;#39;)&amp;lt;0).\]&lt;/span&gt;
Hence, denoting by &lt;span class=&#34;math inline&#34;&gt;\(\zeta=\xi-\min(\eta,\eta&amp;#39;),\)&lt;/span&gt; in the first step we can calculate the distribution function of the RV &lt;span class=&#34;math inline&#34;&gt;\(\zeta.\)&lt;/span&gt; We have &lt;span class=&#34;math inline&#34;&gt;\(\xi\sim{\mathbb E}(\xi)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\min(\eta,\eta&amp;#39;)\sim{\mathbb E}(2\mu)\)&lt;/span&gt; (see Example 2), hence, using the Example 1 we obtain&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[F_\zeta(z)=\left\{\begin{matrix}
&amp;amp;1-\frac{2\mu}{\lambda+2\mu} e^{-\lambda z}, &amp;amp;z\geq 0,\\
&amp;amp;\frac{\lambda}{\lambda+2\mu} e^{2\mu z} &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
For the PDF we have
&lt;span class=&#34;math display&#34;&gt;\[f_\zeta(z)=\left\{\begin{matrix}
&amp;amp;\frac{2\lambda\mu}{\lambda+2\mu} e^{-\lambda z}, &amp;amp;z&amp;gt; 0,\\
&amp;amp;\frac{2\lambda\mu}{\lambda+2\mu} e^{2\mu z} &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
We can, again, check this using simulations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())

n &amp;lt;- 200000
m &amp;lt;- 100000
lambda &amp;lt;- 2.5
mu &amp;lt;- 1.3


X1 &amp;lt;- rexp(n,mu)
X2 &amp;lt;- rexp(n,mu)
Y &amp;lt;- rexp(m, lambda)
Z &amp;lt;- Y - ifelse(X1&amp;gt;=X2,X2,X1)
Coeff &amp;lt;- 2*lambda*mu/(lambda+2*mu)

int &amp;lt;- seq(min(Z), max(Z), (max(Z)-min(Z))/100)
dens &amp;lt;- function(z) Coeff*ifelse(z&amp;gt;=0, exp(-lambda*z), exp(2*mu*z))


hist(Z, nclass = 100, freq=FALSE, ylim=c(0,1.5), 
     xlim=c(min(Z), max(Z)), main = &amp;quot;&amp;quot;, border = &amp;quot;blue&amp;quot;, col = &amp;quot;lightblue&amp;quot;)
lines(int,dens(int), col=&amp;quot;2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Therefore,
&lt;span class=&#34;math display&#34;&gt;\[\theta=F_\zeta(0)=\frac{\lambda}{\lambda+2\mu}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 4&lt;/h2&gt;
&lt;p&gt;For given three independent RVs such that &lt;span class=&#34;math inline&#34;&gt;\(\xi,\xi&amp;#39;\sim{\mathbb E}(\lambda)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\sim{\mathbb E}(\mu),\)&lt;/span&gt; calculate the following probability
&lt;span class=&#34;math inline&#34;&gt;\(\vartheta=P(\xi&amp;lt;\eta,\xi&amp;#39;&amp;lt;\eta).\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can reformulate this problem as follows
&lt;span class=&#34;math display&#34;&gt;\[\vartheta=P(\xi&amp;lt;\eta,\xi&amp;#39;&amp;lt;\eta)=P(\max(\xi,\xi&amp;#39;)&amp;lt;\eta)=P(\eta-\max(\xi,\xi&amp;#39;)&amp;gt;0).\]&lt;/span&gt;
Hence, denoting by &lt;span class=&#34;math inline&#34;&gt;\(\kappa=\eta-\max(\xi,\xi&amp;#39;),\)&lt;/span&gt; in the first step we can calculate the distribution function of the RV &lt;span class=&#34;math inline&#34;&gt;\(\kappa.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
P(\kappa&amp;lt;z)=\int_0^{+\infty}\int_0^{+\infty}I(y-x\leq z)2\lambda\mu(1-e^{-\lambda x})e^{-\lambda x-\mu y}d x d y.
\end{align*}\]&lt;/span&gt;
If &lt;span class=&#34;math inline&#34;&gt;\(z&amp;gt;0\)&lt;/span&gt; then, denoting &lt;span class=&#34;math inline&#34;&gt;\(y-x=u\)&lt;/span&gt; we get &lt;span class=&#34;math inline&#34;&gt;\(y=u+x,\,u\in[-x,\infty)\ \ d y= d u.\)&lt;/span&gt; Therefore,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
P(\kappa&amp;lt;z)&amp;amp;=2\lambda\mu\int_0^{+\infty}(1-e^{-\lambda x})e^{-\lambda x-\mu x}\int_{-x}^{z}e^{-\mu u}d u d x=\\
&amp;amp;=-2\lambda\int_0^{+\infty}(e^{-(\lambda+\mu) x -\mu z}-e^{-(2\lambda+\mu) x -\mu z}-e^{-\lambda x }+e^{-2\lambda x }) d x=\\
&amp;amp;=1-\frac{2\lambda^2}{(\lambda+\mu)(2\lambda+\mu)}e^{-\mu z },\ \ z&amp;gt;0.
\end{align*}\]&lt;/span&gt;
For &lt;span class=&#34;math inline&#34;&gt;\(z&amp;gt;0\)&lt;/span&gt; calculate also (using the notation &lt;span class=&#34;math inline&#34;&gt;\(x-y=u\)&lt;/span&gt;, which entails &lt;span class=&#34;math inline&#34;&gt;\(x=y+u,\,d x=d u,\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(u\in[-y,+\infty).\)&lt;/span&gt;)
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
P(\max(\xi,\xi&amp;#39;)-\eta&amp;lt;z)&amp;amp;=\int_0^{+\infty}\int_0^{+\infty}I(x-y\leq z)2\lambda\mu(1-e^{-\lambda x})e^{-\lambda x-\mu y}d x d y=\\
&amp;amp;=\int_0^{+\infty}\int_{-y}^{z}(e^{-\lambda(y+u)  - \mu y} - e^{-2\lambda(y+u)  -\mu y}d u dy)=\\
&amp;amp;=1+\mu\left[\frac{e^{-2\lambda z}}{2\lambda+\mu}-\frac{2e^{-\lambda z}}{\lambda+\mu}\right],\ \ z\geq 0.
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, for &lt;span class=&#34;math inline&#34;&gt;\(z&amp;gt;0\)&lt;/span&gt; we have
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
P(\kappa&amp;lt;-z)&amp;amp;=1-P(\max(\xi,\xi&amp;#39;)-\eta&amp;lt;z)=\\
&amp;amp;=-\mu\left[\frac{e^{-2\lambda z}}{2\lambda+\mu}-\frac{2e^{-\lambda z}}{\lambda+\mu}\right],\ \ z\geq 0
\end{align*}\]&lt;/span&gt;
Finally we obtain
&lt;span class=&#34;math display&#34;&gt;\[F_\kappa(z)=\left\{\begin{matrix}
&amp;amp;1-\frac{2\lambda^2}{(\lambda+\mu)(2\lambda+\mu)} e^{-\mu z}, &amp;amp;z\geq 0,\\
&amp;amp;-\mu\left[\frac{e^{2\lambda z}}{2\lambda+\mu}-\frac{2e^{\lambda z}}{\lambda+\mu}\right] &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
For the PDF we have
&lt;span class=&#34;math display&#34;&gt;\[f_\kappa(z)=\left\{\begin{matrix}
&amp;amp;\frac{2\lambda^2\mu}{(\lambda+\mu)(2\lambda+\mu)} e^{-\mu z}, &amp;amp;z&amp;gt; 0,\\
&amp;amp;-2\lambda\mu\left[\frac{e^{2\lambda z}}{2\lambda+\mu}-\frac{e^{\lambda z}}{\lambda+\mu}\right] &amp;amp;z&amp;lt;0.
\end{matrix}\right.\]&lt;/span&gt;
To check this formula we can make the following simulations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())

n &amp;lt;- 200000
m &amp;lt;- 100000
lambda &amp;lt;- 2.5
mu &amp;lt;- 5.5


X1 &amp;lt;- rexp(n,lambda)
X2 &amp;lt;- rexp(n,lambda)
Y &amp;lt;- rexp(m, mu)
Z &amp;lt;- Y - ifelse(X1 &amp;gt;= X2, X1, X2)
Coeff &amp;lt;- 2*mu*lambda^2/((lambda+mu)*(2*lambda+mu))
Coeff1 &amp;lt;- -2*lambda*mu/(2*lambda+mu)
Coeff2 &amp;lt;- -2*lambda*mu/(lambda+mu)

int &amp;lt;- seq(min(Z), max(Z), (max(Z)-min(Z))/100)

dens &amp;lt;- function(z) ifelse(z&amp;gt;=0, Coeff*exp(-mu*z),
                           Coeff1*exp(2*lambda*z)-Coeff2*exp(lambda*z))



hist(Z, nclass = 100, freq=FALSE,ylim=c(0, 1.5), xlim=c(min(Z), max(Z)),
     main = &amp;quot;&amp;quot;, col = &amp;quot;lightblue&amp;quot;, border = &amp;quot;blue&amp;quot;)
lines(int,dens(int), col=&amp;quot;2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/exp/2019-12-20-r-rmarkdown_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Therefore,
&lt;span class=&#34;math display&#34;&gt;\[\vartheta=P(\xi&amp;lt;\eta, \xi&amp;#39;&amp;lt;\eta)=P(\kappa&amp;gt;0)=\frac{2\lambda^2}{(\lambda+\mu)(2\lambda+\mu)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Armdown</title>
      <link>https://gasparyan.co/project/internal-project/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://gasparyan.co/project/internal-project/</guid>
      <description>&lt;p&gt;This project is dedicated to promoting Statistics and Data Science texts in Armenian language. Follow the &lt;em&gt;Rmenia&lt;/em&gt; link above to access &lt;em&gt;R&lt;/em&gt; programming lecture notes in Armenian. Follow the &lt;em&gt;Armdown&lt;/em&gt; link above to access the configuration files which will help to produce Armenian texts using &lt;em&gt;Markdown&lt;/em&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://gasparyan.co/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://gasparyan.co/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dose-Response Curves in R</title>
      <link>https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown/</link>
      <pubDate>Thu, 13 Sep 2018 09:00:00 +0400</pubDate>
      <guid>https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown/</guid>
      <description>
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://gasparyan.co/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In clinical research the dose–response relationship is often non-linear, therefore advanced fitting models are needed to capture their behavior. The talk will concentrate on fitting non-linear parametric models to the dose–response data and will explain some specificities of this problem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The talk is based on the book by &lt;strong&gt;Christian Ritz, Jens Carl Streibig “Nonlinear regression with R”.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;purpose-of-dose-response-information&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Purpose of Dose-Response Information&lt;/h1&gt;
&lt;div id=&#34;ich-e4-harmonized-tripartite-guideline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ICH E4 Harmonized Tripartite Guideline&lt;/h2&gt;
&lt;p&gt;Knowledge of the relationships among dose, drug-concentration in blood, and clinical
response (effectiveness and undesirable effects) is important for the safe and effective
use of drugs in individual patients. It helps identify&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;an appropriate starting dose,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;the best way to adjust dosage to the needs of a particular patient,&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;a dose beyond which increases would be unlikely to provide added benefit or would
produce unacceptable side effects.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;There are other fields of studies were concentration analysis can be used.&lt;/p&gt;
&lt;p&gt;Consider the following relationship (&lt;em&gt;Chwirut2&lt;/em&gt; is included in the package &lt;strong&gt;NISTnls&lt;/strong&gt;),
where the response variable is ultrasonic response, and the predictor variable is metal distance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#library(NISTnls)
dim(Chwirut2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 54  2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(Chwirut2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         y     x
## 1 92.9000 0.500
## 2 57.1000 1.000
## 3 31.0500 1.750
## 4 11.5875 3.750
## 5  8.0250 5.750
## 6 63.6000 0.875&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;p&gt;A distance-amplitude relationship of attenuation that an ultrasound beam experiences traveling through a medium&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#library(ggplot2)
g &amp;lt;- ggplot(data=Chwirut2,aes(x=x,y=y)) + 
  geom_point() + xlab(&amp;quot;Metal distance&amp;quot;) + ylab(&amp;quot;Ultrasonic respons&amp;quot;)
g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-built-in-function-nls&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The built-in function &lt;em&gt;nls()&lt;/em&gt;&lt;/h1&gt;
&lt;div id=&#34;fitting-a-parametric-family&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting a parametric family&lt;/h2&gt;
&lt;p&gt;Suppose we observe the following pair of data points &lt;span class=&#34;math inline&#34;&gt;\((x_i,y_i),\,i=1,\cdots,n.\)&lt;/span&gt; Consider a parametric family (usually non-linear) that we want to fit to the data
&lt;span class=&#34;math display&#34;&gt;\[f(x,(\theta_1,\cdots,\theta_k)).\]&lt;/span&gt;
The hypothesis is that there is the following relationship between observed points&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i=f(x_i,(\theta_1,\cdots,\theta_k))+\varepsilon_i,\ \ i=1,\cdots,n.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The fitting method that we are going to use is &lt;em&gt;the Non-linear Least Squares (NLS)&lt;/em&gt; method. NLS estimates can be found using this minimization problem.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[(\hat\theta_1,\cdots,\hat\theta_k)=\arg\min_{\theta_1,\cdots,\theta_k}\sum_{i=1}^n(y_i-f(x_i,(\theta_1,\cdots,\theta_k)))^2.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gauss---newton-algorithm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gauss - Newton algorithm&lt;/h2&gt;
&lt;p&gt;The Gauss - Newton algorithm is used to solve the optimization problem described above.&lt;/p&gt;
&lt;p&gt;If we denote &lt;span class=&#34;math inline&#34;&gt;\(g_i(\theta)=y_i-f(x_i,\theta), \ \ \theta=(\theta_1,\cdots,\theta_k),\)&lt;/span&gt;
then the minimization problem is the following
&lt;span class=&#34;math display&#34;&gt;\[\min_{\theta\in R^k}\sum_{i=1}^ng_i^2(\theta)=\min_{\theta\in R^k}||g(\theta)||^2.\]&lt;/span&gt;
Choose an initial value &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; and linearize the function &lt;span class=&#34;math inline&#34;&gt;\(g(\theta)\)&lt;/span&gt; around &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; as follows&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat g(\theta,\theta_0)=g(\theta_0)+Dg(\theta_0)(\theta-\theta_0).\]&lt;/span&gt;
Solving &lt;span class=&#34;math inline&#34;&gt;\(\min_{\theta\in R^k}||g(\theta_0)+Dg(\theta_0)(\theta-\theta_0)||^2,\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(m\times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(Dg(\theta)\)&lt;/span&gt; is the Jacobian, we get &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt; and continuing this process interatively, we get the sequence &lt;span class=&#34;math inline&#34;&gt;\((\theta_m),\,m=0,1,\cdots.\)&lt;/span&gt; Moreover, if &lt;span class=&#34;math inline&#34;&gt;\(Dg(\theta_k)\)&lt;/span&gt; has linearly independent columns,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_{k+1}=\theta_k-\left([Dg(\theta_k)]^TDg(\theta_k)\right)^{-1}[Dg(\theta_k)]^Tg(\theta_k).\]&lt;/span&gt;
The question is - how to choose the initial value &lt;span class=&#34;math inline&#34;&gt;\(\theta_0\)&lt;/span&gt; of iteration?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-parametric-family&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The parametric family&lt;/h2&gt;
&lt;p&gt;The following parametric family was suggested to fit the distance-amplitude relationship&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x,(\beta_1,\beta_2,\beta_3))=\frac{\exp(-\beta_1x)}{\beta_2+\beta_3x}\]&lt;/span&gt;
Assumptions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;correct mean function &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;variance homogeneity (homoscedasticity)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;normally distributed measurements errors&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;mutually independent measurement errors &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_i\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;If we take &lt;span class=&#34;math inline&#34;&gt;\(x=0\)&lt;/span&gt; then we get &lt;span class=&#34;math inline&#34;&gt;\(f(0,(\beta_1,\beta_2,\beta_3))=\frac{1}{\beta_2}\)&lt;/span&gt; so the initial estimate of &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; can be the reciprocal of the response value closest to the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis, that is, roughly, &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{100}=0.01.\)&lt;/span&gt; For the&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-non-linear-parametric-family&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting non-linear parametric family&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expFct &amp;lt;- function(x,beta1,beta2=0.01,beta3)  exp(-beta1*x)/(beta2+beta3*x)
l &amp;lt;- list(beta1=0.1, beta2=0.01, beta3=0.001)
fit &amp;lt;- nls(data=Chwirut2, y ~ expFct(x, beta1, beta2, beta3), start=l)
fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Nonlinear regression model
##   model: y ~ expFct(x, beta1, beta2, beta3)
##    data: Chwirut2
##    beta1    beta2    beta3 
## 0.166576 0.005165 0.012150 
##  residual sum-of-squares: 513
## 
## Number of iterations to convergence: 9 
## Achieved convergence tolerance: 7.467e-07&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 5
##   term  estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 beta1  0.167    0.0383        4.35 6.56e- 5
## 2 beta2  0.00517  0.000666      7.75 3.54e-10
## 3 beta3  0.0122   0.00153       7.94 1.81e-10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-the-fitted-non-linear-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting the fitted non-linear function&lt;/h2&gt;
&lt;p&gt;Hence, we can plot the fitted curve&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#library(broom)
values &amp;lt;- tidy(fit)$estimate
g + stat_function(fun=expFct, args=values, colour=&amp;quot;blue&amp;quot;, lwd=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-package-nls2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The package &lt;em&gt;nls2&lt;/em&gt;&lt;/h1&gt;
&lt;div id=&#34;searching-a-grid&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Searching a grid&lt;/h2&gt;
&lt;p&gt;The procedure of finding initial points for the minimization iteration can be automated in the following way. In case the range of the parameter estimates can be envisaged grid search can be carried out:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;the residual sums-of-squares &lt;span class=&#34;math inline&#34;&gt;\(RSS(\beta),\,\beta=(\beta_1,\cdots,\beta_k)\)&lt;/span&gt; is calculated for all the values in the intervals&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;starting values are chosen in the way to provide the smallest value of &lt;span class=&#34;math inline&#34;&gt;\(RSS(\beta)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;In our example, having &lt;span class=&#34;math inline&#34;&gt;\(\beta_2=0.01,\)&lt;/span&gt; for other two parameters, knowing only that they are positive numbers, we can search in the interval &lt;span class=&#34;math inline&#34;&gt;\([0.1,1]\)&lt;/span&gt; taking the step size equal to &lt;span class=&#34;math inline&#34;&gt;\(0.1.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Create a &lt;em&gt;data.frame&lt;/em&gt; containing all possible combinations of suggested initial values for the parameters&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta1 &amp;lt;- seq(0.1, 1, by = 0.1)
beta2 &amp;lt;- 0.01
beta3 &amp;lt;- seq(0.1, 1, by = 0.1)
grid.Chwirut2 &amp;lt;- expand.grid(list(beta1 = beta1, beta2 = beta2, beta3 = beta3))
head(grid.Chwirut2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   beta1 beta2 beta3
## 1   0.1  0.01   0.1
## 2   0.2  0.01   0.1
## 3   0.3  0.01   0.1
## 4   0.4  0.01   0.1
## 5   0.5  0.01   0.1
## 6   0.6  0.01   0.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-nls2-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;em&gt;nls2()&lt;/em&gt; function&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;nls2()&lt;/em&gt; function differs from the original function by a way that it provides the possibility to specify a data frame of values of starting points. Its argument &lt;em&gt;algorithm=“brute-force”&lt;/em&gt; evaluates &lt;em&gt;RSS&lt;/em&gt; for the parameter values provided through the &lt;em&gt;start&lt;/em&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(nls2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit2 &amp;lt;- nls2(data = Chwirut2, y ~ expFct(x, beta1, beta2, beta3), 
             start = grid.Chwirut2, algorithm = &amp;quot;brute-force&amp;quot;)
fit2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Nonlinear regression model
##   model: y ~ expFct(x, beta1, beta2, beta3)
##    data: Chwirut2
## beta1 beta2 beta3 
##  0.10  0.01  0.10 
##  residual sum-of-squares: 60696
## 
## Number of iterations to convergence: 100 
## Achieved convergence tolerance: NA&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-using-the-function-nls2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting using the function &lt;em&gt;nls2()&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Fitting procedure can be continued this way: replace the value of the &lt;em&gt;start&lt;/em&gt; argument by &lt;em&gt;fit2&lt;/em&gt; and leave out the argument &lt;em&gt;algorithm&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(nls2(data = Chwirut2, y ~ expFct(x, beta1, beta2, beta3), start=fit2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 5
##   term  estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 beta1  0.167    0.0383        4.35 6.56e- 5
## 2 beta2  0.00517  0.000666      7.75 3.54e-10
## 3 beta3  0.0121   0.00153       7.94 1.81e-10&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;self-starter-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Self-Starter functions&lt;/h1&gt;
&lt;div id=&#34;automating-further-the-search-for-initial-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Automating further the search for initial points&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Self-starter functions are special type of functions. They are implementations of specific given mean functions and designed in a way to calculate starting values for a given dataset. They can be thought as the definition of the parametric family combined with the logic of how to choose initial values based on the given dataset.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;We have already encountered such example: for the parametric family
&lt;span class=&#34;math display&#34;&gt;\[f(x,(\beta_1,\beta_2,\beta_3))=\frac{\exp(-\beta_1x)}{\beta_2+\beta_3x}\]&lt;/span&gt;
we said that the initial estimate of &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; can be the reciprocal of the response value closest to the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis. So, if we find also logic of choosing initial values for &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_3,\)&lt;/span&gt; then we can add these logics into the definition of this parametric family and get a Self-Starter function.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;We will construct a Self-Starter function later, but before that we will explore the built-in Self-Starter functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;built-in-self-starters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Built-in Self-Starters&lt;/h2&gt;
&lt;p&gt;Consider the following dataset&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat &amp;lt;- data.frame(
  conc = c(2.856829,  5.005303,  7.519473,  22.101664,  
         27.769976,  39.198025,  45.483269, 203.784238),
rate = c(14.58342, 24.74123, 31.34551, 72.96985, 77.50099, 
         96.08794, 96.96624, 108.88374)
)
dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         conc      rate
## 1   2.856829  14.58342
## 2   5.005303  24.74123
## 3   7.519473  31.34551
## 4  22.101664  72.96985
## 5  27.769976  77.50099
## 6  39.198025  96.08794
## 7  45.483269  96.96624
## 8 203.784238 108.88374&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- ggplot(data = dat, aes(x = conc, y = rate)) + geom_point()
g&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-michaelis-menten-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Michaelis-Menten model&lt;/h2&gt;
&lt;p&gt;We are going to fit this data using the parametric family of Michaelis-Menten functions&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x,(V_m,k))=\frac{V_m x}{K+x}.\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Here the constants &lt;span class=&#34;math inline&#34;&gt;\(V_m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; are positive. &lt;span class=&#34;math inline&#34;&gt;\(V_{m}\)&lt;/span&gt; represents the maximum rate achieved by the system, at saturating substrate concentration. The constant &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is the substrate concentration at which the reaction rate is half of &lt;span class=&#34;math inline&#34;&gt;\(V_m.\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Again we can interactively find the initial estimates for the parameters to fit the NLS curve. But there is a built-in Self-Starter function called &lt;em&gt;SSmicmen()&lt;/em&gt; which has a logic inscribed in it which lets it work with &lt;em&gt;nls()&lt;/em&gt; without specifying starting points.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3 &amp;lt;- nls(rate ~ SSmicmen(conc, Vm, K), data = dat)
tidy(fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   term  estimate std.error statistic    p.value
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1 Vm       126.       7.17     17.6  0.00000218
## 2 K         17.1      2.95      5.78 0.00117&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g + stat_function(fun = SSmicmen, args = list(Vm = tidy(fit3)$estimate[1],
                                       K=tidy(fit3)$estimate[2]), 
                  colour = &amp;quot;darkgreen&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;defining-self-starter-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Defining Self-Starter functions&lt;/h1&gt;
&lt;div id=&#34;a-simple-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A simple example&lt;/h2&gt;
&lt;p&gt;Consider the following parametric family&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x,(b,y_0))=y_0 e^{\frac{x}{b}}.\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;This model can be used to describe radioactive decay. The parameter &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; is
the initial amount of the radioactive substance (at time &lt;span class=&#34;math inline&#34;&gt;\(x = 0\)&lt;/span&gt;). The rate of
decay is governed by the second parameter &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; (the inverse decay constant).&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The definition of a self-starter contains the formula of the parametric family, a logic of constructing initial values for parameters, a function taking the dataset as argument and other arguments ensuring that the logic of construction of initial values, the definition of parametric family and &lt;em&gt;nls()&lt;/em&gt; are tied together.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logic-of-construction-of-initial-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The logic of construction of initial values&lt;/h2&gt;
&lt;p&gt;Apply the log transform on the given mean function&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log f(x,(b,y_0))=\log y_0+\frac{x}{b}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence, if we apply the log transform on the response data, we can apply linear regression techniques to estimate the slope and the intercept, then, from the equalities&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\log\tilde y_0={\beta_0}, \ \ \tilde \beta_1=\frac{1}{b}\]&lt;/span&gt;
we get&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\tilde y_0=e^{\tilde \beta_0}, \ \ \tilde b=\frac{1}{\tilde\beta_1}.\]&lt;/span&gt;
This is the logic of construction of initial parameter values that we need to include in the definition of a Self-Starter.&lt;/p&gt;
&lt;p&gt;(We are not interested in the error structure that will change after the transformation, because we need only initial estimates.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-match.call-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The &lt;em&gt;match.call()&lt;/em&gt; function&lt;/h2&gt;
&lt;p&gt;The function &lt;em&gt;match.call()&lt;/em&gt; can be used in the definition of the function to store the call in the resulting object. In addition, if that function is called without specifying the arguments, the function &lt;em&gt;match.call()&lt;/em&gt; matches the arguments to their names by their positions. Observe below&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Func &amp;lt;- function(input, parameter){
  value &amp;lt;- input^parameter
  attr(value,&amp;quot;call&amp;quot;) &amp;lt;- match.call()
  value
}
power &amp;lt;- Func(2,16) #observe that we did not specify the names of parameters
power&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 65536
## attr(,&amp;quot;call&amp;quot;)
## Func(input = 2, parameter = 16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hence, their argument values can be extracted from the call by their names as from a list&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attr(power,&amp;quot;call&amp;quot;)$parameter&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 16&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;three-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Three steps&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Define the mean function&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expModel &amp;lt;- function(predictor,b, y0) {
  y0 * exp(predictor/b)
}&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Define the function with the initialization logic&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expModelInit &amp;lt;- function(mCall, LHS, data) {
 xy &amp;lt;- sortedXyData(mCall[[&amp;quot;predictor&amp;quot;]], LHS, data)
 lmFit &amp;lt;- lm(log(xy[, &amp;quot;y&amp;quot;]) ~ xy[, &amp;quot;x&amp;quot;])
 coefs &amp;lt;- coef(lmFit)
 y0 &amp;lt;- exp(coefs[1])
 b &amp;lt;- 1/coefs[2]
 value &amp;lt;- c(b, y0)
 names(value) &amp;lt;- c(&amp;quot;b&amp;quot;,&amp;quot;y0&amp;quot;)
 value
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;define-the-self-starter-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Define the Self-Starter function&lt;/h1&gt;
&lt;div id=&#34;specificities-of-working-in-a-markdown-environment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Specificities of working in a Markdown Environment&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SSexp &amp;lt;- selfStart(expModel, expModelInit, c(&amp;quot;b&amp;quot;, &amp;quot;y0&amp;quot;))
class(SSexp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;selfStart&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;RStudio actually creates a separate R session to render the document. This causes problem with the &lt;em&gt;getInitial()&lt;/em&gt; function which searches for user created SS functions in the Global environment and cannot find them because they are created in a new environment only for the Markdown use. Hence, we need to ensure that our created Self-Starter is defined in the Global environment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Only for Markdown execution
l &amp;lt;- list(SSexp = SSexp)
list2env(l, envir = .GlobalEnv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;environment: R_GlobalEnv&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-the-inital-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting the inital values&lt;/h2&gt;
&lt;p&gt;Self-starters can be passed as an argument to the function &lt;em&gt;getInitial()&lt;/em&gt; to obtain the initial values. Consider the following dataset&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(file = &amp;quot;RGRcurve.rda&amp;quot;)
head(RGRcurve)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Day   RGR
## 1   0 0.169
## 2   0 0.164
## 3   0 0.210
## 4   0 0.215
## 5   0 0.183
## 6   0 0.181&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, the initial values, determined according to our logic, will be&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getInitial(RGR ~ SSexp(Day, b,y0), data = RGRcurve)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         b        y0 
## 3.8450187 0.1674235&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-by-user-defined-ss&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting by user defined SS&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit4 &amp;lt;- nls(RGR ~ SSexp(Day, b,y0), data = RGRcurve)
getInitial(RGR ~ SSexp(Day, b,y0), data = RGRcurve)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         b        y0 
## 3.8450187 0.1674235&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compare the initial values with the estimated values
tidy(fit4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   term  estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 b        3.76     0.175       21.5 3.40e-23
## 2 y0       0.164    0.0142      11.5 4.03e-14&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- ggplot(data = RGRcurve, aes(x = Day, y = RGR)) + geom_point()
g + stat_function(fun = SSexp, args=tidy(fit4)$estimate, colour=&amp;quot;darkgreen&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;difficult-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Difficult Example&lt;/h1&gt;
&lt;div id=&#34;returning-to-the-difficult-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Returning to the difficult example&lt;/h2&gt;
&lt;p&gt;Considering once again the parametric family&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x,\beta)=\frac{\exp(-\beta_1x)}{\beta_2+\beta_3x},\ \ \beta=(\beta_1,\beta_2,\beta_3).\]&lt;/span&gt;
We can use Taylor’s formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x,\beta)\approx f(0,\beta)+f&amp;#39;(0,\beta)x,\ \ x\approx 0.\]&lt;/span&gt;
Simplifying this and remembering that &lt;span class=&#34;math inline&#34;&gt;\(f(0,\beta)=\frac{1}{\beta_2}\)&lt;/span&gt; we get
&lt;span class=&#34;math display&#34;&gt;\[f(x,\beta)\approx \frac{1}{\beta_2}-\frac{1}{\beta_2}\left(\beta_1+\frac{\beta_3}{\beta_2}\right)x.\]&lt;/span&gt;
Which in turn gives the equality
&lt;span class=&#34;math display&#34;&gt;\[1-\beta_2 f(x,\beta)=\left(\beta_1+\frac{\beta_3}{\beta_2}\right)x,\]&lt;/span&gt;
hence, transforming the data according to the left hand side and fitting a linear model without an intercept we can estimate &lt;span class=&#34;math inline&#34;&gt;\(\left(\beta_1+\frac{\beta_3}{\beta_2}\right).\)&lt;/span&gt; For two parameters out of three we have a rule to find starting values, for the third one we still need a guess. Taking &lt;span class=&#34;math inline&#34;&gt;\(\beta_1=0\)&lt;/span&gt; may work.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expFctInit &amp;lt;- function(mCall, LHS, data){
  xy &amp;lt;- sortedXyData(mCall[[&amp;quot;x&amp;quot;]], LHS, data)
  beta2 &amp;lt;- 1/xy[1,&amp;quot;y&amp;quot;]
  coefs &amp;lt;- coef(lm(1-beta2*xy[,&amp;quot;y&amp;quot;]~xy[,&amp;quot;x&amp;quot;]+0))
  beta1 &amp;lt;- 0
  beta3 &amp;lt;- coefs*beta2
  value &amp;lt;- c(beta1, beta2, beta3)
  names(value) &amp;lt;- c(&amp;quot;beta1&amp;quot;, &amp;quot;beta2&amp;quot;, &amp;quot;beta3&amp;quot;)
  value
}
SSexpFct &amp;lt;- selfStart(expFct, expFctInit, c(&amp;quot;beta1&amp;quot;, &amp;quot;beta2&amp;quot;, &amp;quot;beta3&amp;quot;))
l &amp;lt;- list(SSexpFct=SSexpFct)
list2env(l, envir = .GlobalEnv)#Only for Markdown execution&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;environment: R_GlobalEnv&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getInitial(data = Chwirut2, y ~ SSexpFct(x, beta1, beta2, beta3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       beta1       beta2       beta3 
## 0.000000000 0.012140834 0.002537341&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit0 &amp;lt;- nls(data = Chwirut2, y ~ SSexpFct(x, beta1, beta2, beta3))&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit0_tidy &amp;lt;- tidy(fit0)
fit0_tidy$p.value &amp;lt;- format.pval(fit0_tidy$p.value,eps=0.001,digits=3)
CI &amp;lt;- as.data.frame(suppressMessages(confint(fit0)))
CI$term &amp;lt;- row.names(CI)
fit0_tidy &amp;lt;- merge(fit0_tidy,CI,by=&amp;quot;term&amp;quot;)
fit0_tidy[,!names(fit0_tidy)%in%c(&amp;quot;term&amp;quot;, &amp;quot;p.value&amp;quot;)]&amp;lt;-
  round(fit0_tidy[,!names(fit0_tidy)%in%c(&amp;quot;term&amp;quot;, &amp;quot;p.value&amp;quot;)], 3)
fit0_tidy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    term estimate std.error statistic p.value  2.5% 97.5%
## 1 beta1    0.167     0.038     4.349  &amp;lt;0.001 0.094 0.254
## 2 beta2    0.005     0.001     7.753  &amp;lt;0.001 0.004 0.006
## 3 beta3    0.012     0.002     7.939  &amp;lt;0.001 0.009 0.015&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we used the generic function &lt;span class=&#34;math inline&#34;&gt;\(confint()\)&lt;/span&gt; on the object of class &lt;span class=&#34;math inline&#34;&gt;\(nls\)&lt;/span&gt; and got the confidence intervals for the parameters estimates. In addition, the function &lt;span class=&#34;math inline&#34;&gt;\(format.pval()\)&lt;/span&gt; was used to simplify the presentation of p.values.&lt;/p&gt;
&lt;hr /&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;values &amp;lt;- tidy(fit0)$estimate
g &amp;lt;- ggplot(data = Chwirut2, aes(x = x, y = y))+
  geom_point() + xlab(&amp;quot;Metal distance&amp;quot;) + ylab(&amp;quot;Ultrasonic respons&amp;quot;)
g + stat_function(fun = expFct, args = values,
                  colour=&amp;quot;blue&amp;quot;, lwd=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/drc/2018-03-19-r-rmarkdown_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(fit0, newdata = data.frame(x=6:7))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4.715006 3.453957&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Further Reading&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The package &lt;em&gt;drc&lt;/em&gt; (Dose-Response curves) contains much more prespecified parametric families (22, to be exact - run &lt;em&gt;drc::getMeanFunctions()&lt;/em&gt; to see the list).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The package &lt;em&gt;nlme&lt;/em&gt; (Linear and Nonlinear Mixed Effects Models) which allows the inclusion of the donor effect as a random effect in dose-response studies.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;More robust algorithms for fitting (&lt;em&gt;The Levenberg-Marquardt curve-fitting method&lt;/em&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-levenberg-marquardt-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Levenberg-Marquardt method&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The Levenberg-Marquardt curve-fitting method is a combination of the two other minimization methods: the gradient descent method and the Gauss-Newton method.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the gradient descent method, the sum of the squared errors is reduced by updating the parameters in the steepest-descent direction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the Gauss-Newton method, the sum of the squared errors is reduced by assuming the least squares function is locally quadratic, and finding the minimum of the quadratic.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The Levenberg-Marquardt method acts more like a gradient-descent method when the parameters are far from their optimal value, and acts more like the Gauss-Newton method when the parameters are close to their optimal value.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://people.duke.edu/~hpgavin/ce281/lm.pdf&#34;&gt;&lt;em&gt;Henri P. Gavin, The Levenberg-Marquardt method for nonlinear least squares curve-fitting problems&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;presentation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Presentation&lt;/h1&gt;
&lt;p&gt;Below is a presentation version of this lecture which contains also interactive applications for curve fitting.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Dose Response Curves in R</title>
      <link>https://gasparyan.co/talk/aua2018/</link>
      <pubDate>Thu, 13 Sep 2018 00:00:00 +0200</pubDate>
      <guid>https://gasparyan.co/talk/aua2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EasySTAT Project</title>
      <link>https://gasparyan.co/project/external-project/</link>
      <pubDate>Mon, 20 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://gasparyan.co/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Optimality of Estimators in Regular Models</title>
      <link>https://gasparyan.co/post/regmod/2018-09-13-r-rmarkdown/</link>
      <pubDate>Mon, 19 Mar 2018 09:00:00 +0400</pubDate>
      <guid>https://gasparyan.co/post/regmod/2018-09-13-r-rmarkdown/</guid>
      <description>
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://gasparyan.co/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://gasparyan.co/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;consistent-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Consistent Estimators&lt;/h2&gt;
In the estimation problem of a one-dimensional parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; from an i.i.d. sample &lt;span class=&#34;math display&#34;&gt;\[X^n=(X_1,\cdots,X_n),\ \ X_i\sim F(x,\theta),\,\theta\in\Theta\subset R\]&lt;/span&gt;
we introduced the notion of consistency of an estimator.
&lt;p&gt;This means that whatever the unknown value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is, this estimator is going to be close to that value in higher probability as &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; increases
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n\approx\theta, \text{ for large } n.\]&lt;/span&gt;
Now our goal is to quantify this closeness.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asymptotically-normal-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Asymptotically Normal Estimators&lt;/h2&gt;
&lt;p&gt;We say that the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_n\)&lt;/span&gt; is asymptotically normal if
&lt;span class=&#34;math display&#34;&gt;\[\frac{\hat\theta_n-\theta}{\sqrt{Var(\hat\theta_n)}}\stackrel{d}{\rightarrow}N (0,1),\]&lt;/span&gt;
which means that
&lt;span class=&#34;math display&#34;&gt;\[P\left(\frac{\hat\theta_n-\theta}{\sqrt{Var(\hat\theta_n)}}&amp;lt;x\right)\rightarrow P(\xi&amp;lt;x),\,x\in R,\,\xi\sim N(0,1).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;asymptotically-normal-estimators-example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Asymptotically Normal Estimators (Example 1)&lt;/h2&gt;
&lt;p&gt;Suppose that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X^n=(X_1,\cdots,X_n),\ \ X_i\sim N(\theta,\sigma^2),\,\theta\in R,\,\sigma&amp;gt;0.\]&lt;/span&gt;
We know that the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_n=\bar X_n=\frac{1}{n}\sum_{i=1}^nX_i\)&lt;/span&gt; is an unbiased, consistent estimator for &lt;span class=&#34;math inline&#34;&gt;\(\theta.\)&lt;/span&gt; By central limit theorem, as &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow+\infty,\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sqrt{n}\frac{\bar X_n-\theta}{\sigma}\stackrel{d}{\rightarrow}N(0,1),\]&lt;/span&gt;
hence this estimator is also asymptotically normal. The convergence above can be written as (when &lt;span class=&#34;math inline&#34;&gt;\(n\rightarrow+\infty\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sqrt{n}(\bar X_n-\theta)\stackrel{d}{\rightarrow}N(0,\sigma^2).\]&lt;/span&gt;
Here &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{n}\)&lt;/span&gt; is rate of convergence of the estimator and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; is the asymptotic variance.
Hence, higher the rate of convergence or smaller the asymptotic variance, better is the estimator.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-of-moments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method of Moments&lt;/h2&gt;
&lt;p&gt;Consider again the estimation problem of a one-dimensional parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; from an i.i.d. sample &lt;span class=&#34;math display&#34;&gt;\[X^n=(X_1,\cdots,X_n),\ \ X_i\sim F(x,\theta),\,\theta\in\Theta\subset R.\]&lt;/span&gt;
The idea of the method of moments is based on the fact that we can estimate the mathematical expectation, that is, for any given function &lt;span class=&#34;math inline&#34;&gt;\(g(\cdot),\)&lt;/span&gt; so that &lt;span class=&#34;math inline&#34;&gt;\(E|g(X_1)|&amp;lt;+\infty,\)&lt;/span&gt;
the following convergence is true
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\frac{1}{n}\sum_{j=1}^n g(X_j)\stackrel{P}{\rightarrow} E g(X_1).
\end{align*}\]&lt;/span&gt;
Hence, for the estimation of the parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta,\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(g(\cdot)\)&lt;/span&gt; we can calculate &lt;span class=&#34;math inline&#34;&gt;\(E_\theta g(X_1)=T(\theta).\)&lt;/span&gt; That will ensure the convergence
&lt;span class=&#34;math display&#34;&gt;\[
\frac{1}{n}\sum_{j=1}^n g(X_j)\stackrel{P}{\rightarrow}T(\theta).
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-of-moments-page-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method of Moments (Page 2)&lt;/h2&gt;
&lt;p&gt;Therefore, if the function &lt;span class=&#34;math inline&#34;&gt;\(T(\cdot)\)&lt;/span&gt; has continuous inverse function &lt;span class=&#34;math inline&#34;&gt;\(h=T^{-1},\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_n\)&lt;/span&gt; will be a consistent estimator
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n=h\left(\frac{1}{n}\sum_{j=1}^n g(X_j)\right)\stackrel{P}{\rightarrow}\theta.\]&lt;/span&gt;
Furthermore, if the function &lt;span class=&#34;math inline&#34;&gt;\(h(\cdot)\)&lt;/span&gt; is also differentiable and &lt;span class=&#34;math inline&#34;&gt;\(E_\theta g^2(X_1)&amp;lt;+\infty,\)&lt;/span&gt; then the delta method ensures that the MM estimator is also asymptotically normal
&lt;span class=&#34;math display&#34;&gt;\[\sqrt{n}(\hat\theta_n-\theta)\stackrel{d}{\rightarrow}N\left(0,[h&amp;#39;(E_\theta(X_1))]^2 Var_\theta(g(X_1))\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-of-moments-example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method of Moments (Example 2)&lt;/h2&gt;
&lt;p&gt;Consider the problem of parameter estimation in uniform distribution&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_1,\cdots,X_n,\ \ X_i\sim U[0,\theta],\,\theta&amp;gt;0.\]&lt;/span&gt;
Construct MM estimators using the functions &lt;span class=&#34;math inline&#34;&gt;\(g(x)=x,\ \ g(x)=x^2.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(g(x)=x\)&lt;/span&gt; we have &lt;span class=&#34;math inline&#34;&gt;\(E_\theta X_1=\frac{\theta}{2}=t,\)&lt;/span&gt; were the last equality is a notation. Then, &lt;span class=&#34;math inline&#34;&gt;\(\theta=2t=h(t),\)&lt;/span&gt; again, the last equality is a notation and
&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{n}\sum_{i=1}^n X_i\stackrel{P_\theta}{\rightarrow}E_\theta X_1=t.\]&lt;/span&gt;
Therefore,
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n^1=\frac{2}{n}\sum_{i=1}^n X_i\stackrel{P_\theta}{\rightarrow}2t=\theta,\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[\sqrt{n}(\hat\theta_n^1-\theta)\stackrel{d}{\rightarrow}N\left(0,\frac{\theta^2}{3}\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-of-moments-example-2-page-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method of Moments (Example 2, Page 2)&lt;/h2&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(g(x)=x^2\)&lt;/span&gt; we have &lt;span class=&#34;math inline&#34;&gt;\(E_\theta X_1^2=\frac{\theta^2}{3}=t,\)&lt;/span&gt; were the last equality is a notation. Then, &lt;span class=&#34;math inline&#34;&gt;\(\theta=\sqrt{3t}=h(t), \ \ (\theta&amp;gt;0)\)&lt;/span&gt; again, the last equality is a notation and
&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{n}\sum_{i=1}^n X_i^2\stackrel{P_\theta}{\rightarrow}E_\theta X_1^2=t.\]&lt;/span&gt;
Therefore,
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n^2=\sqrt{\frac{3}{n}\sum_{i=1}^n X_i^2}\stackrel{P_\theta}{\rightarrow}\sqrt{3t}=\theta,\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[\sqrt{n}(\hat\theta_n^2-\theta)\stackrel{d}{\rightarrow}N\left(0,\frac{\theta^2}{5}\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-of-moments-example-2-page-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method of Moments (Example 2, Page 3)&lt;/h2&gt;
&lt;p&gt;Hence, in the estimation problem of a one-dimensional parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; from a uniform distribution&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_1,\cdots,X_n,\ \ X_i\sim U[0,\theta],\,\theta&amp;gt;0,\]&lt;/span&gt;
we have constructed two estimators using the MM
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n^1=\frac{2}{n}\sum_{i=1}^n X_i, \ \ \hat\theta_n^2=\sqrt{\frac{3}{n}\sum_{i=1}^n X_i^2},\]&lt;/span&gt;
with the following properties
&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{n}(\hat\theta_n^1-\theta)\stackrel{d}{\rightarrow}N\left(0,\frac{\theta^2}{3}\right), \ \
\sqrt{n}(\hat\theta_n^2-\theta)\stackrel{d}{\rightarrow}N\left(0,\frac{\theta^2}{5}\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Both have the same rate of convergence, but the second one has smaller asymptotic variance, so, asymptotically, the second one is better, although the first one is computationally easier to implement (an important trade-off in Statistics).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-of-moments-page-2-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method of Moments (Page 2)&lt;/h2&gt;
&lt;p&gt;Therefore, if the function &lt;span class=&#34;math inline&#34;&gt;\(T(\cdot)\)&lt;/span&gt; has continuous inverse function &lt;span class=&#34;math inline&#34;&gt;\(h=T^{-1},\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_n\)&lt;/span&gt; will be a consistent estimator
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n=h\left(\frac{1}{n}\sum_{j=1}^n g(X_j)\right)\stackrel{P}{\rightarrow}\theta.\]&lt;/span&gt;
Furthermore, if the function &lt;span class=&#34;math inline&#34;&gt;\(h(\cdot)\)&lt;/span&gt; is also differentiable and &lt;span class=&#34;math inline&#34;&gt;\(E_\theta g^2(X_1)&amp;lt;+\infty,\)&lt;/span&gt; then the delta method ensures that the MM estimator is also asymptotically normal
&lt;span class=&#34;math display&#34;&gt;\[\sqrt{n}(\hat\theta_n-\theta)\stackrel{d}{\rightarrow}N\left(0,[h&amp;#39;(E_\theta(X_1))]^2Var_\theta(g(X_1))\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-of-moments-example-2-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method of Moments (Example 2)&lt;/h2&gt;
&lt;p&gt;Consider the problem of parameter estimation in uniform distribution&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_1,\cdots,X_n,\ \ X_i\sim U[0,\theta],\,\theta&amp;gt;0.\]&lt;/span&gt;
Construct MM estimators using the functions &lt;span class=&#34;math inline&#34;&gt;\(g(x)=x,\ \ g(x)=x^2.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(g(x)=x\)&lt;/span&gt; we have &lt;span class=&#34;math inline&#34;&gt;\(E_\theta X_1=\frac{\theta}{2}=t,\)&lt;/span&gt; were the last equality is a notation. Then, &lt;span class=&#34;math inline&#34;&gt;\(\theta=2t=h(t),\)&lt;/span&gt; again, the last equality is a notation and
&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{n}\sum_{i=1}^n X_i\stackrel{P_\theta}{\rightarrow}E_\theta X_1=t.\]&lt;/span&gt;
Therefore,
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n^1=\frac{2}{n}\sum_{i=1}^n X_i\stackrel{P_\theta}{\rightarrow}2t=\theta,\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[\sqrt{n}(\hat\theta_n^1-\theta)\stackrel{d}{\rightarrow}N\left(0,\frac{\theta^2}{3}\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-of-moments-example-2-page-2-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method of Moments (Example 2, Page 2)&lt;/h2&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(g(x)=x^2\)&lt;/span&gt; we have &lt;span class=&#34;math inline&#34;&gt;\(E_\theta X_1^2=\frac{\theta^2}{3}=t,\)&lt;/span&gt; were the last equality is a notation. Then, &lt;span class=&#34;math inline&#34;&gt;\(\theta=\sqrt{3t}=h(t), \ \ (\theta&amp;gt;0)\)&lt;/span&gt; again, the last equality is a notation and
&lt;span class=&#34;math display&#34;&gt;\[\frac{1}{n}\sum_{i=1}^n X_i^2\stackrel{P_\theta}{\rightarrow}E_\theta X_1^2=t.\]&lt;/span&gt;
Therefore,
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n^2=\sqrt{\frac{3}{n}\sum_{i=1}^n X_i^2}\stackrel{P_\theta}{\rightarrow}\sqrt{3t}=\theta,\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[\sqrt{n}(\hat\theta_n^2-\theta)\stackrel{d}{\rightarrow}N\left(0,\frac{\theta^2}{5}\right).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-of-moments-example-2-page-3-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method of Moments (Example 2, Page 3)&lt;/h2&gt;
&lt;p&gt;Hence, in the estimation problem of a one-dimensional parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; from a uniform distribution&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_1,\cdots,X_n,\ \ X_i\sim U[0,\theta],\,\theta&amp;gt;0,\]&lt;/span&gt;
we have constructed two estimators using the MM
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n^1=\frac{2}{n}\sum_{i=1}^n X_i, \ \ \hat\theta_n^2=\sqrt{\frac{3}{n}\sum_{i=1}^n X_i^2},\]&lt;/span&gt;
with the following properties
&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{n}(\hat\theta_n^1-\theta)\stackrel{d}{\rightarrow}N\left(0,\frac{\theta^2}{3}\right), \ \
\sqrt{n}(\hat\theta_n^2-\theta)\stackrel{d}{\rightarrow}N\left(0,\frac{\theta^2}{5}\right).\]&lt;/span&gt;
Both have the same rate of convergence, but the second one has smaller asymptotic variance, so, asymptotically, the second one is better, although the first one is computationally easier to implement (an important trade-off in Statistics).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-maximum-likelihood-estimator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Maximum Likelihood Estimator&lt;/h2&gt;
&lt;p&gt;Consider again the estimation problem of a one-dimensional parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; from an i.i.d. sample &lt;span class=&#34;math display&#34;&gt;\[X^n=(X_1,\cdots,X_n),\ \ X_i\sim F(x,\theta),\,\theta\in\Theta\subset R.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The second method to construct estimators is the Maximum Likelihood Estimator. Here we need the existence of the density functions &lt;span class=&#34;math inline&#34;&gt;\(\{f(x,\theta,\,\theta\in\Theta)\}.\)&lt;/span&gt;
The following function is called the Likelihood function of the above model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[L(X^n,\theta)=\prod_{i=1}^n f(X_i,\theta).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Maximum Likelihood Estimator (the MLE) is defined as the point of maximum of the likelihood function
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n^{MLE}=\arg\max_{\theta\in\Theta}L(X^n,\theta).\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regular-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regular Models&lt;/h2&gt;
&lt;p&gt;We call the model &lt;span class=&#34;math inline&#34;&gt;\(\{f(x,\theta),\,\theta\in\Theta\}\)&lt;/span&gt; a regular if the derivative of the density functions &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot,\theta)\)&lt;/span&gt; exists with respect to &lt;span class=&#34;math inline&#34;&gt;\(\theta.\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Beware that there are other technical conditions as well in the definition of regular models, but we will check only the condition above.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; The uniform distribution has the density function
&lt;span class=&#34;math display&#34;&gt;\[f(x,\theta)=\frac{1}{\theta}I_{[0,\theta]}(x)=\frac{1}{\theta}I_{[0,+\infty)}(x)I_{[x,+\infty)}(\theta).\]&lt;/span&gt;
This function is not differentiable w.r.t. the unknown parameter &lt;span class=&#34;math inline&#34;&gt;\(\theta,\)&lt;/span&gt; hence this is not a regular model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-properties-of-the-mle&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Properties of the MLE&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Since the natural logarithm is an increasing function, then we can define (if &lt;span class=&#34;math inline&#34;&gt;\(f(x,\theta)&amp;gt;0,\,(x,\theta)\in\Theta\times R\)&lt;/span&gt;)
&lt;span class=&#34;math display&#34;&gt;\[V(X^n,\theta)=\ln L(X^n,\theta)=\sum_{i=1}^n\ln f(X_i,\theta)\]&lt;/span&gt;
and for the MLE we will have
&lt;span class=&#34;math display&#34;&gt;\[\hat\theta_n^{MLE}=\arg\max_{\theta\in\Theta}V(X^n,\theta)=\arg\max_{\theta\in\Theta}\sum_{i=1}^n\ln f(X_i,\theta).\]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;the-properties-of-the-mle-page-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Properties of the MLE (Page 2)&lt;/h2&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In the regular models the MLE is consistent and asymptotically normal with the asymptotic variance equal to the inverse of the Fisher information &lt;span class=&#34;math inline&#34;&gt;\(I^{-1}(\theta)\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sqrt{n}(\hat\theta_n^{MLE}-\theta)\stackrel{d}{\rightarrow}N(0,I^{-1}(\theta)),\]&lt;/span&gt;
where the Fisher information is defined as
&lt;span class=&#34;math display&#34;&gt;\[I(\theta)=\int_{R}\left[\frac{\partial(\ln f(x,\theta))}{\partial\theta}\right]^2f(x,\theta){\rm d} x.\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the second derivative of the density functions &lt;span class=&#34;math inline&#34;&gt;\(f(x,\theta)\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; exists then the Fisher information can be calculated by the following formula&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[I(\theta)=-\int_{R}\left[\frac{\partial^2(\ln f(x,\theta))}{\partial\theta^2}\right]f(x,\theta){\rm d} x.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-of-a-non-regular-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example of a Non-Regular Model&lt;/h2&gt;
&lt;p&gt;Consider again the problem of parameter estimation in uniform distribution
&lt;span class=&#34;math display&#34;&gt;\[X_1,\cdots,X_n,\ \ X_i\sim U[0,\theta],\,\theta&amp;gt;0.\]&lt;/span&gt;
The density function of the uniform distribution is given by the formula
&lt;span class=&#34;math display&#34;&gt;\[f(x,\theta)=\frac{1}{\theta}I_{[0,\theta]}(x),\]&lt;/span&gt;
hence the likelihood function will be
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
L(X^n,\theta)&amp;amp;=\frac{1}{\theta^n}\prod_{i=1}^n I_{[0,\theta]}(X_i)=\frac{1}{\theta^n}I_{[0,+\infty)}(X_{(1)})I_{[0,\theta]}(X_{(n)})=\\
&amp;amp;=\frac{1}{\theta^n}I_{[0,+\infty)}(X_{(1)})I_{[X_{(n)},+\infty)}(\theta)\stackrel{a.s.}{=}\frac{1}{\theta^n}I_{[X_{(n)},+\infty)}(\theta),
\end{align*}\]&lt;/span&gt;
which attains its maximum at the point &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_n^\ast=X_{(n)},\)&lt;/span&gt; so that is the MLE.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-of-a-non-regular-model-page-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example of a Non-Regular Model (Page 2)&lt;/h2&gt;
&lt;p&gt;Calculate the distribution function of this estimator
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
P(X_{(n)}&amp;lt; x)&amp;amp;=P(\max\{X_1,\cdots,X_n\}&amp;lt;x)=P(X_1&amp;lt;x,\cdots,X_n&amp;lt;x)=\\
&amp;amp;=P(X_1&amp;lt;x)\cdots P(X_n&amp;lt;x)=F^n(x),
\end{align*}\]&lt;/span&gt;
hence
&lt;span class=&#34;math display&#34;&gt;\[P(n(\theta-X_{(n)})&amp;lt;x)=P\left(\theta-\frac{x}{n}&amp;lt;X_{(n)}\right)=1-F^n\left(\theta-\frac{x}{n}\right),\]&lt;/span&gt;
therefore, &lt;span class=&#34;math inline&#34;&gt;\(P(n(\theta-X_{(n)})&amp;lt;x)=0,\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(x\leq0,\)&lt;/span&gt; and for &lt;span class=&#34;math inline&#34;&gt;\(x&amp;gt;0\)&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
&amp;amp;P(n(\theta-X_{(n)})&amp;lt;x)=P\left(\theta-\frac{x}{n}&amp;lt;X_{(n)}\right)=1-F^n\left(\theta-\frac{x}{n}\right)=\\
&amp;amp;=1-\left(\frac{\theta-\frac{x}{n}}{\theta}\right)^n=1-\left[\left(1-\frac{x}{\theta n}\right)^{-\frac{n\theta}{x}}\right]^{-\frac{x}{\theta}}\rightarrow1-e^{-\frac{x}{\theta}}.
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-of-a-non-regular-model-page-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example of a Non-Regular Model (Page 3)&lt;/h2&gt;
&lt;p&gt;So, for the MLE we have the following convergence to the exponential distribution
&lt;span class=&#34;math display&#34;&gt;\[n(\theta-\hat\theta_n^\ast)\stackrel{d}{\rightarrow}E\left(\frac{1}{\theta}\right),\]&lt;/span&gt;
which means that the rate of convergence for the MLE is &lt;span class=&#34;math inline&#34;&gt;\(n,\)&lt;/span&gt; unlike the two previous estimators constructed by the method of moments for which the rate of convergence was &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{n}.\)&lt;/span&gt; In fact, we can show that even non-asymptotically&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
E_\theta(\hat\theta^\ast_n-\theta)^2=\frac{2\theta^2}{(n+1)(n+2)}&amp;lt;\frac{\theta^2}{3n}=E_\theta(\hat\theta^{1}_n-\theta)^2,\,n&amp;gt;2,
\end{align*}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta^1_n=2\bar X_n,\)&lt;/span&gt; which entails that even for small sample sizes the MLE is better.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-of-a-non-regular-model-page-4-simulations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example of a Non-Regular Model (Page 4, Simulations)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
n&amp;lt;-1000
theta&amp;lt;-2
X&amp;lt;-runif(n,0,theta)
th1&amp;lt;-2*cumsum(X)/(1:n)
th2&amp;lt;-sqrt(3*cumsum(X^2)/(1:n))
th&amp;lt;-numeric()
for(i in 1:n){
  th[i]&amp;lt;-max(X[1:i])
}
plot(1:n,th1,type=&amp;quot;l&amp;quot;,col=1,ylim=c(1.8,2.2),
     xlab=&amp;quot;Sample Size&amp;quot;,ylab=&amp;quot;Estimators&amp;quot;)
lines(1:n,th2,type=&amp;quot;l&amp;quot;,col=2)
lines(1:n,th,type=&amp;quot;l&amp;quot;,col=3)
abline(h=theta,col=4,lty=2)
legend(500,2.2,col=c(1,2,3,4),lty=c(1,1,1,2),
       legend=c(&amp;quot;MM1&amp;quot;,&amp;quot;MM2&amp;quot;,&amp;quot;MLE&amp;quot;,&amp;quot;True Value&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-of-a-non-regular-model-page-5&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example of a Non-Regular Model (Page 5)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/regmod/2018-09-13-r-rmarkdown_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-of-a-non-regular-model-page-6&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example of a Non-Regular Model (Page 6)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n&amp;lt;-10000
m&amp;lt;-1000
theta&amp;lt;-2

th1&amp;lt;-numeric()
th2&amp;lt;-numeric()
th&amp;lt;-numeric()

for(i in 1:m){set.seed(i);X&amp;lt;-runif(n,0,theta)
  th1[i] &amp;lt;- 2*mean(X)
  th2[i] &amp;lt;- sqrt(3*mean(X^2))
  th[i] &amp;lt;- max(X) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-of-a-non-regular-model-page-7&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example of a Non-Regular Model (Page 7)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mfrow=c(1,3))
x&amp;lt;-seq(-4,4,0.001)
hist(sqrt(n)*(th1-theta),nclass=50,freq=FALSE,
     col=&amp;quot;lightblue&amp;quot;, border=&amp;quot;blue&amp;quot;,ylab=&amp;quot;&amp;quot;,
     xlab=&amp;quot;&amp;quot;,main=&amp;quot;MM1&amp;quot;,xlim=c(-4,4),ylim=c(0,0.5))
lines(x,dnorm(x,0,theta^2/3),col=2)

hist(sqrt(n)*(th2-theta),nclass=50,freq=FALSE,
     col=&amp;quot;lightblue&amp;quot;, border=&amp;quot;blue&amp;quot;,ylab=&amp;quot;&amp;quot;,
     xlab=&amp;quot;&amp;quot;,main=&amp;quot;MM2&amp;quot;,xlim=c(-4,4),ylim=c(0,0.5))
lines(x,dnorm(x,0,theta^2/5),col=2)

x&amp;lt;-seq(0,10,0.001)
hist(n*(theta-th),nclass=50,freq=FALSE,col=&amp;quot;lightblue&amp;quot;,
     border=&amp;quot;blue&amp;quot;,ylab=&amp;quot;&amp;quot;,xlab=&amp;quot;&amp;quot;,main=&amp;quot;MLE&amp;quot;,
     xlim=c(0,10),ylim=c(0,0.5))
lines(x,dexp(x,1/theta),col=2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-of-a-non-regular-model-page-7-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example of a Non-Regular Model (Page 7)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://gasparyan.co/post/regmod/2018-09-13-r-rmarkdown_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Two problems of statistical estimation for stochastic processes</title>
      <link>https://gasparyan.co/talk/defense2016/</link>
      <pubDate>Mon, 12 Dec 2016 00:00:00 +0100</pubDate>
      <guid>https://gasparyan.co/talk/defense2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Some Problems of Statistical Estimation</title>
      <link>https://gasparyan.co/talk/aua2016/</link>
      <pubDate>Fri, 09 Sep 2016 00:00:00 +0200</pubDate>
      <guid>https://gasparyan.co/talk/aua2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Second Order Asymptotic Efficiency for a Poisson process</title>
      <link>https://gasparyan.co/talk/dynstoch2016/</link>
      <pubDate>Fri, 10 Jun 2016 00:00:00 +0200</pubDate>
      <guid>https://gasparyan.co/talk/dynstoch2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Publications related to the PhD thesis</title>
      <link>https://gasparyan.co/publication/phd-thesis/</link>
      <pubDate>Wed, 12 Dec 2012 00:00:00 +0000</pubDate>
      <guid>https://gasparyan.co/publication/phd-thesis/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
